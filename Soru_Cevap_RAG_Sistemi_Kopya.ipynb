{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOKYLRuRoqu5vGtioTSFmZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehmetaksoy/Soru-Cevap-Sistemi-RAG/blob/main/Soru_Cevap_RAG_Sistemi_Kopya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUrhtORDz3qv"
      },
      "outputs": [],
      "source": [
        "# HÃ¼cre 1: Gerekli KÃ¼tÃ¼phanelerin Kurulumu (GÃœNCELLENDÄ° - Loglama Eklendi)\n",
        "\n",
        "# HÃ¼cre 0'daki log_cell_start ve log_cell_end fonksiyonlarÄ±nÄ±n bu hÃ¼crede de eriÅŸilebilir olduÄŸunu varsayÄ±yoruz.\n",
        "# GÃ¼venlik iÃ§in, eÄŸer fonksiyonlar bulunamazsa diye bir kontrol eklenebilir.\n",
        "# Åimdilik HÃ¼cre 0'Ä±n Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nÄ± ve logger'Ä±n ayarlandÄ±ÄŸÄ±nÄ± varsayÄ±yoruz.\n",
        "\n",
        "CELL_NAME_H1 = \"1: Gerekli KÃ¼tÃ¼phanelerin Kurulumu\"\n",
        "if 'log_cell_start' in globals() and 'logger' in globals(): # logger'Ä±n da varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "    log_cell_start(CELL_NAME_H1)\n",
        "    logger.info(\"HÃ¼cre 1 baÅŸlatÄ±ldÄ±: Gerekli kÃ¼tÃ¼phanelerin kurulumu.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H1} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(\"HÃ¼cre 1 baÅŸlatÄ±ldÄ±: Gerekli kÃ¼tÃ¼phanelerin kurulumu.\")\n",
        "\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"pip paket yÃ¶neticisi gÃ¼ncelleniyor...\")\n",
        "else:\n",
        "    print(\"pip paket yÃ¶neticisi gÃ¼ncelleniyor...\")\n",
        "!pip install --upgrade pip -q\n",
        "\n",
        "# Temel RAG ve LLM kÃ¼tÃ¼phaneleri\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel RAG ve LLM kÃ¼tÃ¼phaneleri kuruluyor: transformers, sentence-transformers, pdfplumber, faiss-cpu, nltk, rouge-score, scikit-learn, bert_score, langchain, torch, accelerate, bitsandbytes\")\n",
        "else:\n",
        "    print(\"Temel RAG ve LLM kÃ¼tÃ¼phaneleri kuruluyor...\")\n",
        "!pip install transformers sentence-transformers pdfplumber faiss-cpu nltk rouge-score scikit-learn bert_score langchain torch accelerate bitsandbytes -q\n",
        "\n",
        "# Langchain topluluk ve deneysel kÃ¼tÃ¼phaneleri\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Langchain topluluk ve deneysel kÃ¼tÃ¼phaneleri kuruluyor: langchain_community, langchain_experimental\")\n",
        "else:\n",
        "    print(\"Langchain topluluk ve deneysel kÃ¼tÃ¼phaneleri kuruluyor...\")\n",
        "!pip install langchain_community langchain_experimental -q\n",
        "\n",
        "# BLEURT Kurulumu\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BLEURT ve tf-slim (GitHub'dan) kuruluyor...\")\n",
        "else:\n",
        "    print(\"ğŸ”„ BLEURT ve tf-slim kuruluyor (GitHub'dan)...\")\n",
        "!pip install git+https://github.com/google-research/bleurt.git tf-slim -q\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BLEURT ve tf-slim kurulum denemesi tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"âœ… BLEURT ve tf-slim kurulum denemesi tamamlandÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(\"KÃ¼tÃ¼phane kurulumlarÄ± tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H1)\n",
        "else:\n",
        "    print(\"KÃ¼tÃ¼phane kurulumlarÄ± tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H1} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 2: Hugging Face GiriÅŸi (Sabit Token ile) (GÃœNCELLENDÄ° - Loglama Eklendi)\n",
        "\n",
        "CELL_NAME_H2 = \"2: Hugging Face GiriÅŸi\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H2)\n",
        "    logger.info(\"HÃ¼cre 2 baÅŸlatÄ±ldÄ±: Hugging Face giriÅŸi.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H2} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(\"HÃ¼cre 2 baÅŸlatÄ±ldÄ±: Hugging Face giriÅŸi.\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os # os import'u burada tekrar edilse de zararÄ± olmaz, zaten edilmiÅŸ olabilir.\n",
        "\n",
        "# Sabit token'Ä± doÄŸrudan kullanÄ±yoruz\n",
        "hf_token_sabit = \"hf_**************************************************************\" # <--- BU TOKEN GENEL KULLANIMA AÃ‡IK OLMAMALI, GEREKÄ°RSE ORTAM DEÄÄ°ÅKENÄ°NDEN ALINMALI\n",
        "hf_token_to_use = None # KullanÄ±lacak nihai token\n",
        "\n",
        "if hf_token_sabit and hf_token_sabit != \"YOUR_PLACEHOLDER_TOKEN\" and hf_token_sabit.startswith(\"hf_\"):\n",
        "    hf_token_to_use = hf_token_sabit\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Sabit Hugging Face tokenÄ± kullanÄ±lacak.\")\n",
        "    else:\n",
        "        print(\"INFO: Sabit Hugging Face tokenÄ± kullanÄ±lacak.\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"GeÃ§erli bir sabit Hugging Face tokenÄ± (hf_token_sabit) tanÄ±mlanmamÄ±ÅŸ veya formatÄ± yanlÄ±ÅŸ.\")\n",
        "    else:\n",
        "        print(\"WARNING: GeÃ§erli bir sabit Hugging Face tokenÄ± (hf_token_sabit) tanÄ±mlanmamÄ±ÅŸ veya formatÄ± yanlÄ±ÅŸ.\")\n",
        "\n",
        "if hf_token_to_use:\n",
        "    try:\n",
        "        login(token=hf_token_to_use)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"Hugging Face'e baÅŸarÄ±yla giriÅŸ yapÄ±ldÄ±.\")\n",
        "        else:\n",
        "            print(\"INFO: Hugging Face'e baÅŸarÄ±yla giriÅŸ yapÄ±ldÄ±.\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Hugging Face giriÅŸi baÅŸarÄ±sÄ±z oldu. Hata: {e}\")\n",
        "        else:\n",
        "            print(f\"ERROR: Hugging Face giriÅŸi baÅŸarÄ±sÄ±z oldu. Hata: {e}\")\n",
        "else:\n",
        "    # YukarÄ±daki if bloÄŸunda zaten uyarÄ± verildi, burada ek bir loglama yapÄ±labilir veya pas geÃ§ilebilir.\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Hugging Face tokenÄ± saÄŸlanmadÄ±ÄŸÄ± iÃ§in giriÅŸ denemesi yapÄ±lmadÄ±.\")\n",
        "    else:\n",
        "        print(\"INFO: Hugging Face tokenÄ± saÄŸlanmadÄ±ÄŸÄ± iÃ§in giriÅŸ denemesi yapÄ±lmadÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(\"Hugging Face giriÅŸ iÅŸlemleri tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H2)\n",
        "else:\n",
        "    print(\"Hugging Face giriÅŸ iÅŸlemleri tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H2} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "I34YPyNY0nyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 3: KÃ¼tÃ¼phane Ä°Ã§e Aktarma, NLTK Kurulumu, BLEURT ModÃ¼l KontrolÃ¼ (GÃœNCELLENDÄ° - Loglama Eklendi)\n",
        "\n",
        "CELL_NAME_H3 = \"3: KÃ¼tÃ¼phane Ä°Ã§e Aktarma, NLTK ve BLEURT YÃ¶netimi\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H3)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H3} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H3} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H3} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "# --- BÃ–LÃœM A: Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ± ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- BÃ¶lÃ¼m A: KÃ¼tÃ¼phaneler Ä°Ã§e AktarÄ±lÄ±yor ---\")\n",
        "else:\n",
        "    print(\"--- BÃ¶lÃ¼m A: KÃ¼tÃ¼phaneler Ä°Ã§e AktarÄ±lÄ±yor ---\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "import nltk # nltk.download kullanmadan Ã¶nce import edilmeli\n",
        "import torch\n",
        "import faiss\n",
        "import hashlib\n",
        "import pickle\n",
        "import zipfile # Manuel zip aÃ§ma iÃ§in\n",
        "\n",
        "# NLTK alt modÃ¼llerini, NLTK ana veri yolu ayarlandÄ±ktan ve kaynaklar indirildikten SONRA import etmek daha gÃ¼venli olabilir.\n",
        "# Ancak stopwords ve tokenizers genellikle temel nltk kurulumu ile gelir veya erken import edilebilir.\n",
        "# Åimdilik burada bÄ±rakÄ±yoruz, sorun olursa yerlerini deÄŸiÅŸtirebiliriz.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from rouge_score import rouge_scorer # nltk.translate'den farklÄ±, kendi baÅŸÄ±na bir kÃ¼tÃ¼phane\n",
        "from bert_score import score as bert_score_L # Ä°sim Ã§akÄ±ÅŸmasÄ±nÄ± Ã¶nlemek iÃ§in bert_score_L olarak bÄ±rakÄ±ldÄ±\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Langchain importlarÄ±\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel kÃ¼tÃ¼phaneler iÃ§e aktarÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(\"Temel kÃ¼tÃ¼phaneler iÃ§e aktarÄ±ldÄ±.\")\n",
        "\n",
        "# BLEURT kÃ¼tÃ¼phanesini koÅŸullu olarak iÃ§e aktarma\n",
        "bleurt_available = False\n",
        "bleurt_score_module_global_for_h3_and_h9 = None # HÃ¼cre 9'da scorer'Ä± baÅŸlatmak iÃ§in kullanÄ±lacak, global yaptÄ±k\n",
        "try:\n",
        "    from bleurt import score as bleurt_score_module_temp # geÃ§ici bir isimle al\n",
        "    bleurt_score_module_global_for_h3_and_h9 = bleurt_score_module_temp # globale ata\n",
        "    bleurt_available = True\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"BLEURT kÃ¼tÃ¼phanesi (bleurt.score modÃ¼lÃ¼) baÅŸarÄ±yla iÃ§e aktarÄ±ldÄ± ve global deÄŸiÅŸkene atandÄ±.\")\n",
        "    else:\n",
        "        print(\"INFO: BLEURT kÃ¼tÃ¼phanesi (bleurt.score modÃ¼lÃ¼) baÅŸarÄ±yla iÃ§e aktarÄ±ldÄ± ve global deÄŸiÅŸkene atandÄ±.\")\n",
        "except ImportError:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"BLEURT kÃ¼tÃ¼phanesi bulunamadÄ± veya iÃ§e aktarÄ±lamadÄ±. Kurulumda bir sorun olabilir. BLEURT skorlamasÄ± atlanacak.\")\n",
        "    else:\n",
        "        print(\"WARNING: BLEURT kÃ¼tÃ¼phanesi bulunamadÄ± veya iÃ§e aktarÄ±lamadÄ±. Kurulumda bir sorun olabilir. BLEURT skorlamasÄ± atlanacak.\")\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"BLEURT iÃ§e aktarÄ±lÄ±rken beklenmedik bir hata: {e}. BLEURT skorlamasÄ± atlanacak.\")\n",
        "    else:\n",
        "        print(f\"ERROR: BLEURT iÃ§e aktarÄ±lÄ±rken beklenmedik bir hata: {e}. BLEURT skorlamasÄ± atlanacak.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BÃ¶lÃ¼m A (KÃ¼tÃ¼phane Ä°Ã§e Aktarma) tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"BÃ¶lÃ¼m A (KÃ¼tÃ¼phane Ä°Ã§e Aktarma) tamamlandÄ±.\")\n",
        "\n",
        "# --- BÃ–LÃœM B: NLTK KaynaklarÄ±nÄ±n Ä°ndirilmesi ve HazÄ±rlanmasÄ± ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- BÃ¶lÃ¼m B: NLTK Kaynak YÃ¶netimi BaÅŸlatÄ±lÄ±yor ---\")\n",
        "else:\n",
        "    print(\"--- BÃ¶lÃ¼m B: NLTK Kaynak YÃ¶netimi BaÅŸlatÄ±lÄ±yor ---\")\n",
        "\n",
        "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK ana veri dizini olarak '{nltk_data_dir}' kullanÄ±lacak/oluÅŸturulacak.\")\n",
        "else:\n",
        "    print(f\"NLTK ana veri dizini olarak '{nltk_data_dir}' kullanÄ±lacak/oluÅŸturulacak.\")\n",
        "\n",
        "# Gerekli alt dizinlerin varlÄ±ÄŸÄ±nÄ± kontrol et/oluÅŸtur\n",
        "# nltk.download() bu dizinleri zaten oluÅŸturur, ancak manuel kontrol de zararsÄ±zdÄ±r.\n",
        "os.makedirs(os.path.join(nltk_data_dir, 'corpora'), exist_ok=True)\n",
        "os.makedirs(os.path.join(nltk_data_dir, 'tokenizers'), exist_ok=True)\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK iÃ§in gerekli alt dizinler ('corpora', 'tokenizers') '{nltk_data_dir}' altÄ±nda kontrol edildi/oluÅŸturuldu.\")\n",
        "else:\n",
        "    print(f\"NLTK iÃ§in gerekli alt dizinler ('corpora', 'tokenizers') '{nltk_data_dir}' altÄ±nda kontrol edildi/oluÅŸturuldu.\")\n",
        "\n",
        "# NLTK'nÄ±n arama yolunu gÃ¼ncelle (nltk_data_dir'i baÅŸa ekle)\n",
        "if nltk_data_dir not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, nltk_data_dir)\n",
        "elif nltk.data.path[0] != nltk_data_dir: # EÄŸer listede var ama baÅŸta deÄŸilse, silip baÅŸa ekle\n",
        "    nltk.data.path.remove(nltk_data_dir)\n",
        "    nltk.data.path.insert(0, nltk_data_dir)\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK veri yolu gÃ¼ncellendi. Aktif yollar: {nltk.data.path}\")\n",
        "else:\n",
        "    print(f\"NLTK veri yolu gÃ¼ncellendi. Aktif yollar: {nltk.data.path}\")\n",
        "\n",
        "resource_configs = {\n",
        "    'wordnet':   {'zip_filename': 'wordnet.zip',   'type_dir_name': 'corpora',    'verification_path': 'corpora/wordnet',   'manual_unzip_needed': True, 'unzip_target_name': 'wordnet'},\n",
        "    'omw-1.4':   {'zip_filename': 'omw-1.4.zip',   'type_dir_name': 'corpora',    'verification_path': 'corpora/omw-1.4',   'manual_unzip_needed': True, 'unzip_target_name': 'omw-1.4'},\n",
        "    'punkt':     {'zip_filename': 'punkt.zip',     'type_dir_name': 'tokenizers', 'verification_path': 'tokenizers/punkt',  'manual_unzip_needed': False}, # NLTK kendi aÃ§ar\n",
        "    'stopwords': {'zip_filename': 'stopwords.zip', 'type_dir_name': 'corpora',    'verification_path': 'corpora/stopwords', 'manual_unzip_needed': False}, # NLTK kendi aÃ§ar\n",
        "    # 'punkt_tab' genellikle 'punkt' ile birlikte gelir veya daha az yaygÄ±ndÄ±r. Sorun Ã§Ä±karÄ±rsa kaldÄ±rÄ±labilir.\n",
        "    # Åimdilik koruyoruz, Ã§Ä±ktÄ±da baÅŸarÄ±lÄ± gÃ¶rÃ¼nÃ¼yordu.\n",
        "    'punkt_tab': {'zip_filename': 'punkt_tab.zip', 'type_dir_name': 'tokenizers', 'verification_path': 'tokenizers/punkt_tab', 'manual_unzip_needed': False}\n",
        "}\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- NLTK kaynaklarÄ± indirme ve doÄŸrulama iÅŸlemi baÅŸlÄ±yor ---\")\n",
        "else:\n",
        "    print(\"--- NLTK kaynaklarÄ± indirme ve doÄŸrulama iÅŸlemi baÅŸlÄ±yor ---\")\n",
        "\n",
        "for resource_id, config in resource_configs.items():\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"'{resource_id}' NLTK kaynaÄŸÄ± iÅŸleniyor...\")\n",
        "    print(f\"\\nğŸ”„ '{resource_id}' NLTK kaynaÄŸÄ± iÃ§in iÅŸlem yapÄ±lÄ±yor...\")\n",
        "\n",
        "    type_dir_absolute_path = os.path.join(nltk_data_dir, config['type_dir_name'])\n",
        "    # zip dosyasÄ±nÄ±n tam yolu (eÄŸer manuel aÃ§ma gerekiyorsa)\n",
        "    actual_zip_file_path = os.path.join(type_dir_absolute_path, config['zip_filename'])\n",
        "    # aÃ§Ä±lmÄ±ÅŸ dizinin tam yolu (eÄŸer manuel aÃ§ma gerekiyorsa)\n",
        "    unzipped_target_dir_absolute_path = os.path.join(type_dir_absolute_path, config.get('unzip_target_name', resource_id))\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 1. AdÄ±m: NLTK ile indirmeyi dene\n",
        "        print(f\"   -> '{resource_id}' NLTK ile '{nltk_data_dir}' dizinine indiriliyor/kontrol ediliyor...\")\n",
        "        # quiet=False NLTK'nÄ±n kendi loglarÄ±nÄ± gÃ¶sterir, raise_on_error=True hata durumunda exception fÄ±rlatÄ±r.\n",
        "        nltk.download(resource_id, download_dir=nltk_data_dir, quiet=False, raise_on_error=True)\n",
        "        print(f\"   -> '{resource_id}' iÃ§in NLTK download komutu baÅŸarÄ±yla tamamlandÄ± (veya kaynak zaten gÃ¼nceldi).\")\n",
        "\n",
        "        # 2. AdÄ±m: Manuel aÃ§ma (eÄŸer gerekiyorsa ve NLTK otomatik yapmadÄ±ysa)\n",
        "        # NLTK'nÄ±n download fonksiyonu genellikle zip dosyalarÄ±nÄ± kendisi aÃ§ar (extract=True varsayÄ±lan).\n",
        "        # Bu yÃ¼zden manuel_unzip_needed=True olanlar iÃ§in bile, Ã¶nce aÃ§Ä±lÄ±p aÃ§Ä±lmadÄ±ÄŸÄ±nÄ± kontrol etmek iyi bir fikir.\n",
        "        # Åimdiki kodunuzda NLTK'nÄ±n aÃ§masÄ±na gÃ¼veniliyor (manuel_unzip=False olanlar iÃ§in) veya Ã¼zerine yazÄ±lÄ±yor (manuel_unzip=True olanlar iÃ§in).\n",
        "        # Bu yaklaÅŸÄ±mÄ± koruyalÄ±m, Ã§Ã¼nkÃ¼ Ã¶nceki Ã§Ä±ktÄ±da Ã§alÄ±ÅŸmÄ±ÅŸtÄ±.\n",
        "        if config.get('manual_unzip_needed', False):\n",
        "            print(f\"   -> '{resource_id}' iÃ§in manuel ZIP aÃ§ma iÅŸlemi kontrol ediliyor/yapÄ±lÄ±yor...\")\n",
        "            if not os.path.exists(actual_zip_file_path):\n",
        "                # Bu durum, nltk.download'Ä±n zip'i indirip sildiÄŸi veya hiÃ§ indirmediÄŸi anlamÄ±na gelebilir.\n",
        "                # EÄŸer nltk.download baÅŸarÄ±lÄ± olduysa ve kaynak doÄŸrulandÄ±ysa, zip'in olmamasÄ± sorun deÄŸil.\n",
        "                # Ancak manuel aÃ§ma iÃ§in zip dosyasÄ± gerekiyorsa bu bir sorundur.\n",
        "                # 'punkt_tab' gibi bazÄ± kaynaklar iÃ§in ayrÄ± zip olmayabilir, NLTK'nÄ±n kendi yÃ¶netimine bÄ±rakmak daha iyi.\n",
        "                if 'logger' in globals():\n",
        "                    logger.warning(f\"Manuel aÃ§ma iÃ§in beklenen '{actual_zip_file_path}' ZIP dosyasÄ± bulunamadÄ±. NLTK'nÄ±n kaynaÄŸÄ± zaten aÃ§mÄ±ÅŸ veya farklÄ± bir ÅŸekilde yÃ¶netmiÅŸ olmasÄ± umuluyor.\")\n",
        "                print(f\"   -> âš ï¸ UYARI: Manuel aÃ§ma iÃ§in '{actual_zip_file_path}' ZIP dosyasÄ± bulunamadÄ±. KaynaÄŸÄ±n zaten NLTK tarafÄ±ndan aÃ§Ä±ldÄ±ÄŸÄ± varsayÄ±lÄ±yor.\")\n",
        "            elif os.path.exists(actual_zip_file_path): # Zip dosyasÄ± varsa ve manuel aÃ§ma gerekiyorsa\n",
        "                print(f\"   -> '{actual_zip_file_path}' ZIP dosyasÄ± bulundu.\")\n",
        "                # Hedef dizin zaten varsa ve Ã¼zerine yazÄ±lacaksa, Ã¶nce silmek daha gÃ¼venli olabilir (kodunuzda vardÄ±).\n",
        "                if os.path.exists(unzipped_target_dir_absolute_path):\n",
        "                    print(f\"   -> '{unzipped_target_dir_absolute_path}' hedef dizini zaten mevcut. Manuel aÃ§ma Ã¶ncesi temizleniyor...\")\n",
        "                    # os.system(f\"rm -rf '{unzipped_target_dir_absolute_path}'\") # Bu riskli olabilir, shutil.rmtree daha iyi\n",
        "                    import shutil\n",
        "                    try:\n",
        "                        shutil.rmtree(unzipped_target_dir_absolute_path)\n",
        "                        print(f\"   -> '{unzipped_target_dir_absolute_path}' baÅŸarÄ±yla silindi.\")\n",
        "                    except Exception as e_rm:\n",
        "                        if 'logger' in globals(): logger.error(f\"'{unzipped_target_dir_absolute_path}' silinirken hata: {e_rm}\")\n",
        "                        print(f\"   -> âŒ HATA: '{unzipped_target_dir_absolute_path}' silinirken: {e_rm}\")\n",
        "\n",
        "                print(f\"   -> '{actual_zip_file_path}' dosyasÄ± manuel olarak '{type_dir_absolute_path}' dizinine aÃ§Ä±lÄ±yor...\")\n",
        "                try:\n",
        "                    with zipfile.ZipFile(actual_zip_file_path, 'r') as zip_ref:\n",
        "                        zip_ref.extractall(type_dir_absolute_path)\n",
        "                    print(f\"   -> '{resource_id}' zipfile modÃ¼lÃ¼ ile '{type_dir_absolute_path}' iÃ§ine baÅŸarÄ±yla aÃ§Ä±ldÄ±.\")\n",
        "                    if not os.path.exists(unzipped_target_dir_absolute_path):\n",
        "                         if 'logger' in globals(): logger.warning(f\"Manuel aÃ§ma sonrasÄ± '{unzipped_target_dir_absolute_path}' hala bulunamadÄ±. Zip iÃ§eriÄŸi veya hedef isim kontrol edilmeli.\")\n",
        "                         print(f\"   -> âš ï¸ UYARI: Manuel aÃ§ma sonrasÄ± '{unzipped_target_dir_absolute_path}' hala bulunamadÄ±.\")\n",
        "                except Exception as e_unzip:\n",
        "                    if 'logger' in globals(): logger.error(f\"'{resource_id}' manuel olarak aÃ§Ä±lÄ±rken (zipfile) hata: {e_unzip}\")\n",
        "                    print(f\"   -> âŒ HATA: '{resource_id}' manuel olarak (zipfile) aÃ§Ä±lÄ±rken: {e_unzip}\")\n",
        "        else:\n",
        "            print(f\"   -> '{resource_id}' iÃ§in NLTK'nÄ±n otomatik aÃ§ma iÅŸlemine gÃ¼veniliyor (manual_unzip_needed=False).\")\n",
        "\n",
        "        # 3. AdÄ±m: KaynaÄŸÄ±n NLTK tarafÄ±ndan bulunabildiÄŸini doÄŸrula\n",
        "        print(f\"   -> '{resource_id}' kaynaÄŸÄ±nÄ±n NLTK tarafÄ±ndan ('{config['verification_path']}') bulunabilirliÄŸi doÄŸrulanÄ±yor...\")\n",
        "        nltk.data.find(config['verification_path']) # Bu satÄ±r hata fÄ±rlatÄ±rsa, kaynak bulunamamÄ±ÅŸtÄ±r.\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"'{resource_id}' NLTK kaynaÄŸÄ± baÅŸarÄ±yla doÄŸrulandÄ± (yol: {config['verification_path']}).\")\n",
        "        print(f\"   -> âœ… '{resource_id}' NLTK tarafÄ±ndan baÅŸarÄ±yla bulundu ve doÄŸrulandÄ±.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"'{resource_id}' NLTK kaynaÄŸÄ± iÅŸlenirken genel bir HATA oluÅŸtu: {e}\", exc_info=True) # exc_info=True ile traceback loglanÄ±r\n",
        "        print(f\"   -> âŒ '{resource_id}' NLTK kaynaÄŸÄ± iÅŸlenirken genel bir HATA oluÅŸtu: {e}\")\n",
        "        print(f\"         LÃ¼tfen '{resource_id}' kaynaÄŸÄ±nÄ±n durumunu ve NLTK veri dizinini ('{nltk_data_dir}') manuel olarak kontrol edin.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- NLTK Kaynak Kurulum Ä°ÅŸlemleri TamamlandÄ± ---\")\n",
        "    logger.info(\"BÃ¶lÃ¼m B (NLTK Kaynak YÃ¶netimi) tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"--- NLTK Kaynak Kurulum Ä°ÅŸlemleri TamamlandÄ± ---\")\n",
        "    print(\"BÃ¶lÃ¼m B (NLTK Kaynak YÃ¶netimi) tamamlandÄ±.\")\n",
        "\n",
        "\n",
        "# --- BÃ–LÃœM C: Kurulum SonrasÄ± NLTK Fonksiyonellik Testi ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- BÃ¶lÃ¼m C: Kurulum SonrasÄ± NLTK Fonksiyonellik Testi BaÅŸlatÄ±lÄ±yor ---\")\n",
        "else:\n",
        "    print(\"--- BÃ¶lÃ¼m C: Kurulum SonrasÄ± NLTK Fonksiyonellik Testi BaÅŸlatÄ±lÄ±yor ---\")\n",
        "\n",
        "try:\n",
        "    # `nltk.translate` importlarÄ±, bu metrikler burada veya skorlama hÃ¼cresinde kullanÄ±lacaksa gerekli.\n",
        "    # Åimdilik test amaÃ§lÄ± burada bÄ±rakalÄ±m. AsÄ±l kullanÄ±m HÃ¼cre 9'da.\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "    from nltk.translate.meteor_score import meteor_score # meteor_score iÃ§in wordnet gerekli olabilir.\n",
        "\n",
        "    # Test 1: Stopwords\n",
        "    test_en_stopwords = stopwords.words('english')\n",
        "    test_tr_stopwords = stopwords.words('turkish')\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ã–rnek Ä°ngilizce stopword: '{test_en_stopwords[0]}' (Toplam: {len(test_en_stopwords)})\")\n",
        "        logger.info(f\"Ã–rnek TÃ¼rkÃ§e stopword: '{test_tr_stopwords[0]}' (Toplam: {len(test_tr_stopwords)})\")\n",
        "    else:\n",
        "        print(f\"INFO: Ã–rnek Ä°ngilizce stopword: '{test_en_stopwords[0]}' (Toplam: {len(test_en_stopwords)})\")\n",
        "        print(f\"INFO: Ã–rnek TÃ¼rkÃ§e stopword: '{test_tr_stopwords[0]}' (Toplam: {len(test_tr_stopwords)})\")\n",
        "\n",
        "    # Test 2: Tokenization (Punkt)\n",
        "    test_en_sent = \"Hello world. NLTK is fun.\"\n",
        "    test_tr_sent = \"Merhaba dÃ¼nya. NLTK eÄŸlencelidir.\"\n",
        "    tokenized_en_sents = sent_tokenize(test_en_sent)\n",
        "    tokenized_tr_words = word_tokenize(test_tr_sent, language='turkish')\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ã–rnek Ä°ngilizce cÃ¼mle tokenizasyonu ('{test_en_sent}'): {tokenized_en_sents}\")\n",
        "        logger.info(f\"Ã–rnek TÃ¼rkÃ§e kelime tokenizasyonu ('{test_tr_sent}'): {tokenized_tr_words}\")\n",
        "    else:\n",
        "        print(f\"INFO: Ã–rnek Ä°ngilizce cÃ¼mle tokenizasyonu ('{test_en_sent}'): {tokenized_en_sents}\")\n",
        "        print(f\"INFO: Ã–rnek TÃ¼rkÃ§e kelime tokenizasyonu ('{test_tr_sent}'): {tokenized_tr_words}\")\n",
        "\n",
        "    # Test 3: WordNet ve OMW (Open Multilingual WordNet)\n",
        "    # Bu testler 'wordnet' ve 'omw-1.4' kaynaklarÄ±nÄ±n doÄŸru yÃ¼klendiÄŸini kontrol eder.\n",
        "    from nltk.corpus import wordnet as wn\n",
        "    if 'logger' in globals(): logger.info(\"--- WordNet ve OMW-1.4 Testi ---\")\n",
        "    else: print(\"--- WordNet ve OMW-1.4 Testi ---\")\n",
        "\n",
        "    dog_synsets_eng = wn.synsets('dog', lang='eng')\n",
        "    if dog_synsets_eng:\n",
        "        if 'logger' in globals(): logger.info(f\"WordNet'ten 'dog' (Ä°ngilizce) iÃ§in ilk synset: {dog_synsets_eng[0].name()} - TanÄ±m: {dog_synsets_eng[0].definition()}\")\n",
        "        else: print(f\"INFO: WordNet'ten 'dog' (Ä°ngilizce) iÃ§in ilk synset: {dog_synsets_eng[0].name()} - TanÄ±m: {dog_synsets_eng[0].definition()}\")\n",
        "    else:\n",
        "        if 'logger' in globals(): logger.warning(\"WordNet'ten 'dog' (Ä°ngilizce) iÃ§in synset bulunamadÄ±.\")\n",
        "        else: print(\"WARNING: WordNet'ten 'dog' (Ä°ngilizce) iÃ§in synset bulunamadÄ±.\")\n",
        "\n",
        "    # OMW-1.4 kullanarak baÅŸka bir dilde lemma bulma testi (Ã¶rneÄŸin Ä°spanyolca)\n",
        "    # 'cat' kelimesinin Ä°ngilizce synset'ini bul, sonra Ä°spanyolca karÅŸÄ±lÄ±klarÄ±nÄ± ara\n",
        "    cat_synsets_eng_noun = wn.synsets('cat', pos=wn.NOUN, lang='eng')\n",
        "    if cat_synsets_eng_noun:\n",
        "        first_cat_synset = cat_synsets_eng_noun[0]\n",
        "        lemmas_spanish = first_cat_synset.lemmas(lang='spa') # 'spa' OMW-1.4'ten gelmeli\n",
        "        if lemmas_spanish:\n",
        "            if 'logger' in globals(): logger.info(f\"WordNet/OMW-1.4: 'cat' (Ä°ngilizce synset: {first_cat_synset.name()}) iÃ§in Ä°spanyolca lemmalar: {[lemma.name() for lemma in lemmas_spanish]}\")\n",
        "            else: print(f\"INFO: WordNet/OMW-1.4: 'cat' (Ä°ngilizce synset: {first_cat_synset.name()}) iÃ§in Ä°spanyolca lemmalar: {[lemma.name() for lemma in lemmas_spanish]}\")\n",
        "        else:\n",
        "            if 'logger' in globals(): logger.warning(f\"'cat' iÃ§in Ä°spanyolca lemma bulunamadÄ± (OMW-1.4 kontrol edilmeli). Synset: {first_cat_synset.name()}\")\n",
        "            else: print(f\"WARNING: 'cat' iÃ§in Ä°spanyolca lemma bulunamadÄ± (OMW-1.4 kontrol edilmeli). Synset: {first_cat_synset.name()}\")\n",
        "    else:\n",
        "        if 'logger' in globals(): logger.warning(\"'cat' (isim, Ä°ngilizce) iÃ§in WordNet'te synset bulunamadÄ±.\")\n",
        "        else: print(\"WARNING: 'cat' (isim, Ä°ngilizce) iÃ§in WordNet'te synset bulunamadÄ±.\")\n",
        "\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"NLTK fonksiyonellik testleri baÅŸarÄ±yla tamamlandÄ±.\")\n",
        "    else:\n",
        "        print(\"INFO: NLTK fonksiyonellik testleri baÅŸarÄ±yla tamamlandÄ±.\")\n",
        "\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"NLTK kaynaklarÄ± test edilirken bir hata oluÅŸtu: {e}\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: NLTK kaynaklarÄ± test edilirken bir hata oluÅŸtu: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BÃ¶lÃ¼m C (NLTK Fonksiyonellik Testi) tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"BÃ¶lÃ¼m C (NLTK Fonksiyonellik Testi) tamamlandÄ±.\")\n",
        "\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H3} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H3)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H3} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H3} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "aFKZtxnW0-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 4: Temel KonfigÃ¼rasyonlar ve Google Drive BaÄŸlantÄ±sÄ± (Embedding Modeli: paraphrase-multilingual-mpnet-base-v2)\n",
        "\n",
        "CELL_NAME_H4 = \"4: Temel KonfigÃ¼rasyonlar ve Google Drive BaÄŸlantÄ±sÄ±\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H4)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H4} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H4} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H4} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch # torch import'u HÃ¼cre 3'te zaten var, burada tekrar edilmesinde sakÄ±nca yok\n",
        "# BitsAndBytesConfig ve stopwords HÃ¼cre 3'te import edildi, burada tekrar etmeye gerek yok.\n",
        "# from transformers import BitsAndBytesConfig\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# --- Google Drive BaÄŸlantÄ±sÄ± ve Temel Dizin YapÄ±landÄ±rmasÄ± ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Google Drive baÄŸlantÄ±sÄ± deneniyor...\")\n",
        "else:\n",
        "    print(\"INFO: Google Drive baÄŸlantÄ±sÄ± deneniyor...\")\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount=True her zaman yeniden baÄŸlanmayÄ± zorlar\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Google Drive baÅŸarÄ±yla baÄŸlandÄ±: /content/drive\")\n",
        "    else:\n",
        "        print(\"INFO: Google Drive baÅŸarÄ±yla baÄŸlandÄ±: /content/drive\")\n",
        "\n",
        "    # Ana Ã§alÄ±ÅŸma dizinini tanÄ±mla (Google Drive Ã¼zerinde)\n",
        "    BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab_RAG_Projesi\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ana Ã§alÄ±ÅŸma dizini (BASE_DRIVE_PATH) olarak '{BASE_DRIVE_PATH}' ayarlandÄ±.\")\n",
        "    else:\n",
        "        print(f\"INFO: Ana Ã§alÄ±ÅŸma dizini (BASE_DRIVE_PATH) olarak '{BASE_DRIVE_PATH}' ayarlandÄ±.\")\n",
        "\n",
        "    if not os.path.exists(BASE_DRIVE_PATH):\n",
        "        os.makedirs(BASE_DRIVE_PATH)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana Ã§alÄ±ÅŸma dizini oluÅŸturuldu: {BASE_DRIVE_PATH}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana Ã§alÄ±ÅŸma dizini oluÅŸturuldu: {BASE_DRIVE_PATH}\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana Ã§alÄ±ÅŸma dizini zaten mevcut: {BASE_DRIVE_PATH}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana Ã§alÄ±ÅŸma dizini zaten mevcut: {BASE_DRIVE_PATH}\")\n",
        "\n",
        "    # PDF dosyalarÄ± iÃ§in klasÃ¶r\n",
        "    PDF_KLASORU_ADI = \"PDF_Dosyalari\"\n",
        "    PDF_KLASOR_YOLU = os.path.join(BASE_DRIVE_PATH, PDF_KLASORU_ADI)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyalarÄ± klasÃ¶rÃ¼ (PDF_KLASOR_YOLU) olarak '{PDF_KLASOR_YOLU}' ayarlandÄ±.\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyalarÄ± klasÃ¶rÃ¼ (PDF_KLASOR_YOLU) olarak '{PDF_KLASOR_YOLU}' ayarlandÄ±.\")\n",
        "\n",
        "    if not os.path.exists(PDF_KLASOR_YOLU):\n",
        "        os.makedirs(PDF_KLASOR_YOLU)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF dosyalarÄ± iÃ§in klasÃ¶r oluÅŸturuldu: {PDF_KLASOR_YOLU}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF dosyalarÄ± iÃ§in klasÃ¶r oluÅŸturuldu: {PDF_KLASOR_YOLU}\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF dosyalarÄ± klasÃ¶rÃ¼ zaten mevcut: {PDF_KLASOR_YOLU}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF dosyalarÄ± klasÃ¶rÃ¼ zaten mevcut: {PDF_KLASOR_YOLU}\")\n",
        "\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"Google Drive baÄŸlanÄ±rken veya ana dizinler oluÅŸturulurken HATA: {e}\", exc_info=True)\n",
        "        logger.warning(\"Google Drive baÄŸlanamadÄ±ÄŸÄ± iÃ§in lokal dizinler kullanÄ±lacak. KalÄ±cÄ± depolama olmayacaktÄ±r.\")\n",
        "    else:\n",
        "        print(f\"ERROR: Google Drive baÄŸlanÄ±rken veya ana dizinler oluÅŸturulurken HATA: {e}\")\n",
        "        print(\"WARNING: Google Drive baÄŸlanamadÄ±ÄŸÄ± iÃ§in lokal dizinler kullanÄ±lacak. KalÄ±cÄ± depolama olmayacaktÄ±r.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Google Drive baÄŸlanamazsa fallback olarak lokal dizinler\n",
        "    BASE_DRIVE_PATH = \"./colab_rag_data_local\" # Lokalde Ã§alÄ±ÅŸacaksa, bu dizin oturum sonunda silinir.\n",
        "    PDF_KLASOR_ADI = \"PDF_Dosyalari_local\"\n",
        "    PDF_KLASOR_YOLU = os.path.join(BASE_DRIVE_PATH, PDF_KLASOR_ADI)\n",
        "\n",
        "    if not os.path.exists(BASE_DRIVE_PATH): os.makedirs(BASE_DRIVE_PATH)\n",
        "    if not os.path.exists(PDF_KLASOR_YOLU): os.makedirs(PDF_KLASOR_YOLU)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Lokal ana Ã§alÄ±ÅŸma dizini: {BASE_DRIVE_PATH}\")\n",
        "        logger.info(f\"Lokal PDF dosyalarÄ± klasÃ¶rÃ¼: {PDF_KLASOR_YOLU}\")\n",
        "    else:\n",
        "        print(f\"INFO: Lokal ana Ã§alÄ±ÅŸma dizini: {BASE_DRIVE_PATH}\")\n",
        "        print(f\"INFO: Lokal PDF dosyalarÄ± klasÃ¶rÃ¼: {PDF_KLASOR_YOLU}\")\n",
        "\n",
        "\n",
        "# --- Model ve DiÄŸer KonfigÃ¼rasyonlar ---\n",
        "LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "EMBEDDING_MODEL_ID = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' # Bu model zaten Ã§ok dilli ve iyi bir baÅŸlangÄ±Ã§\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"KullanÄ±lacak LLM Model ID (LLM_MODEL_ID): {LLM_MODEL_ID}\")\n",
        "    logger.info(f\"KullanÄ±lacak Embedding Model ID (EMBEDDING_MODEL_ID): {EMBEDDING_MODEL_ID}\")\n",
        "else:\n",
        "    print(f\"INFO: KullanÄ±lacak LLM Model ID (LLM_MODEL_ID): {LLM_MODEL_ID}\")\n",
        "    print(f\"INFO: KullanÄ±lacak Embedding Model ID (EMBEDDING_MODEL_ID): {EMBEDDING_MODEL_ID}\")\n",
        "\n",
        "# Cache dizini (BASE_DRIVE_PATH altÄ±nda)\n",
        "CACHE_DIR_NAME = \"cache_data\" # Ã–nceki \"cache\" ile karÄ±ÅŸmamasÄ± iÃ§in veya aynÄ± kalabilir\n",
        "CACHE_DIR = os.path.join(BASE_DRIVE_PATH, CACHE_DIR_NAME)\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"Cache dizini (CACHE_DIR) olarak '{CACHE_DIR}' ayarlandÄ±.\")\n",
        "else:\n",
        "    print(f\"INFO: Cache dizini (CACHE_DIR) olarak '{CACHE_DIR}' ayarlandÄ±.\")\n",
        "\n",
        "if not os.path.exists(CACHE_DIR):\n",
        "    os.makedirs(CACHE_DIR)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Cache dizini oluÅŸturuldu: {CACHE_DIR}\")\n",
        "    else:\n",
        "        print(f\"INFO: Cache dizini oluÅŸturuldu: {CACHE_DIR}\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Cache dizini zaten mevcut: {CACHE_DIR}. Ã–NEMLÄ°: EÄŸer embedding modeli, chunking stratejisi gibi temel ayarlar deÄŸiÅŸtiyse, bu cache klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini manuel olarak temizlemeniz Ã¶nerilir.\")\n",
        "    else:\n",
        "        print(f\"INFO: Cache dizini zaten mevcut: {CACHE_DIR}. Ã–NEMLÄ°: EÄŸer embedding modeli, chunking stratejisi gibi temel ayarlar deÄŸiÅŸtiyse, bu cache klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini manuel olarak temizlemeniz Ã¶nerilir.\")\n",
        "\n",
        "\n",
        "# Cihaz AyarÄ±\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"Hesaplamalar iÃ§in kullanÄ±lacak cihaz (DEVICE): {DEVICE}\")\n",
        "else:\n",
        "    print(f\"INFO: Hesaplamalar iÃ§in kullanÄ±lacak cihaz (DEVICE): {DEVICE}\")\n",
        "\n",
        "# TÃ¼rkÃ§e Stopwords (HÃ¼cre 3'te nltk.corpus.stopwords import edildi)\n",
        "turkish_stopwords = []\n",
        "try:\n",
        "    # HÃ¼cre 3'te NLTK kaynaklarÄ± indirildiÄŸi iÃ§in burada doÄŸrudan kullanÄ±labilir olmalÄ±.\n",
        "    from nltk.corpus import stopwords as nltk_stopwords_corpus\n",
        "    turkish_stopwords = list(set(nltk_stopwords_corpus.words('turkish')))\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"TÃ¼rkÃ§e stopwords listesi baÅŸarÄ±yla yÃ¼klendi ({len(turkish_stopwords)} adet). Ä°lk 5: {turkish_stopwords[:5] if turkish_stopwords else 'Liste boÅŸ'}\")\n",
        "    else:\n",
        "        print(f\"INFO: TÃ¼rkÃ§e stopwords listesi baÅŸarÄ±yla yÃ¼klendi ({len(turkish_stopwords)} adet). Ä°lk 5: {turkish_stopwords[:5] if turkish_stopwords else 'Liste boÅŸ'}\")\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"TÃ¼rkÃ§e stopwords yÃ¼klenirken HATA: {e}. Stopword kaldÄ±rma iÅŸlemi etkilenemeyebilir veya hatalÄ± olabilir.\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: TÃ¼rkÃ§e stopwords yÃ¼klenirken HATA: {e}. Stopword kaldÄ±rma iÅŸlemi etkilenemeyebilir veya hatalÄ± olabilir.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    turkish_stopwords = [] # Hata durumunda boÅŸ liste\n",
        "\n",
        "# Quantization Configuration (HÃ¼cre 3'te BitsAndBytesConfig import edildi)\n",
        "quantization_config = None # VarsayÄ±lan olarak None\n",
        "try:\n",
        "    if DEVICE.type == 'cuda':\n",
        "        from transformers import BitsAndBytesConfig # Burada tekrar import etmek gÃ¼venli\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16, # veya torch.bfloat16 (GPU destekliyorsa)\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"CUDA cihazÄ± iÃ§in 4-bit quantization (BitsAndBytesConfig) ayarlandÄ±.\")\n",
        "        else:\n",
        "            print(\"INFO: CUDA cihazÄ± iÃ§in 4-bit quantization (BitsAndBytesConfig) ayarlandÄ±.\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"Cihaz CUDA deÄŸil, bu yÃ¼zden quantization config (BitsAndBytesConfig) ayarlanmadÄ± (None olarak kaldÄ±).\")\n",
        "        else:\n",
        "            print(\"INFO: Cihaz CUDA deÄŸil, bu yÃ¼zden quantization config (BitsAndBytesConfig) ayarlanmadÄ± (None olarak kaldÄ±).\")\n",
        "except ImportError:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"BitsAndBytesConfig import edilemedi. Quantization config ayarlanamadÄ±.\")\n",
        "    else:\n",
        "        print(\"WARNING: BitsAndBytesConfig import edilemedi. Quantization config ayarlanamadÄ±.\")\n",
        "    quantization_config = None\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"Quantization config (BitsAndBytesConfig) ayarlanÄ±rken bir HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: Quantization config (BitsAndBytesConfig) ayarlanÄ±rken bir HATA oluÅŸtu: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    quantization_config = None\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel konfigÃ¼rasyonlar ve Google Drive baÄŸlantÄ±/ayarlarÄ± tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"INFO: Temel konfigÃ¼rasyonlar ve Google Drive baÄŸlantÄ±/ayarlarÄ± tamamlandÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H4} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H4)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H4} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H4} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "K6w7h8Ct1UbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 5: Model YÃ¼kleme (LLM ve GÃ¼ncel Embedding Modeli)\n",
        "\n",
        "CELL_NAME_H5 = \"5: Model YÃ¼kleme (LLM ve Embedding)\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H5)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H5} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H5} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H5} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "# Gerekli importlar (HÃ¼cre 3'te zaten yapÄ±ldÄ±, burada tekrar edilmeleri genellikle sorun olmaz ama gereksiz olabilir)\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "# torch ve BitsAndBytesConfig HÃ¼cre 3 ve 4'te zaten import edildi.\n",
        "\n",
        "# Global deÄŸiÅŸkenlerin (LLM_MODEL_ID, hf_token_to_use, quantization_config, DEVICE, EMBEDDING_MODEL_ID)\n",
        "# Ã¶nceki hÃ¼crelerde tanÄ±mlandÄ±ÄŸÄ±nÄ± varsayÄ±yoruz.\n",
        "\n",
        "# --- LLM Tokenizer YÃ¼kleme ---\n",
        "llm_tokenizer = None\n",
        "if 'LLM_MODEL_ID' in globals() and LLM_MODEL_ID:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"LLM Tokenizer yÃ¼kleniyor: {LLM_MODEL_ID}\")\n",
        "    else:\n",
        "        print(f\"INFO: LLM Tokenizer yÃ¼kleniyor: {LLM_MODEL_ID}\")\n",
        "    try:\n",
        "        # Hugging Face token'Ä±nÄ±n adÄ±nÄ± HÃ¼cre 2'deki ile tutarlÄ± hale getiriyoruz (hf_token_to_use)\n",
        "        current_hf_token = globals().get('hf_token_to_use', None)\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            LLM_MODEL_ID,\n",
        "            token=current_hf_token\n",
        "        )\n",
        "        # Pad token'Ä± kontrolÃ¼ ve ayarlanmasÄ±\n",
        "        if llm_tokenizer.pad_token is None:\n",
        "            if 'logger' in globals():\n",
        "                logger.warning(f\"LLM Tokenizer ({LLM_MODEL_ID}) iÃ§in 'pad_token' tanÄ±msÄ±z. 'eos_token' ({llm_tokenizer.eos_token}) 'pad_token' olarak atanÄ±yor.\")\n",
        "            else:\n",
        "                print(f\"WARNING: LLM Tokenizer ({LLM_MODEL_ID}) iÃ§in 'pad_token' tanÄ±msÄ±z. 'eos_token' ({llm_tokenizer.eos_token}) 'pad_token' olarak atanÄ±yor.\")\n",
        "            llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"LLM Tokenizer ({LLM_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi. Pad token: {llm_tokenizer.pad_token}\")\n",
        "        else:\n",
        "            print(f\"INFO: LLM Tokenizer ({LLM_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi. Pad token: {llm_tokenizer.pad_token}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"LLM Tokenizer ({LLM_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: LLM Tokenizer ({LLM_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        llm_tokenizer = None # Hata durumunda None olarak kalsÄ±n\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"LLM_MODEL_ID tanÄ±mlanmamÄ±ÅŸ. LLM Tokenizer yÃ¼klenemiyor.\")\n",
        "    else:\n",
        "        print(\"ERROR: LLM_MODEL_ID tanÄ±mlanmamÄ±ÅŸ. LLM Tokenizer yÃ¼klenemiyor.\")\n",
        "\n",
        "# --- LLM Model YÃ¼kleme ---\n",
        "llm_model = None\n",
        "if llm_tokenizer and 'LLM_MODEL_ID' in globals() and LLM_MODEL_ID: # Tokenizer baÅŸarÄ±yla yÃ¼klendiyse devam et\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"LLM ({LLM_MODEL_ID}) yÃ¼kleniyor... Bu iÅŸlem zaman alabilir.\")\n",
        "    else:\n",
        "        print(f\"INFO: LLM ({LLM_MODEL_ID}) yÃ¼kleniyor... Bu iÅŸlem zaman alabilir.\")\n",
        "    try:\n",
        "        current_hf_token = globals().get('hf_token_to_use', None)\n",
        "        # Quantization config ve DEVICE HÃ¼cre 4'ten gelmeli\n",
        "        current_quantization_config = globals().get('quantization_config', None)\n",
        "        current_device = globals().get('DEVICE', torch.device(\"cpu\")) # Fallback to CPU if not defined\n",
        "\n",
        "        effective_quant_config = None\n",
        "        if current_device.type == 'cuda' and current_quantization_config:\n",
        "            effective_quant_config = current_quantization_config\n",
        "            if 'logger' in globals(): logger.info(\"CUDA cihazÄ± algÄ±landÄ± ve quantization_config mevcut, LLM iÃ§in kullanÄ±lacak.\")\n",
        "            else: print(\"INFO: CUDA cihazÄ± algÄ±landÄ± ve quantization_config mevcut, LLM iÃ§in kullanÄ±lacak.\")\n",
        "        elif current_device.type == 'cuda' and not current_quantization_config:\n",
        "            if 'logger' in globals(): logger.warning(\"CUDA cihazÄ± algÄ±landÄ± ancak quantization_config mevcut deÄŸil. Model quantize edilmeden yÃ¼klenecek.\")\n",
        "            else: print(\"WARNING: CUDA cihazÄ± algÄ±landÄ± ancak quantization_config mevcut deÄŸil. Model quantize edilmeden yÃ¼klenecek.\")\n",
        "        else: # CPU\n",
        "             if 'logger' in globals(): logger.info(\"Cihaz CPU, quantization_config LLM yÃ¼klemesinde kullanÄ±lmayacak.\")\n",
        "             else: print(\"INFO: Cihaz CPU, quantization_config LLM yÃ¼klemesinde kullanÄ±lmayacak.\")\n",
        "\n",
        "\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_ID,\n",
        "            torch_dtype=torch.float16, # Genellikle iyi bir denge saÄŸlar\n",
        "            device_map=\"auto\", # Modeli uygun cihazlara (GPU/CPU) otomatik daÄŸÄ±tÄ±r\n",
        "            quantization_config=effective_quant_config, # Sadece CUDA'da ve tanÄ±mlÄ±ysa kullanÄ±lÄ±r\n",
        "            token=current_hf_token\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"LLM ({LLM_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "            if hasattr(llm_model, 'device') and next(llm_model.parameters(), None) is not None:\n",
        "                 logger.info(f\"LLM'in yÃ¼klendiÄŸi ana cihaz: {next(llm_model.parameters()).device}\")\n",
        "            elif hasattr(llm_model, 'hf_device_map'):\n",
        "                 logger.info(f\"LLM cihaz haritasÄ± (device_map): {llm_model.hf_device_map}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"INFO: LLM ({LLM_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "            if hasattr(llm_model, 'device') and next(llm_model.parameters(), None) is not None:\n",
        "                 print(f\"INFO: LLM'in yÃ¼klendiÄŸi ana cihaz: {next(llm_model.parameters()).device}\")\n",
        "            elif hasattr(llm_model, 'hf_device_map'):\n",
        "                 print(f\"INFO: LLM cihaz haritasÄ± (device_map): {llm_model.hf_device_map}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"LLM ({LLM_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: LLM ({LLM_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        llm_model = None # Hata durumunda None olarak kalsÄ±n\n",
        "elif not llm_tokenizer :\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"LLM Tokenizer yÃ¼klenemediÄŸi iÃ§in LLM ({LLM_MODEL_ID if 'LLM_MODEL_ID' in globals() else 'Bilinmeyen'}) yÃ¼kleme iÅŸlemi atlandÄ±.\")\n",
        "    else:\n",
        "        print(f\"ERROR: LLM Tokenizer yÃ¼klenemediÄŸi iÃ§in LLM ({LLM_MODEL_ID if 'LLM_MODEL_ID' in globals() else 'Bilinmeyen'}) yÃ¼kleme iÅŸlemi atlandÄ±.\")\n",
        "\n",
        "# --- Ana Embedding Modeli (SentenceTransformer) YÃ¼kleme ---\n",
        "embedding_model_st = None # SentenceTransformer instance\n",
        "embedding_dimension = None\n",
        "if 'EMBEDDING_MODEL_ID' in globals() and EMBEDDING_MODEL_ID:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ana Embedding Modeli (SentenceTransformer) yÃ¼kleniyor: {EMBEDDING_MODEL_ID}\")\n",
        "    else:\n",
        "        print(f\"INFO: Ana Embedding Modeli (SentenceTransformer) yÃ¼kleniyor: {EMBEDDING_MODEL_ID}\")\n",
        "    try:\n",
        "        current_device_for_emb = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "        embedding_model_st = SentenceTransformer(EMBEDDING_MODEL_ID, device=current_device_for_emb)\n",
        "        # Embedding boyutunu al\n",
        "        if hasattr(embedding_model_st, 'get_sentence_embedding_dimension'):\n",
        "            embedding_dimension = embedding_model_st.get_sentence_embedding_dimension()\n",
        "        else: # Eski versiyonlar veya farklÄ± SentenceTransformer modelleri iÃ§in fallback\n",
        "            try:\n",
        "                test_emb = embedding_model_st.encode(\"test\")\n",
        "                embedding_dimension = test_emb.shape[-1]\n",
        "            except Exception as e_dim:\n",
        "                 if 'logger' in globals(): logger.warning(f\"Embedding boyutu otomatik algÄ±lanamadÄ±: {e_dim}\")\n",
        "                 else: print(f\"WARNING: Embedding boyutu otomatik algÄ±lanamadÄ±: {e_dim}\")\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi. Cihaz: {embedding_model_st.device}, Embedding Boyutu: {embedding_dimension if embedding_dimension else 'Bilinmiyor'}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) baÅŸarÄ±yla yÃ¼klendi. Cihaz: {embedding_model_st.device}, Embedding Boyutu: {embedding_dimension if embedding_dimension else 'Bilinmiyor'}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) yÃ¼klenirken HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        embedding_model_st = None\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"EMBEDDING_MODEL_ID tanÄ±mlanmamÄ±ÅŸ. Ana Embedding Modeli yÃ¼klenemiyor.\")\n",
        "    else:\n",
        "        print(\"ERROR: EMBEDDING_MODEL_ID tanÄ±mlanmamÄ±ÅŸ. Ana Embedding Modeli yÃ¼klenemiyor.\")\n",
        "\n",
        "\n",
        "# --- Langchain uyumlu Embedding Modeli OluÅŸturma (SemanticChunker iÃ§in) ---\n",
        "langchain_embeddings_for_semantic_chunker = None\n",
        "if embedding_model_st and 'EMBEDDING_MODEL_ID' in globals() and EMBEDDING_MODEL_ID: # Ana model baÅŸarÄ±yla yÃ¼klendiyse\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Langchain uyumlu Embedding Modeli '{EMBEDDING_MODEL_ID}' kullanÄ±larak SentenceTransformerEmbeddings ile oluÅŸturuluyor.\")\n",
        "    else:\n",
        "        print(f\"INFO: Langchain uyumlu Embedding Modeli '{EMBEDDING_MODEL_ID}' kullanÄ±larak SentenceTransformerEmbeddings ile oluÅŸturuluyor.\")\n",
        "    try:\n",
        "        from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings # Burada tekrar import gÃ¼venli\n",
        "        current_device_for_lc_emb = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "        langchain_embeddings_for_semantic_chunker = SentenceTransformerEmbeddings(\n",
        "            model_name=EMBEDDING_MODEL_ID,\n",
        "            model_kwargs={'device': current_device_for_lc_emb}\n",
        "            # encode_kwargs={'normalize_embeddings': True} # BazÄ± durumlarda faydalÄ± olabilir, cosine similarity iÃ§in normalizasyon Ã¶nerilir.\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Langchain uyumlu Embedding Modeli (SentenceTransformerEmbeddings) baÅŸarÄ±yla oluÅŸturuldu. Cihaz: {current_device_for_lc_emb}\")\n",
        "        else:\n",
        "            print(f\"INFO: Langchain uyumlu Embedding Modeli (SentenceTransformerEmbeddings) baÅŸarÄ±yla oluÅŸturuldu. Cihaz: {current_device_for_lc_emb}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Langchain uyumlu Embedding Modeli oluÅŸturulurken HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Langchain uyumlu Embedding Modeli oluÅŸturulurken HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        langchain_embeddings_for_semantic_chunker = None\n",
        "elif not embedding_model_st:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(f\"Ana embedding modeli (embedding_model_st) yÃ¼klenemediÄŸi iÃ§in Langchain uyumlu embedding modeli oluÅŸturulamadÄ±.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Ana embedding modeli (embedding_model_st) yÃ¼klenemediÄŸi iÃ§in Langchain uyumlu embedding modeli oluÅŸturulamadÄ±.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"TÃ¼m model yÃ¼kleme/oluÅŸturma denemeleri tamamlandÄ±.\")\n",
        "else:\n",
        "    print(\"INFO: TÃ¼m model yÃ¼kleme/oluÅŸturma denemeleri tamamlandÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H5} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H5)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H5} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H5} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "Hj9BcCuv1sfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 6: YardÄ±mcÄ± Fonksiyonlar (GÃœNCELLENDÄ° - Loglar DÃ¼zenlendi)\n",
        "\n",
        "CELL_NAME_H6 = \"6: YardÄ±mcÄ± Fonksiyonlar\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H6)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H6} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H6} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H6} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "import hashlib\n",
        "import pickle\n",
        "import os\n",
        "import pdfplumber\n",
        "import re\n",
        "# nltk.tokenize.word_tokenize, RecursiveCharacterTextSplitter, SemanticChunker HÃ¼cre 3'te import edildi.\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# --- Dosya Hashleme Fonksiyonu ---\n",
        "def generate_file_hash(file_path):\n",
        "    \"\"\"Verilen dosyanÄ±n MD5 hash'ini oluÅŸturur.\"\"\"\n",
        "    hasher = hashlib.md5()\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            buf = f.read()\n",
        "            hasher.update(buf)\n",
        "        file_hash = hasher.hexdigest()\n",
        "        if 'logger' in globals():\n",
        "            logger.debug(f\"'{os.path.basename(file_path)}' iÃ§in MD5 hash oluÅŸturuldu: {file_hash}\")\n",
        "        return file_hash\n",
        "    except FileNotFoundError:\n",
        "        if 'logger' in globals():\n",
        "            logger.warning(f\"Hash oluÅŸturmak iÃ§in dosya bulunamadÄ±: {file_path}\")\n",
        "        else:\n",
        "            print(f\"WARNING: Hash oluÅŸturmak iÃ§in dosya bulunamadÄ±: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"'{file_path}' iÃ§in dosya hash'i oluÅŸturulurken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: '{file_path}' iÃ§in dosya hash'i oluÅŸturulurken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- Cache Kaydetme ve YÃ¼kleme FonksiyonlarÄ± ---\n",
        "def save_chunks_and_index(chunks_to_save, faiss_index_to_save, file_identifier_prefix, cache_directory=None):\n",
        "    \"\"\"ParÃ§alarÄ± (chunks) ve FAISS indeksini Ã¶nbelleÄŸe kaydeder.\"\"\"\n",
        "    final_cache_dir = cache_directory if cache_directory else globals().get('CACHE_DIR', './cache_default')\n",
        "    if not os.path.exists(final_cache_dir):\n",
        "        try:\n",
        "            os.makedirs(final_cache_dir)\n",
        "            if 'logger' in globals(): logger.info(f\"Ã–nbellek dizini oluÅŸturuldu: {final_cache_dir}\")\n",
        "            else: print(f\"INFO: Ã–nbellek dizini oluÅŸturuldu: {final_cache_dir}\")\n",
        "        except Exception as e_mkdir:\n",
        "            if 'logger' in globals(): logger.error(f\"Ã–nbellek dizini ({final_cache_dir}) oluÅŸturulamadÄ±: {e_mkdir}. KayÄ±t yapÄ±lamayacak.\", exc_info=True)\n",
        "            else: print(f\"ERROR: Ã–nbellek dizini ({final_cache_dir}) oluÅŸturulamadÄ±: {e_mkdir}. KayÄ±t yapÄ±lamayacak.\")\n",
        "            return\n",
        "\n",
        "    chunk_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_chunks.pkl\")\n",
        "    index_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_index.faiss\")\n",
        "\n",
        "    try:\n",
        "        with open(chunk_file_path, 'wb') as f:\n",
        "            pickle.dump(chunks_to_save, f)\n",
        "        faiss.write_index(faiss_index_to_save, index_file_path)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"ParÃ§alar ve FAISS indeksi baÅŸarÄ±yla Ã¶nbelleÄŸe kaydedildi. Prefix: '{file_identifier_prefix}', Dizine: {final_cache_dir}\")\n",
        "        else:\n",
        "            print(f\"INFO: ParÃ§alar ve FAISS indeksi baÅŸarÄ±yla Ã¶nbelleÄŸe kaydedildi. Prefix: '{file_identifier_prefix}', Dizine: {final_cache_dir}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Ã–nbellek (prefix: {file_identifier_prefix}) kaydedilirken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Ã–nbellek (prefix: {file_identifier_prefix}) kaydedilirken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "def load_chunks_and_index(file_identifier_prefix, cache_directory=None):\n",
        "    \"\"\"ParÃ§alarÄ± (chunks) ve FAISS indeksini Ã¶nbellekten yÃ¼kler.\"\"\"\n",
        "    final_cache_dir = cache_directory if cache_directory else globals().get('CACHE_DIR', './cache_default')\n",
        "    chunk_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_chunks.pkl\")\n",
        "    index_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_index.faiss\")\n",
        "\n",
        "    if os.path.exists(chunk_file_path) and os.path.exists(index_file_path):\n",
        "        try:\n",
        "            with open(chunk_file_path, 'rb') as f:\n",
        "                loaded_chunks = pickle.load(f)\n",
        "            loaded_index = faiss.read_index(index_file_path)\n",
        "            if 'logger' in globals():\n",
        "                logger.info(f\"Ã–nbellek (prefix: {file_identifier_prefix}) baÅŸarÄ±yla yÃ¼klendi. {len(loaded_chunks) if loaded_chunks else 0} chunk, Ä°ndeks yÃ¼klendi: {'Evet' if loaded_index else 'HayÄ±r'}.\")\n",
        "            else:\n",
        "                print(f\"INFO: Ã–nbellek (prefix: {file_identifier_prefix}) baÅŸarÄ±yla yÃ¼klendi. {len(loaded_chunks) if loaded_chunks else 0} chunk, Ä°ndeks yÃ¼klendi: {'Evet' if loaded_index else 'HayÄ±r'}.\")\n",
        "            return loaded_chunks, loaded_index\n",
        "        except Exception as e:\n",
        "            if 'logger' in globals():\n",
        "                logger.error(f\"Ã–nbellek (prefix: {file_identifier_prefix}) yÃ¼klenirken HATA: {e}. Bozuk Ã¶nbellek dosyalarÄ± silinip yeniden oluÅŸturulacak.\", exc_info=True)\n",
        "            else:\n",
        "                print(f\"ERROR: Ã–nbellek (prefix: {file_identifier_prefix}) yÃ¼klenirken HATA: {e}. Bozuk Ã¶nbellek dosyalarÄ± silinip yeniden oluÅŸturulacak.\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            # Hata durumunda bozuk olabilecek cache dosyalarÄ±nÄ± silmeyi dene\n",
        "            if os.path.exists(chunk_file_path):\n",
        "                try: os.remove(chunk_file_path)\n",
        "                except Exception as e_rm_c:\n",
        "                    if 'logger' in globals(): logger.error(f\"Bozuk cache chunk dosyasÄ± ({chunk_file_path}) silinirken HATA: {e_rm_c}\")\n",
        "                    else: print(f\"ERROR: Bozuk cache chunk dosyasÄ± ({chunk_file_path}) silinirken HATA: {e_rm_c}\")\n",
        "            if os.path.exists(index_file_path):\n",
        "                try: os.remove(index_file_path)\n",
        "                except Exception as e_rm_i:\n",
        "                    if 'logger' in globals(): logger.error(f\"Bozuk cache index dosyasÄ± ({index_file_path}) silinirken HATA: {e_rm_i}\")\n",
        "                    else: print(f\"ERROR: Bozuk cache index dosyasÄ± ({index_file_path}) silinirken HATA: {e_rm_i}\")\n",
        "            return None, None\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ã–nbellek (prefix: {file_identifier_prefix}) bulunamadÄ±. Veri yeniden iÅŸlenecek.\")\n",
        "        else:\n",
        "            print(f\"INFO: Ã–nbellek (prefix: {file_identifier_prefix}) bulunamadÄ±. Veri yeniden iÅŸlenecek.\")\n",
        "        return None, None\n",
        "\n",
        "# --- PDF Metin Ã‡Ä±karma Fonksiyonu ---\n",
        "def pdf_to_text(pdf_path):\n",
        "    \"\"\"Verilen PDF dosyasÄ±ndan metin iÃ§eriÄŸini Ã§Ä±karÄ±r.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyasÄ±ndan metin Ã§Ä±karÄ±lÄ±yor: {os.path.basename(pdf_path)}\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyasÄ±ndan metin Ã§Ä±karÄ±lÄ±yor: {os.path.basename(pdf_path)}\")\n",
        "    extracted_text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    extracted_text += page_text + \"\\n\" # Sayfalar arasÄ±na yeni satÄ±r ekle\n",
        "                else:\n",
        "                    if 'logger' in globals(): logger.debug(f\"PDF: {os.path.basename(pdf_path)}, Sayfa {i+1} metin iÃ§ermiyor veya Ã§Ä±karÄ±lamadÄ±.\")\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF metin Ã§Ä±karma tamamlandÄ±. '{os.path.basename(pdf_path)}' iÃ§in toplam karakter sayÄ±sÄ±: {len(extracted_text)}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF metin Ã§Ä±karma tamamlandÄ±. '{os.path.basename(pdf_path)}' iÃ§in toplam karakter sayÄ±sÄ±: {len(extracted_text)}\")\n",
        "        return extracted_text\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"PDF ({os.path.basename(pdf_path)}) okunurken/iÅŸlenirken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: PDF ({os.path.basename(pdf_path)}) okunurken/iÅŸlenirken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return \"\" # Hata durumunda boÅŸ string dÃ¶n\n",
        "\n",
        "# --- Metin Ã–n Ä°ÅŸleme Fonksiyonu ---\n",
        "def preprocess_text(text_input):\n",
        "    \"\"\"GiriÅŸ metnini Ã¶n iÅŸler: kÃ¼Ã§Ã¼k harf, boÅŸluk normalizasyonu, URL/hashtag temizliÄŸi, tokenizasyon ve stopword kaldÄ±rma.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Ã–n iÅŸleme iÃ§in geÃ§ersiz metin giriÅŸi (boÅŸ veya string deÄŸil). BoÅŸ string dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return \"\"\n",
        "\n",
        "    # KÃ¼Ã§Ã¼k harfe Ã§evirme\n",
        "    processed_text = text_input.lower()\n",
        "    # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa indirgeme ve baÅŸtaki/sondaki boÅŸluklarÄ± temizleme\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "    # URL, e-posta ve hashtag'leri kaldÄ±rma (daha kapsamlÄ± regex'ler eklenebilir)\n",
        "    processed_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', processed_text, flags=re.MULTILINE)\n",
        "    processed_text = re.sub(r'\\S*@\\S*\\s?', '', processed_text) # E-posta\n",
        "    processed_text = re.sub(r'#\\S+', '', processed_text) # Hashtag\n",
        "    # SayÄ±sal olmayan karakterleri ve temel noktalama iÅŸaretlerini koru, diÄŸerlerini temizle (isteÄŸe baÄŸlÄ±)\n",
        "    # processed_text = re.sub(r'[^a-z0-9ÄŸÃ¼ÅŸÄ±Ã¶Ã§\\s\\.,!?]', '', processed_text) # Bu Ã§ok agresif olabilir.\n",
        "\n",
        "    # Tokenize etme (HÃ¼cre 3'te nltk.tokenize.word_tokenize import edildi)\n",
        "    try:\n",
        "        from nltk.tokenize import word_tokenize # GÃ¼venlik iÃ§in tekrar import\n",
        "        tokens = word_tokenize(processed_text, language='turkish')\n",
        "    except Exception as e_tok:\n",
        "        if 'logger' in globals(): logger.error(f\"Metin tokenizasyonu sÄ±rasÄ±nda hata: {e_tok}. Ham metin kullanÄ±lacak.\", exc_info=True)\n",
        "        else: print(f\"ERROR: Metin tokenizasyonu sÄ±rasÄ±nda hata: {e_tok}. Ham metin kullanÄ±lacak.\")\n",
        "        tokens = processed_text.split() # Fallback olarak basit split\n",
        "\n",
        "    # Stopword'leri ve kÄ±sa/alfanÃ¼merik olmayan token'larÄ± kaldÄ±r\n",
        "    # turkish_stopwords global deÄŸiÅŸkeni HÃ¼cre 4'te tanÄ±mlandÄ±.\n",
        "    current_turkish_stopwords = globals().get('turkish_stopwords', [])\n",
        "    if not isinstance(current_turkish_stopwords, list): current_turkish_stopwords = []\n",
        "\n",
        "    filtered_tokens = []\n",
        "    if current_turkish_stopwords: # EÄŸer stopword listesi varsa\n",
        "        for word in tokens:\n",
        "            if word.isalpha() and len(word) > 1 and word not in current_turkish_stopwords:\n",
        "                filtered_tokens.append(word)\n",
        "    else: # Stopword listesi yoksa, sadece alfanÃ¼merik ve uzunluk kontrolÃ¼\n",
        "        for word in tokens:\n",
        "            if word.isalpha() and len(word) > 1:\n",
        "                filtered_tokens.append(word)\n",
        "\n",
        "    final_text = ' '.join(filtered_tokens)\n",
        "    if 'logger' in globals():\n",
        "        logger.debug(f\"Metin Ã¶n iÅŸleme tamamlandÄ±. Orijinal (ilk 50): '{text_input[:50]}...' -> Ä°ÅŸlenmiÅŸ (ilk 50): '{final_text[:50]}...'\")\n",
        "    return final_text\n",
        "\n",
        "# --- Metin ParÃ§alama FonksiyonlarÄ± ---\n",
        "def chunk_text_recursive(text_to_chunk, chunk_size=1000, chunk_overlap=150):\n",
        "    \"\"\"Metni RecursiveCharacterTextSplitter kullanarak parÃ§alar.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Recursive metin parÃ§alama baÅŸlatÄ±lÄ±yor. Ayarlar: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
        "    else:\n",
        "        print(f\"INFO: Recursive metin parÃ§alama baÅŸlatÄ±lÄ±yor. Ayarlar: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
        "\n",
        "    if not text_to_chunk or not isinstance(text_to_chunk, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Recursive parÃ§alama iÃ§in geÃ§ersiz metin (boÅŸ veya string deÄŸil). BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return []\n",
        "    try:\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter # GÃ¼venlik iÃ§in tekrar import\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"] # Daha fazla ayÄ±rÄ±cÄ± eklenebilir\n",
        "        )\n",
        "        chunks = text_splitter.split_text(text_to_chunk)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Metin (Recursive) {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼.\")\n",
        "        else:\n",
        "            print(f\"INFO: Metin (Recursive) {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Recursive metin parÃ§alama sÄ±rasÄ±nda HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Recursive metin parÃ§alama sÄ±rasÄ±nda HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "def chunk_text_semantic(text_to_chunk, langchain_embedding_model_instance,\n",
        "                        breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=95):\n",
        "    \"\"\"Metni SemanticChunker kullanarak anlamsal olarak parÃ§alar.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Anlamsal metin parÃ§alama baÅŸlatÄ±lÄ±yor. Ayarlar: threshold_type='{breakpoint_threshold_type}', threshold_amount='{breakpoint_threshold_amount}'\")\n",
        "    else:\n",
        "        print(f\"INFO: Anlamsal metin parÃ§alama baÅŸlatÄ±lÄ±yor. Ayarlar: threshold_type='{breakpoint_threshold_type}', threshold_amount='{breakpoint_threshold_amount}'\")\n",
        "\n",
        "    if not text_to_chunk or not isinstance(text_to_chunk, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Anlamsal parÃ§alama iÃ§in geÃ§ersiz metin (boÅŸ veya string deÄŸil). BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return []\n",
        "    if langchain_embedding_model_instance is None:\n",
        "        if 'logger' in globals(): logger.error(\"Anlamsal parÃ§alama iÃ§in Langchain embedding modeli (langchain_embedding_model_instance) saÄŸlanmadÄ±. BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        else: print(\"ERROR: Anlamsal parÃ§alama iÃ§in Langchain embedding modeli (langchain_embedding_model_instance) saÄŸlanmadÄ±. BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        from langchain_experimental.text_splitter import SemanticChunker # GÃ¼venlik iÃ§in tekrar import\n",
        "        # breakpoint_threshold_type iÃ§in \"standard_deviation\", \"interquartile\" gibi seÃ§enekler de mevcut.\n",
        "        # \"percentile\" genellikle iyi bir baÅŸlangÄ±Ã§tÄ±r.\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=langchain_embedding_model_instance,\n",
        "            breakpoint_threshold_type=breakpoint_threshold_type\n",
        "            # EÄŸer percentile ise, SemanticChunker bunu __init__'te doÄŸrudan almÄ±yor olabilir,\n",
        "            # doÄŸrudan _breakpoint_threshold_amount olarak ayarlanabilir veya instance oluÅŸturulduktan sonra.\n",
        "            # DÃ¶kÃ¼mantasyona gÃ¶re, breakpoint_threshold_type=\"percentile\" ise,\n",
        "            # percentile_threshold parametresi __init__'e verilebilir veya sonradan ayarlanabilir.\n",
        "            # Åimdiki kodunuzda (HÃ¼cre 10'da) create_documents sonrasÄ± set ediliyordu, bu da bir yÃ¶ntem.\n",
        "            # Ya da doÄŸrudan __init__ iÃ§inde deneyelim, eÄŸer varsa.\n",
        "            # **{'percentile_threshold': breakpoint_threshold_amount} if breakpoint_threshold_type == \"percentile\" else {} # Bu ÅŸekilde denenebilir\n",
        "        )\n",
        "        # EÄŸer __init__ percentile_threshold almÄ±yorsa, sonradan ayarlama (Ã¶nceki kodunuzdaki gibi)\n",
        "        if breakpoint_threshold_type == \"percentile\" and hasattr(semantic_splitter, 'percentile_threshold'):\n",
        "           semantic_splitter.percentile_threshold = breakpoint_threshold_amount\n",
        "        elif breakpoint_threshold_type != \"percentile\" and hasattr(semantic_splitter, breakpoint_threshold_type + '_threshold'):\n",
        "            # DiÄŸer threshold tipleri iÃ§in (Ã¶rn: standard_deviation_threshold)\n",
        "            # setattr(semantic_splitter, breakpoint_threshold_type + '_threshold', breakpoint_threshold_amount)\n",
        "            # Ancak bu SemanticChunker'Ä±n iÃ§ yapÄ±sÄ±na baÄŸlÄ±, dÃ¶kÃ¼mantasyon kontrol edilmeli.\n",
        "            # Åimdilik sadece percentile iÃ§in aÃ§Ä±k ayar bÄ±rakalÄ±m.\n",
        "            pass\n",
        "\n",
        "\n",
        "        # SemanticChunker Langchain Document objeleri listesi bekler ve Document listesi dÃ¶ndÃ¼rÃ¼r.\n",
        "        # Biz tek bir metinle Ã§alÄ±ÅŸÄ±yorsak, onu bir Document iÃ§ine almalÄ±yÄ±z.\n",
        "        from langchain_core.documents import Document\n",
        "        documents_to_split = [Document(page_content=text_to_chunk)]\n",
        "\n",
        "        # split_documents metodu kullanÄ±lmalÄ±\n",
        "        semantic_docs = semantic_splitter.split_documents(documents_to_split)\n",
        "        chunks = [doc.page_content for doc in semantic_docs]\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Metin (Anlamsal) {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼.\")\n",
        "        else:\n",
        "            print(f\"INFO: Metin (Anlamsal) {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Anlamsal metin parÃ§alama sÄ±rasÄ±nda HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Anlamsal metin parÃ§alama sÄ±rasÄ±nda HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"YardÄ±mcÄ± fonksiyonlar tanÄ±mlandÄ±.\")\n",
        "else:\n",
        "    print(\"INFO: YardÄ±mcÄ± fonksiyonlar tanÄ±mlandÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H6} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H6)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H6} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H6} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "KgYshIl02cLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 7: FAISS Ä°ndeksi OluÅŸturma ve Arama FonksiyonlarÄ± (GÃœNCELLENDÄ° - Loglar DÃ¼zenlendi)\n",
        "\n",
        "CELL_NAME_H7 = \"7: FAISS Ä°ndeksi OluÅŸturma ve Arama FonksiyonlarÄ±\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H7)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H7} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H7} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H7} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "import faiss\n",
        "import numpy as np # HÃ¼cre 3'te import edildi, burada tekrar edilmesinde sakÄ±nca yok\n",
        "\n",
        "# --- FAISS Ä°ndeksi OluÅŸturma Fonksiyonu ---\n",
        "def build_faiss_index(chunks_to_index, sentence_transformer_embedding_model):\n",
        "    \"\"\"\n",
        "    Verilen metin parÃ§alarÄ±ndan (chunks) bir FAISS indeksi oluÅŸturur.\n",
        "    KullanÄ±lan embedding modeli bir SentenceTransformer instance olmalÄ±dÄ±r.\n",
        "    \"\"\"\n",
        "    num_chunks = len(chunks_to_index) if chunks_to_index else 0\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"FAISS indeksi oluÅŸturma iÅŸlemi baÅŸlatÄ±lÄ±yor ({num_chunks} adet chunk iÃ§in)...\")\n",
        "    else:\n",
        "        print(f\"INFO: FAISS indeksi oluÅŸturma iÅŸlemi baÅŸlatÄ±lÄ±yor ({num_chunks} adet chunk iÃ§in)...\")\n",
        "\n",
        "    if not chunks_to_index:\n",
        "        if 'logger' in globals():\n",
        "            logger.warning(\"FAISS indeksi oluÅŸturmak iÃ§in chunk listesi boÅŸ. Ä°ÅŸlem durduruldu.\")\n",
        "        else:\n",
        "            print(\"WARNING: FAISS indeksi oluÅŸturmak iÃ§in chunk listesi boÅŸ. Ä°ÅŸlem durduruldu.\")\n",
        "        return None, None # index, embeddings_np\n",
        "\n",
        "    if sentence_transformer_embedding_model is None:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(\"FAISS indeksi oluÅŸturmak iÃ§in embedding modeli (sentence_transformer_embedding_model) saÄŸlanmadÄ±. Ä°ÅŸlem durduruldu.\")\n",
        "        else:\n",
        "            print(\"ERROR: FAISS indeksi oluÅŸturmak iÃ§in embedding modeli (sentence_transformer_embedding_model) saÄŸlanmadÄ±. Ä°ÅŸlem durduruldu.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # 1. AdÄ±m: Chunk'larÄ±n embedding'lerini al\n",
        "        # SentenceTransformer.encode() metodu numpy array veya tensor dÃ¶ndÃ¼rebilir.\n",
        "        # convert_to_tensor=True ile tensor alÄ±p sonra numpy'a Ã§evirmek tutarlÄ± bir yol.\n",
        "        if 'logger' in globals(): logger.debug(\"Chunk embedding'leri hesaplanÄ±yor...\")\n",
        "        chunk_embeddings_tensor = sentence_transformer_embedding_model.encode(\n",
        "            chunks_to_index,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True # Uzun listeler iÃ§in ilerleme Ã§ubuÄŸu faydalÄ± olabilir\n",
        "        )\n",
        "        chunk_embeddings_np = chunk_embeddings_tensor.cpu().numpy().astype('float32') # FAISS float32 bekler\n",
        "        if 'logger' in globals(): logger.debug(f\"Chunk embedding'leri hesaplandÄ±. Åekil: {chunk_embeddings_np.shape}\")\n",
        "\n",
        "\n",
        "        # Embedding'lerin geÃ§erliliÄŸini kontrol et\n",
        "        if chunk_embeddings_np.shape[0] == 0 or \\\n",
        "           len(chunk_embeddings_np.shape) < 2 or \\\n",
        "           chunk_embeddings_np.shape[1] == 0:\n",
        "            if 'logger' in globals():\n",
        "                logger.error(f\"Hesaplanan chunk embedding'lerinin boyutu geÃ§ersiz: {chunk_embeddings_np.shape}. FAISS indeksi kurulamÄ±yor.\")\n",
        "            else:\n",
        "                print(f\"ERROR: Hesaplanan chunk embedding'lerinin boyutu geÃ§ersiz: {chunk_embeddings_np.shape}. FAISS indeksi kurulamÄ±yor.\")\n",
        "            return None, None\n",
        "\n",
        "        embedding_dimension = chunk_embeddings_np.shape[1]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Chunk embedding'leri baÅŸarÄ±yla oluÅŸturuldu. Boyut (dimension): {embedding_dimension}, VektÃ¶r sayÄ±sÄ±: {chunk_embeddings_np.shape[0]}\")\n",
        "        else:\n",
        "            print(f\"INFO: Chunk embedding'leri baÅŸarÄ±yla oluÅŸturuldu. Boyut (dimension): {embedding_dimension}, VektÃ¶r sayÄ±sÄ±: {chunk_embeddings_np.shape[0]}\")\n",
        "\n",
        "        # 2. AdÄ±m: FAISS Ä°ndeksi OluÅŸtur\n",
        "        # IndexFlatL2, L2 mesafesini (Ã–klid) kullanÄ±r. Normalize edilmiÅŸ embedding'ler iÃ§in kosinÃ¼s benzerliÄŸine denktir.\n",
        "        # SentenceTransformer modelleri genellikle normalize edilmiÅŸ embedding'ler dÃ¶ndÃ¼rÃ¼r.\n",
        "        faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
        "        faiss_index.add(chunk_embeddings_np) # Embedding'leri indekse ekle\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"FAISS indeksi (IndexFlatL2) baÅŸarÄ±yla oluÅŸturuldu. Toplam {faiss_index.ntotal} vektÃ¶r eklendi.\")\n",
        "        else:\n",
        "            print(f\"INFO: FAISS indeksi (IndexFlatL2) baÅŸarÄ±yla oluÅŸturuldu. Toplam {faiss_index.ntotal} vektÃ¶r eklendi.\")\n",
        "\n",
        "        return faiss_index, chunk_embeddings_np # Ä°ndeksi ve ayrÄ±ca embedding'leri dÃ¶ndÃ¼r (bazÄ± senaryolarda gerekebilir)\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"FAISS indeksi oluÅŸturulurken HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: FAISS indeksi oluÅŸturulurken HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# --- FAISS Ä°ndeksinde Arama Yapma Fonksiyonu ---\n",
        "def retrieve_relevant_chunks(query_text, faiss_index_instance, original_indexed_chunks,\n",
        "                             sentence_transformer_embedding_model, top_k=5):\n",
        "    \"\"\"\n",
        "    Verilen sorgu metni iÃ§in FAISS indeksinde arama yaparak en ilgili chunk'larÄ± getirir.\n",
        "    \"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"FAISS indeksinde ilgili chunk'lar aranÄ±yor. Top K: {top_k}, Sorgu (ilk 50 karakter): '{query_text[:50]}...'\")\n",
        "    else:\n",
        "        print(f\"INFO: FAISS indeksinde ilgili chunk'lar aranÄ±yor. Top K: {top_k}, Sorgu (ilk 50 karakter): '{query_text[:50]}...'\")\n",
        "\n",
        "    if not faiss_index_instance:\n",
        "        if 'logger' in globals(): logger.error(\"Arama iÃ§in FAISS indeksi (faiss_index_instance) saÄŸlanmadÄ±.\")\n",
        "        else: print(\"ERROR: Arama iÃ§in FAISS indeksi (faiss_index_instance) saÄŸlanmadÄ±.\")\n",
        "        return []\n",
        "    if not sentence_transformer_embedding_model:\n",
        "        if 'logger' in globals(): logger.error(\"Arama iÃ§in embedding modeli (sentence_transformer_embedding_model) saÄŸlanmadÄ±.\")\n",
        "        else: print(\"ERROR: Arama iÃ§in embedding modeli (sentence_transformer_embedding_model) saÄŸlanmadÄ±.\")\n",
        "        return []\n",
        "    if not original_indexed_chunks:\n",
        "        if 'logger' in globals(): logger.warning(\"Arama sonuÃ§larÄ±nÄ± eÅŸleÅŸtirmek iÃ§in orijinal chunk listesi (original_indexed_chunks) boÅŸ.\")\n",
        "        # BoÅŸ liste dÃ¶ndÃ¼rmek yerine, bu durumda da arama yapÄ±labilir, ancak sonuÃ§lar sadece indis olurdu.\n",
        "        # Mevcut mantÄ±k, orijinal chunk'larÄ± dÃ¶ndÃ¼rmek Ã¼zerine kurulu olduÄŸu iÃ§in boÅŸ liste daha uygun.\n",
        "        else: print(\"WARNING: Arama sonuÃ§larÄ±nÄ± eÅŸleÅŸtirmek iÃ§in orijinal chunk listesi (original_indexed_chunks) boÅŸ.\")\n",
        "        return []\n",
        "    if not query_text or not query_text.strip():\n",
        "        if 'logger' in globals(): logger.warning(\"Arama sorgusu (query_text) boÅŸ veya sadece boÅŸluk iÃ§eriyor.\")\n",
        "        else: print(\"WARNING: Arama sorgusu (query_text) boÅŸ veya sadece boÅŸluk iÃ§eriyor.\")\n",
        "        return []\n",
        "    if top_k <= 0:\n",
        "        if 'logger' in globals(): logger.warning(f\"Ä°stenen top_k ({top_k}) geÃ§ersiz. En az 1 olmalÄ±. BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        else: print(f\"WARNING: Ä°stenen top_k ({top_k}) geÃ§ersiz. En az 1 olmalÄ±. BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return[]\n",
        "\n",
        "    try:\n",
        "        # 1. AdÄ±m: Sorgu metninin embedding'ini al\n",
        "        if 'logger' in globals(): logger.debug(\"Sorgu embedding'i hesaplanÄ±yor...\")\n",
        "        query_embedding_tensor = sentence_transformer_embedding_model.encode(\n",
        "            [query_text], # encode metodu liste bekler\n",
        "            convert_to_tensor=True\n",
        "        )\n",
        "        query_embedding_np = query_embedding_tensor.cpu().numpy().astype('float32')\n",
        "        if 'logger' in globals(): logger.debug(f\"Sorgu embedding'i hesaplandÄ±. Åekil: {query_embedding_np.shape}\")\n",
        "\n",
        "\n",
        "        # 2. AdÄ±m: FAISS indeksinde arama yap\n",
        "        # .search metodu mesafeleri (distances) ve en yakÄ±n komÅŸularÄ±n indislerini (indices) dÃ¶ndÃ¼rÃ¼r.\n",
        "        if 'logger' in globals(): logger.debug(f\"FAISS indeksinde {top_k} en yakÄ±n komÅŸu iÃ§in arama yapÄ±lÄ±yor...\")\n",
        "        distances, indices = faiss_index_instance.search(query_embedding_np, k=top_k)\n",
        "        if 'logger' in globals(): logger.debug(f\"FAISS arama sonuÃ§larÄ± alÄ±ndÄ±. Ä°ndisler: {indices}, Mesafeler: {distances}\")\n",
        "\n",
        "\n",
        "        # 3. AdÄ±m: Bulunan indislerle orijinal chunk'larÄ± eÅŸleÅŸtir\n",
        "        relevant_chunks_found = []\n",
        "        if indices.size > 0: # EÄŸer en az bir sonuÃ§ bulunduysa\n",
        "            for i, idx in enumerate(indices[0]): # indices[0] Ã§Ã¼nkÃ¼ tek bir sorgu iÃ§in arama yaptÄ±k\n",
        "                if idx == -1: # FAISS bazen -1 dÃ¶nebilir (Ã¶zellikle k > index.ntotal ise veya bazÄ± Ã¶zel index tÃ¼rlerinde)\n",
        "                    if 'logger' in globals(): logger.warning(f\"FAISS aramasÄ±nda geÃ§ersiz indeks (-1) bulundu. Bu sonuÃ§ atlanÄ±yor. SÄ±ra: {i}\")\n",
        "                    continue\n",
        "                if 0 <= idx < len(original_indexed_chunks):\n",
        "                    relevant_chunks_found.append(original_indexed_chunks[idx])\n",
        "                else:\n",
        "                    if 'logger' in globals():\n",
        "                        logger.warning(f\"FAISS aramasÄ±ndan gelen indeks ({idx}) orijinal chunk listesi sÄ±nÄ±rlarÄ± dÄ±ÅŸÄ±nda. Toplam chunk: {len(original_indexed_chunks)}. Bu sonuÃ§ atlanÄ±yor.\")\n",
        "                    else:\n",
        "                        print(f\"WARNING: FAISS aramasÄ±ndan gelen indeks ({idx}) orijinal chunk listesi sÄ±nÄ±rlarÄ± dÄ±ÅŸÄ±nda. Toplam chunk: {len(original_indexed_chunks)}. Bu sonuÃ§ atlanÄ±yor.\")\n",
        "        else:\n",
        "             if 'logger' in globals(): logger.info(\"FAISS aramasÄ±ndan herhangi bir sonuÃ§ (indis) bulunamadÄ±.\")\n",
        "             else: print(\"INFO: FAISS aramasÄ±ndan herhangi bir sonuÃ§ (indis) bulunamadÄ±.\")\n",
        "\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"FAISS arama tamamlandÄ±. '{query_text[:50]}...' sorgusu iÃ§in {len(relevant_chunks_found)} adet ilgili chunk bulundu.\")\n",
        "        else:\n",
        "            print(f\"INFO: FAISS arama tamamlandÄ±. '{query_text[:50]}...' sorgusu iÃ§in {len(relevant_chunks_found)} adet ilgili chunk bulundu.\")\n",
        "\n",
        "        return relevant_chunks_found\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"FAISS arama sÄ±rasÄ±nda HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: FAISS arama sÄ±rasÄ±nda HATA oluÅŸtu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"FAISS indeksi oluÅŸturma ve arama fonksiyonlarÄ± tanÄ±mlandÄ±.\")\n",
        "else:\n",
        "    print(\"INFO: FAISS indeksi oluÅŸturma ve arama fonksiyonlarÄ± tanÄ±mlandÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H7} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H7)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H7} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H7} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "KAkXa2s12s9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 8: LLM ile Cevap Ãœretme Fonksiyonu (GÃœNCELLENDÄ° - Son Prompt AyarÄ±)\n",
        "\n",
        "CELL_NAME_H8 = \"8: LLM ile Cevap Ãœretme Fonksiyonu\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H8)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H8} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H8} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H8} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "import torch\n",
        "import logging # Logger seviyesini kontrol etmek iÃ§in\n",
        "\n",
        "def generate_answer_with_llm(\n",
        "    context_text,\n",
        "    question_text,\n",
        "    llm_model_instance,\n",
        "    tokenizer_instance,\n",
        "    max_new_tokens=512, # Bu deÄŸer HÃ¼cre 12'deki RAG_LLM_MAX_NEW_TOKENS ile override edilecek\n",
        "    temperature=0.3,    # Bu deÄŸer HÃ¼cre 12'deki RAG_LLM_TEMPERATURE ile override edilecek\n",
        "    do_sample=True      # Bu deÄŸer HÃ¼cre 12'deki RAG_LLM_DO_SAMPLE ile override edilecek\n",
        "):\n",
        "    current_device = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "    log_active = 'logger' in globals()\n",
        "\n",
        "    def _log_info(message):\n",
        "        if log_active: logger.info(message)\n",
        "        else: print(f\"INFO: {message}\")\n",
        "    def _log_warning(message):\n",
        "        if log_active: logger.warning(message)\n",
        "        else: print(f\"WARNING: {message}\")\n",
        "    def _log_error(message, exc_info=False):\n",
        "        if log_active: logger.error(message, exc_info=exc_info)\n",
        "        else:\n",
        "            print(f\"ERROR: {message}\")\n",
        "            if exc_info:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "    def _log_debug(message):\n",
        "        if log_active and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG):\n",
        "            logger.debug(message)\n",
        "    def _log_critical(message, exc_info=False):\n",
        "        if log_active: logger.critical(message, exc_info=exc_info)\n",
        "        else:\n",
        "            print(f\"CRITICAL: {message}\")\n",
        "            if exc_info:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    _log_info(f\"LLM ile cevap Ã¼retme iÅŸlemi baÅŸlatÄ±lÄ±yor. Soru (ilk 50 krktr): '{str(question_text)[:50]}...'\")\n",
        "    _log_info(f\"LLM 'generate' ayarlarÄ± (fonksiyona gelen): max_new_tokens={max_new_tokens}, temperature={temperature if do_sample else 'N/A (do_sample=False)'}, do_sample={do_sample}\")\n",
        "\n",
        "    if llm_model_instance is None or tokenizer_instance is None:\n",
        "        error_msg = \"LLM (llm_model_instance) veya Tokenizer (tokenizer_instance) saÄŸlanmadÄ±. Cevap Ã¼retilemiyor.\"\n",
        "        _log_error(error_msg)\n",
        "        return f\"HATA: {error_msg}\"\n",
        "\n",
        "   # Prompt'a ekleme: \"CevabÄ±nÄ±n tam, eksiksiz ve dilbilgisi aÃ§Ä±sÄ±ndan doÄŸru bir cÃ¼mle olmasÄ±na Ã¶zen gÃ¶ster.\"\n",
        "    prompt = f\"\"\"<s>[INST] Senin gÃ¶revin, yalnÄ±zca sana verilen DOKÃœMAN BAÄLAMI'nÄ± kullanarak SORU'yu yanÄ±tlamaktÄ±r.\n",
        "Kesinlikle DOKÃœMAN BAÄLAMI dÄ±ÅŸÄ±ndan bilgi kullanma.\n",
        "EÄŸer cevap DOKÃœMAN BAÄLAMI'nda aÃ§Ä±kÃ§a bulunmuyorsa, \"SaÄŸlanan bilgilerde bu soruya net bir cevap bulunmamaktadÄ±r.\" ÅŸeklinde yanÄ±t ver.\n",
        "CevabÄ±n, sorulan sorunun en doÄŸrudan ve en kÄ±sa yanÄ±tÄ± olmalÄ±dÄ±r. CevabÄ±nÄ±n tam, eksiksiz ve dilbilgisi aÃ§Ä±sÄ±ndan doÄŸru bir cÃ¼mle olmasÄ±na Ã¶zen gÃ¶ster. YanÄ±ta herhangi bir ek aÃ§Ä±klama, giriÅŸ cÃ¼mlesi, madde numarasÄ±, referans veya yorum EKLEME. Sadece ve sadece saf bilgiyi, yani direkt cevabÄ± ver. Ã–rneÄŸin, soru \"X ne zaman yapÄ±lÄ±r?\" ise ve baÄŸlamda \"X, Y zamanÄ±nda yapÄ±lÄ±r.\" bilgisi varsa, cevabÄ±n sadece \"X, Y zamanÄ±nda yapÄ±lÄ±r.\" olmalÄ±dÄ±r. BaÅŸka hiÃ§bir ek ifade kullanma. CevabÄ±nÄ± kÄ±sa ve Ã¶z tut. [/INST]\n",
        "DOKÃœMAN BAÄLAMI:\n",
        "{context_text}\n",
        "\n",
        "SORU:\n",
        "{question_text}\n",
        "YANITIN:\"\"\"\n",
        "    # *** PROMPT GÃœNCELLEMESÄ° SONU ***\n",
        "\n",
        "    _log_debug(f\"LLM'e gÃ¶nderilecek Prompt (ilk 300 karakter): {prompt[:300]}...\") # Daha fazla gÃ¶relim\n",
        "    _log_debug(f\"LLM'e gÃ¶nderilecek Prompt (son 100 karakter): ...{prompt[-100:]}\")\n",
        "\n",
        "    try:\n",
        "        PRACTICAL_MAX_CONTEXT_LEN = 4096\n",
        "        tokenizer_reported_max_len = getattr(tokenizer_instance, 'model_max_length', PRACTICAL_MAX_CONTEXT_LEN)\n",
        "\n",
        "        if not isinstance(tokenizer_reported_max_len, int) or \\\n",
        "           tokenizer_reported_max_len <= 0 or \\\n",
        "           tokenizer_reported_max_len > 1000000:\n",
        "            _log_warning(f\"tokenizer.model_max_length ({tokenizer_reported_max_len}) geÃ§ersiz veya Ã§ok bÃ¼yÃ¼k. Pratik Ã¼st sÄ±nÄ±r {PRACTICAL_MAX_CONTEXT_LEN} kullanÄ±lÄ±yor.\")\n",
        "            effective_model_max_len = PRACTICAL_MAX_CONTEXT_LEN\n",
        "        else:\n",
        "            effective_model_max_len = min(tokenizer_reported_max_len, PRACTICAL_MAX_CONTEXT_LEN)\n",
        "        _log_debug(f\"KullanÄ±lacak efektif model maksimum token uzunluÄŸu: {effective_model_max_len}\")\n",
        "\n",
        "        # HÃ¼cre 12'den gelen max_new_tokens kullanÄ±lÄ±r, buradaki varsayÄ±lan deÄŸil.\n",
        "        # Bu yÃ¼zden fonksiyona gelen max_new_tokens deÄŸerini alalÄ±m.\n",
        "        current_max_new_tokens_param = int(globals().get('RAG_LLM_MAX_NEW_TOKENS', max_new_tokens)) # HÃ¼cre 12'deki ayarÄ± al\n",
        "        if not(isinstance(current_max_new_tokens_param, (int, float)) and current_max_new_tokens_param > 0):\n",
        "            current_max_new_tokens_param = 65 # GÃ¼venli bir varsayÄ±lan (Ã¶nceki baÅŸarÄ±lÄ± deÄŸer)\n",
        "        current_max_new_tokens = int(current_max_new_tokens_param)\n",
        "        if current_max_new_tokens <= 0: current_max_new_tokens = 10\n",
        "\n",
        "        safety_margin = 20\n",
        "        truncation_target_length = effective_model_max_len - current_max_new_tokens - safety_margin\n",
        "\n",
        "        if truncation_target_length <= 0:\n",
        "            original_max_new_tokens_val = current_max_new_tokens\n",
        "            min_prompt_len_for_calc = 100\n",
        "            if effective_model_max_len - min_prompt_len_for_calc - safety_margin > 10:\n",
        "                current_max_new_tokens = max(10, effective_model_max_len - min_prompt_len_for_calc - safety_margin)\n",
        "            else:\n",
        "                current_max_new_tokens = max(10, int(effective_model_max_len * 0.15))\n",
        "\n",
        "            truncation_target_length = effective_model_max_len - current_max_new_tokens - safety_margin\n",
        "            _log_warning(f\"Ä°lk max_new_tokens ({original_max_new_tokens_val}) Ã§ok bÃ¼yÃ¼k veya effective_model_max_len Ã§ok kÃ¼Ã§Ã¼k. \"\n",
        "                         f\"Model kapasitesine gÃ¶re max_new_tokens {current_max_new_tokens}'e ayarlandÄ±. \"\n",
        "                         f\"truncation_target_length: {truncation_target_length}\")\n",
        "\n",
        "        if truncation_target_length <= 0:\n",
        "            error_msg = (f\"Model kapasitesi yetersiz. truncate_len ({truncation_target_length}) < 0. \"\n",
        "                         f\"eff_max_len: {effective_model_max_len}, final_max_new_tokens: {current_max_new_tokens}\")\n",
        "            _log_critical(error_msg)\n",
        "            return f\"HATA: {error_msg}\"\n",
        "\n",
        "        final_max_length_for_tokenizer = int(truncation_target_length)\n",
        "        if final_max_length_for_tokenizer <= 0:\n",
        "            error_msg = f\"Hesaplanan final_max_length_for_tokenizer ({final_max_length_for_tokenizer}) pozitif deÄŸil. Tokenizer'a gÃ¶nderilemiyor.\"\n",
        "            _log_critical(error_msg)\n",
        "            return f\"HATA: {error_msg}\"\n",
        "\n",
        "        _log_debug(f\"Tokenizer'a gÃ¶nderilecek max_length: {final_max_length_for_tokenizer}\")\n",
        "        inputs = tokenizer_instance(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=final_max_length_for_tokenizer\n",
        "        ).to(current_device)\n",
        "\n",
        "        input_token_length = inputs.input_ids.shape[1]\n",
        "        _log_info(f\"Tokenize edilmiÅŸ giriÅŸ (prompt) uzunluÄŸu: {input_token_length} token.\")\n",
        "        _log_debug(f\"GiriÅŸ token ID'leri (ilk 10): {inputs.input_ids[0, :10].tolist()}\")\n",
        "\n",
        "        available_space_for_generation = effective_model_max_len - input_token_length - safety_margin\n",
        "        if current_max_new_tokens > available_space_for_generation:\n",
        "            original_max_new_tokens_val_2 = current_max_new_tokens\n",
        "            current_max_new_tokens = max(10, available_space_for_generation)\n",
        "            _log_warning(f\"Tokenize edilmiÅŸ prompt sonrasÄ± max_new_tokens ({original_max_new_tokens_val_2}) Ã§ok bÃ¼yÃ¼k. \"\n",
        "                         f\"Kalan alana gÃ¶re {current_max_new_tokens}'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼.\")\n",
        "\n",
        "        final_max_new_tokens_for_generation = int(current_max_new_tokens)\n",
        "        if final_max_new_tokens_for_generation <= 0:\n",
        "            error_msg = (f\"Tokenize edilmiÅŸ prompt sonrasÄ± cevap Ã¼retimi iÃ§in yer kalmadÄ± \"\n",
        "                         f\"(final_max_new_tokens: {final_max_new_tokens_for_generation}). Prompt Ã§ok uzun veya model kapasitesi dÃ¼ÅŸÃ¼k.\")\n",
        "            _log_error(error_msg)\n",
        "            return f\"HATA: Prompt Ã§ok uzun, cevap Ã¼retilemedi.\"\n",
        "\n",
        "        # LLM'e gÃ¶nderilecek nihai max_new_tokens deÄŸerini logla (HÃ¼cre 12'deki ayardan gelmeli)\n",
        "        final_temp_for_generation = float(globals().get('RAG_LLM_TEMPERATURE', temperature))\n",
        "        final_do_sample_for_generation = bool(globals().get('RAG_LLM_DO_SAMPLE', do_sample))\n",
        "\n",
        "        _log_info(f\"LLM.generate Ã§aÄŸrÄ±lÄ±yor. max_new_tokens: {final_max_new_tokens_for_generation}, temperature: {final_temp_for_generation if final_do_sample_for_generation else 'N/A'}, do_sample: {final_do_sample_for_generation}\")\n",
        "\n",
        "\n",
        "        output_sequences = llm_model_instance.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=final_max_new_tokens_for_generation,\n",
        "            temperature=final_temp_for_generation if final_do_sample_for_generation else 1.0,\n",
        "            top_k=50 if final_do_sample_for_generation else None,\n",
        "            top_p=0.95 if final_do_sample_for_generation else None,\n",
        "            do_sample=final_do_sample_for_generation,\n",
        "            pad_token_id=tokenizer_instance.eos_token_id,\n",
        "        )\n",
        "\n",
        "        generated_answer = tokenizer_instance.decode(output_sequences[0, input_token_length:], skip_special_tokens=True).strip()\n",
        "        _log_info(f\"LLM'den ham cevap Ã¼retildi (ilk 100 karakter): '{str(generated_answer)[:100]}...'\")\n",
        "\n",
        "        if generated_answer.lower().startswith(\"yanitin:\"):\n",
        "            generated_answer = generated_answer[len(\"yanitin:\"):].strip()\n",
        "            _log_debug(\"CevabÄ±n baÅŸÄ±ndaki 'yanitin:' ifadesi temizlendi.\")\n",
        "\n",
        "        return generated_answer\n",
        "\n",
        "    except OverflowError as oe:\n",
        "        error_msg_overflow = f\"LLM ile cevap Ã¼retirken OverflowError oluÅŸtu (muhtemelen token uzunluÄŸuyla ilgili): {oe}\"\n",
        "        _log_critical(error_msg_overflow, exc_info=True)\n",
        "        return f\"HATA: {error_msg_overflow}\"\n",
        "    except Exception as e:\n",
        "        _log_error(f\"LLM ile cevap Ã¼retirken genel bir HATA oluÅŸtu: {e}\", exc_info=True)\n",
        "        return \"HATA: Cevap Ã¼retimi sÄ±rasÄ±nda bir sorun oluÅŸtu.\"\n",
        "\n",
        "if 'logger' in globals() and 'logging' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"LLM ile cevap Ã¼retme fonksiyonu (generate_answer_with_llm) tanÄ±mlandÄ± (Son Prompt AyarÄ± ile).\")\n",
        "else:\n",
        "    print(\"INFO: LLM ile cevap Ã¼retme fonksiyonu (generate_answer_with_llm) tanÄ±mlandÄ± (Son Prompt AyarÄ± ile).\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H8)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H8} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H8} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "GylzW2cG3AAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 9: Cevap Kalitesi DeÄŸerlendirme FonksiyonlarÄ± (GÃœNCELLENDÄ° - Manuel BLEURT Checkpoint YÃ¼kleme)\n",
        "\n",
        "CELL_NAME_H9 = \"9: Cevap Kalitesi DeÄŸerlendirme FonksiyonlarÄ±\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H9)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H9} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H9} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H9} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "# Gerekli importlar\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score_L\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import torch\n",
        "import os # Dosya ve dizin iÅŸlemleri iÃ§in\n",
        "import zipfile # Zip dosyalarÄ±nÄ± aÃ§mak iÃ§in\n",
        "import logging # Logger seviyesini kontrol etmek iÃ§in\n",
        "\n",
        "# Lokal loglama yardÄ±mcÄ± fonksiyonlarÄ± (HÃ¼cre 9 iÃ§in)\n",
        "log_active_h9 = 'logger' in globals()\n",
        "def _log_h9_info(message):\n",
        "    if log_active_h9: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_h9_warning(message):\n",
        "    if log_active_h9: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_h9_error(message, exc_info=False):\n",
        "    if log_active_h9: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_h9_debug(message):\n",
        "    if log_active_h9 and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG):\n",
        "        logger.debug(message)\n",
        "\n",
        "\n",
        "# --- BLEURT Scorer YÃ¼kleme GiriÅŸimi (Manuel Checkpoint ile) ---\n",
        "bleurt_scorer_instance_global = None\n",
        "# bleurt_available ve bleurt_score_module_global_for_h3_and_h9 HÃ¼cre 3'ten gelmeli\n",
        "\n",
        "# Google Drive'daki zip dosyasÄ±nÄ±n yolu ve Colab'da aÃ§Ä±lacaÄŸÄ± hedef yol\n",
        "drive_bleurt_zip_path = \"/content/drive/MyDrive/Colab_RAG_Projesi/bleurt_resources/BLEURT-20.zip\"\n",
        "colab_extract_base_path = \"/content/bleurt_checkpoint_extracted\" # Checkpoint'in aÃ§Ä±lacaÄŸÄ± ana klasÃ¶r\n",
        "# BLEURT-20.zip aÃ§Ä±ldÄ±ÄŸÄ±nda genellikle iÃ§inde \"BLEURT-20\" adÄ±nda bir klasÃ¶r olur.\n",
        "# Bu yÃ¼zden scorer'a verilecek asÄ±l checkpoint yolu /content/bleurt_checkpoint_extracted/BLEURT-20/ olmalÄ±.\n",
        "# Ancak zip dosyasÄ±nÄ±n iÃ§eriÄŸine gÃ¶re bu deÄŸiÅŸebilir. Åimdilik zip'in doÄŸrudan hedef klasÃ¶re aÃ§Ä±ldÄ±ÄŸÄ±nÄ± varsayalÄ±m\n",
        "# ve BleurtScorer'a aÃ§Ä±lmÄ±ÅŸ ana klasÃ¶rÃ¼n (Ã¶rn: BLEURT-20) yolunu verelim.\n",
        "# EÄŸer zip iÃ§ yapÄ±sÄ± farklÄ±ysa, bu yolun ayarlanmasÄ± gerekebilir.\n",
        "# Ã–rneÄŸin, zip iÃ§inde doÄŸrudan dosyalar varsa colab_extract_base_path + \"BLEURT-20_unzipped_content\" gibi bir yol olabilir.\n",
        "# VEYA zip zaten \"BLEURT-20\" adlÄ± bir klasÃ¶r iÃ§eriyorsa, o zaman:\n",
        "final_bleurt_checkpoint_path_in_colab = os.path.join(colab_extract_base_path, \"BLEURT-20\")\n",
        "\n",
        "\n",
        "if 'bleurt_available' in globals() and bleurt_available and \\\n",
        "   'bleurt_score_module_global_for_h3_and_h9' in globals() and bleurt_score_module_global_for_h3_and_h9 is not None:\n",
        "\n",
        "    _log_h9_info(f\"Manuel BLEURT checkpoint iÅŸlemi baÅŸlatÄ±lÄ±yor.\")\n",
        "    _log_h9_info(f\"Google Drive'da beklenen ZIP yolu: {drive_bleurt_zip_path}\")\n",
        "    _log_h9_info(f\"Colab'da aÃ§Ä±lacak hedef checkpoint yolu: {final_bleurt_checkpoint_path_in_colab}\")\n",
        "\n",
        "    manual_checkpoint_ready = False\n",
        "    if os.path.exists(drive_bleurt_zip_path):\n",
        "        _log_h9_info(f\"'{drive_bleurt_zip_path}' Google Drive'da bulundu.\")\n",
        "        # Hedef aÃ§ma klasÃ¶rÃ¼nÃ¼n (Ã¶rn: /content/bleurt_checkpoint_extracted/BLEURT-20)\n",
        "        # varlÄ±ÄŸÄ±nÄ± kontrol et. EÄŸer yoksa veya boÅŸsa zip'i aÃ§.\n",
        "        # Bu, her Ã§alÄ±ÅŸtÄ±rmada zip'i tekrar tekrar aÃ§mayÄ± Ã¶nler.\n",
        "        # Ancak zip iÃ§eriÄŸi deÄŸiÅŸirse veya Colab oturumu yeniden baÅŸlarsa aÃ§ma iÅŸlemi gerekebilir.\n",
        "        # Åimdilik, eÄŸer final_bleurt_checkpoint_path_in_colab yoksa aÃ§alÄ±m.\n",
        "        if not os.path.exists(final_bleurt_checkpoint_path_in_colab) or \\\n",
        "           not os.listdir(final_bleurt_checkpoint_path_in_colab if os.path.exists(final_bleurt_checkpoint_path_in_colab) else colab_extract_base_path): # KlasÃ¶r varsa ama boÅŸsa da aÃ§\n",
        "            _log_h9_info(f\"Hedef checkpoint '{final_bleurt_checkpoint_path_in_colab}' Colab'da bulunamadÄ± veya boÅŸ. ZIP dosyasÄ± aÃ§Ä±lÄ±yor...\")\n",
        "            try:\n",
        "                os.makedirs(colab_extract_base_path, exist_ok=True) # Ana aÃ§ma klasÃ¶rÃ¼nÃ¼ oluÅŸtur\n",
        "                with zipfile.ZipFile(drive_bleurt_zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(colab_extract_base_path) # ZIP'i ana klasÃ¶re aÃ§\n",
        "                _log_h9_info(f\"ZIP dosyasÄ± '{colab_extract_base_path}' dizinine baÅŸarÄ±yla aÃ§Ä±ldÄ±.\")\n",
        "                # AÃ§Ä±ldÄ±ktan sonra final_bleurt_checkpoint_path_in_colab'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "                if os.path.exists(final_bleurt_checkpoint_path_in_colab):\n",
        "                    _log_h9_info(f\"AÃ§Ä±lan checkpoint dizini '{final_bleurt_checkpoint_path_in_colab}' doÄŸrulandÄ±.\")\n",
        "                    # Dizin iÃ§eriÄŸini loglayalÄ±m (ilk birkaÃ§ dosya/klasÃ¶r)\n",
        "                    try:\n",
        "                        content_list = os.listdir(final_bleurt_checkpoint_path_in_colab)\n",
        "                        _log_h9_debug(f\"'{final_bleurt_checkpoint_path_in_colab}' iÃ§eriÄŸi (ilk 5): {content_list[:5]}\")\n",
        "                    except Exception as e_ls:\n",
        "                        _log_h9_warning(f\"AÃ§Ä±lan checkpoint dizin iÃ§eriÄŸi listelenirken hata: {e_ls}\")\n",
        "                    manual_checkpoint_ready = True\n",
        "                else:\n",
        "                    _log_h9_error(f\"ZIP aÃ§Ä±ldÄ± ancak beklenen '{final_bleurt_checkpoint_path_in_colab}' dizini bulunamadÄ±. LÃ¼tfen ZIP dosyasÄ±nÄ±n iÃ§eriÄŸini kontrol edin. Belki farklÄ± bir alt klasÃ¶r adÄ± vardÄ±r veya dosyalar doÄŸrudan '{colab_extract_base_path}' altÄ±na aÃ§Ä±lmÄ±ÅŸtÄ±r.\")\n",
        "                    # Bu durumda, colab_extract_base_path'in iÃ§eriÄŸini kontrol edebiliriz\n",
        "                    try:\n",
        "                        content_list_base = os.listdir(colab_extract_base_path)\n",
        "                        _log_h9_info(f\"'{colab_extract_base_path}' (ana aÃ§ma dizini) iÃ§eriÄŸi: {content_list_base}\")\n",
        "                        # EÄŸer BLEURT-20.zip dosyalarÄ± doÄŸrudan buraya aÃ§Ä±yorsa, final_bleurt_checkpoint_path_in_colab'Ä± buna gÃ¶re ayarlamamÄ±z gerekebilir.\n",
        "                        # Ã–rneÄŸin, eÄŸer \"saved_model.pb\" bu dizindeyse, path doÄŸru olabilir.\n",
        "                        # Åimdilik hata olarak bÄ±rakÄ±yoruz.\n",
        "                    except Exception as e_ls_base:\n",
        "                         _log_h9_warning(f\"Ana aÃ§ma dizin iÃ§eriÄŸi listelenirken hata: {e_ls_base}\")\n",
        "\n",
        "            except Exception as e_unzip:\n",
        "                _log_h9_error(f\"ZIP dosyasÄ± ('{drive_bleurt_zip_path}') aÃ§Ä±lÄ±rken HATA: {e_unzip}\", exc_info=True)\n",
        "        else:\n",
        "            _log_h9_info(f\"Hedef checkpoint '{final_bleurt_checkpoint_path_in_colab}' Colab'da zaten mevcut gÃ¶rÃ¼nÃ¼yor. ZIP aÃ§ma iÅŸlemi atlandÄ±.\")\n",
        "            manual_checkpoint_ready = True\n",
        "    else:\n",
        "        _log_h9_error(f\"BLEURT ZIP dosyasÄ± ('{drive_bleurt_zip_path}') Google Drive'da bulunamadÄ±. LÃ¼tfen yÃ¼klediÄŸinizden ve yolun doÄŸru olduÄŸundan emin olun.\")\n",
        "\n",
        "    # Manuel checkpoint hazÄ±rsa scorer'Ä± yÃ¼klemeyi dene\n",
        "    if manual_checkpoint_ready:\n",
        "        _log_h9_info(f\"BLEURT scorer manuel olarak belirtilen yoldan yÃ¼kleniyor: '{final_bleurt_checkpoint_path_in_colab}'\")\n",
        "        try:\n",
        "            bleurt_scorer_instance_global = bleurt_score_module_global_for_h3_and_h9.BleurtScorer(final_bleurt_checkpoint_path_in_colab)\n",
        "            _log_h9_info(f\"BLEURT scorer ('{final_bleurt_checkpoint_path_in_colab}') baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "        except AssertionError as e_assert: # Checkpoint yapÄ±sÄ± beklenen gibi deÄŸilse bu hata alÄ±nabilir\n",
        "             _log_h9_error(f\"BLEURT checkpoint ('{final_bleurt_checkpoint_path_in_colab}') yÃ¼klenirken AssertionError (muhtemelen checkpoint yapÄ±sÄ±/dosyalarÄ± eksik veya yanlÄ±ÅŸ): {e_assert}. LÃ¼tfen aÃ§Ä±lan dizinin iÃ§eriÄŸini ve ZIP dosyasÄ±nÄ±n doÄŸruluÄŸunu kontrol edin.\", exc_info=True)\n",
        "             bleurt_scorer_instance_global = None\n",
        "        except Exception as e_bleurt_manual:\n",
        "            _log_h9_error(f\"Manuel BLEURT checkpoint ('{final_bleurt_checkpoint_path_in_colab}') yÃ¼klenirken genel bir HATA oluÅŸtu: {e_bleurt_manual}\", exc_info=True)\n",
        "            bleurt_scorer_instance_global = None\n",
        "    else:\n",
        "        if os.path.exists(drive_bleurt_zip_path): # Zip bulundu ama aÃ§Ä±lamadÄ± veya doÄŸrulanamadÄ±\n",
        "             _log_h9_warning(f\"BLEURT ZIP dosyasÄ± bulundu ancak checkpoint hazÄ±rlanamadÄ±. BLEURT skorlamasÄ± atlanacak.\")\n",
        "        # else: Zip zaten bulunamadÄ±, yukarÄ±da hata loglandÄ±.\n",
        "        bleurt_scorer_instance_global = None\n",
        "else:\n",
        "    _log_h9_warning(\"BLEURT kÃ¼tÃ¼phanesi/modÃ¼lÃ¼ (HÃ¼cre 3'ten) bulunamadÄ± veya iÃ§e aktarÄ±lamadÄ±. BLEURT skorlamasÄ± atlanacak.\")\n",
        "    bleurt_scorer_instance_global = None\n",
        "\n",
        "\n",
        "# --- KosinÃ¼s BenzerliÄŸi Hesaplama Fonksiyonu ---\n",
        "def compute_cosine_similarity_for_eval(\n",
        "    sentence_transformer_model_instance,\n",
        "    candidate_text,\n",
        "    reference_text\n",
        "):\n",
        "    if sentence_transformer_model_instance is None:\n",
        "        _log_h9_warning(\"KosinÃ¼s benzerliÄŸi iÃ§in embedding modeli saÄŸlanmadÄ±. 0.0 dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return 0.0\n",
        "    if not candidate_text or not isinstance(candidate_text, str) or not str(candidate_text).strip() or \\\n",
        "       not reference_text or not isinstance(reference_text, str) or not str(reference_text).strip():\n",
        "        _log_h9_debug(\"KosinÃ¼s benzerliÄŸi iÃ§in aday veya referans metin boÅŸ/geÃ§ersiz. 0.0 dÃ¶ndÃ¼rÃ¼lÃ¼yor.\")\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        embeddings = sentence_transformer_model_instance.encode(\n",
        "            [str(candidate_text), str(reference_text)],\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        similarity_matrix = cosine_similarity(\n",
        "            embeddings[0].reshape(1, -1).cpu().numpy(),\n",
        "            embeddings[1].reshape(1, -1).cpu().numpy()\n",
        "        )\n",
        "        similarity_score = similarity_matrix[0][0]\n",
        "        return float(similarity_score)\n",
        "    except Exception as e:\n",
        "        _log_h9_error(f\"KosinÃ¼s benzerliÄŸi hesaplanÄ±rken HATA: {e}\", exc_info=False)\n",
        "        return 0.0\n",
        "\n",
        "# --- Ana DeÄŸerlendirme Fonksiyonu ---\n",
        "def evaluate_answer_quality(\n",
        "    generated_answer_text,\n",
        "    reference_answer_text,\n",
        "    main_embedding_model_instance\n",
        "):\n",
        "    _log_h9_info(\"Cevap kalitesi metrikleri hesaplanÄ±yor...\")\n",
        "    _log_h9_debug(f\"DeÄŸerlendirilecek Cevap (ilk 50): '{str(generated_answer_text)[:50]}...'\")\n",
        "    _log_h9_debug(f\"Referans Cevap (ilk 50): '{str(reference_answer_text)[:50]}...'\")\n",
        "\n",
        "    metrics = {}\n",
        "    current_device_for_metrics = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "\n",
        "    ref_for_token = str(reference_answer_text) if reference_answer_text and isinstance(reference_answer_text, str) and str(reference_answer_text).strip() else \"[REFERANS_YOK]\"\n",
        "    gen_for_token = str(generated_answer_text) if generated_answer_text and isinstance(generated_answer_text, str) and str(generated_answer_text).strip() else \"[CEVAP_YOK]\"\n",
        "\n",
        "    try:\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        reference_tokens_lower = word_tokenize(ref_for_token.lower(), language='turkish')\n",
        "        generated_tokens_lower = word_tokenize(gen_for_token.lower(), language='turkish')\n",
        "    except Exception as e_tokenize:\n",
        "        _log_h9_error(f\"DeÄŸerlendirme iÃ§in tokenizasyon sÄ±rasÄ±nda HATA: {e_tokenize}\", exc_info=False)\n",
        "        reference_tokens_lower, generated_tokens_lower = [], []\n",
        "\n",
        "    try:\n",
        "        rouge_calculator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rs = rouge_calculator.score(str(reference_answer_text) or \"\", str(generated_answer_text) or \"\")\n",
        "        metrics.update({\n",
        "            'ROUGE1_F': rs['rouge1'].fmeasure,\n",
        "            'ROUGE2_F': rs['rouge2'].fmeasure,\n",
        "            'ROUGEL_F': rs['rougeL'].fmeasure\n",
        "        })\n",
        "    except Exception as e_rouge:\n",
        "        _log_h9_error(f\"ROUGE skorlarÄ± hesaplanÄ±rken HATA: {e_rouge}\", exc_info=False)\n",
        "        metrics.update({'ROUGE1_F': \"HATA\", 'ROUGE2_F': \"HATA\", 'ROUGEL_F': \"HATA\"})\n",
        "\n",
        "    try:\n",
        "        bleu_val = 0.0\n",
        "        if reference_tokens_lower and generated_tokens_lower:\n",
        "            bleu_val = sentence_bleu([reference_tokens_lower], generated_tokens_lower, smoothing_function=SmoothingFunction().method4)\n",
        "        elif not reference_tokens_lower and not generated_tokens_lower:\n",
        "            if not (str(reference_answer_text) or \"\").strip() and not (str(generated_answer_text) or \"\").strip():\n",
        "                bleu_val = 1.0\n",
        "        metrics['BLEU'] = bleu_val\n",
        "    except Exception as e_bleu:\n",
        "        _log_h9_error(f\"BLEU skoru hesaplanÄ±rken HATA: {e_bleu}\", exc_info=False)\n",
        "        metrics['BLEU'] = \"HATA\"\n",
        "\n",
        "    try:\n",
        "        meteor_val = 0.0\n",
        "        if reference_tokens_lower and generated_tokens_lower:\n",
        "            meteor_val = meteor_score([reference_tokens_lower], generated_tokens_lower)\n",
        "        elif not reference_tokens_lower and not generated_tokens_lower:\n",
        "            if not (str(reference_answer_text) or \"\").strip() and not (str(generated_answer_text) or \"\").strip():\n",
        "                meteor_val = 1.0\n",
        "        metrics['METEOR'] = meteor_val\n",
        "    except Exception as e_meteor:\n",
        "        _log_h9_error(f\"METEOR skoru hesaplanÄ±rken HATA: {e_meteor}\", exc_info=False)\n",
        "        metrics['METEOR'] = \"HATA\"\n",
        "\n",
        "    try:\n",
        "        P, R, F1_bs = bert_score_L(\n",
        "            [gen_for_token], [ref_for_token],\n",
        "            lang=\"tr\",\n",
        "            model_type='bert-base-multilingual-cased',\n",
        "            device=current_device_for_metrics,\n",
        "            verbose=False\n",
        "        )\n",
        "        metrics['BERTScore_F1'] = F1_bs.mean().item()\n",
        "    except Exception as e_bertscore:\n",
        "        _log_h9_error(f\"BERTScore hesaplanÄ±rken HATA: {e_bertscore}\", exc_info=True)\n",
        "        metrics['BERTScore_F1'] = \"HATA\"\n",
        "\n",
        "    metrics['Cosine_Similarity'] = compute_cosine_similarity_for_eval(\n",
        "        main_embedding_model_instance,\n",
        "        generated_answer_text, # Orijinal stringleri gÃ¶nderiyoruz\n",
        "        reference_answer_text\n",
        "    )\n",
        "\n",
        "    if bleurt_scorer_instance_global:\n",
        "        try:\n",
        "            bleurt_scores_raw = bleurt_scorer_instance_global.score(\n",
        "                references=[ref_for_token],\n",
        "                candidates=[gen_for_token]\n",
        "            )\n",
        "            metrics['BLEURT'] = bleurt_scores_raw[0] if bleurt_scores_raw and isinstance(bleurt_scores_raw[0], (float, np.number)) else \"HATA (Tip)\" # np.number daha genel\n",
        "        except Exception as e_bleurt_score:\n",
        "            _log_h9_error(f\"BLEURT skoru hesaplanÄ±rken HATA: {e_bleurt_score}\", exc_info=False)\n",
        "            metrics['BLEURT'] = \"HATA (Hesaplama)\"\n",
        "    else:\n",
        "        metrics['BLEURT'] = \"N/A (YÃ¼klenemedi)\"\n",
        "\n",
        "    try:\n",
        "        f1_token = 0.0\n",
        "        if not generated_tokens_lower and not reference_tokens_lower:\n",
        "            if not (str(generated_answer_text) or \"\").strip() and not (str(reference_answer_text) or \"\").strip():\n",
        "                f1_token = 1.0\n",
        "        elif generated_tokens_lower and reference_tokens_lower:\n",
        "            common_tokens = set(reference_tokens_lower) & set(generated_tokens_lower)\n",
        "            precision = len(common_tokens) / len(generated_tokens_lower) if len(generated_tokens_lower) > 0 else 0\n",
        "            recall = len(common_tokens) / len(reference_tokens_lower) if len(reference_tokens_lower) > 0 else 0\n",
        "            if (precision + recall) > 0:\n",
        "                f1_token = 2 * (precision * recall) / (precision + recall)\n",
        "        metrics['Token_F1'] = f1_token\n",
        "    except Exception as e_token_f1:\n",
        "        _log_h9_error(f\"Token F1 skoru hesaplanÄ±rken HATA: {e_token_f1}\", exc_info=False)\n",
        "        metrics['Token_F1'] = \"HATA\"\n",
        "\n",
        "    final_metrics = {}\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, (float, np.number)): # np.number daha genel\n",
        "            final_metrics[key] = round(float(value), 4) # float'a Ã§evirip yuvarla\n",
        "        else:\n",
        "            final_metrics[key] = value\n",
        "\n",
        "    _log_h9_info(f\"DeÄŸerlendirme metrikleri hesaplandÄ±: {final_metrics}\")\n",
        "    return final_metrics\n",
        "\n",
        "if 'logger' in globals() and 'logging' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"Cevap kalitesi deÄŸerlendirme fonksiyonlarÄ± tanÄ±mlandÄ± (manuel BLEURT yÃ¼kleme denemesi ile).\")\n",
        "else:\n",
        "    print(\"INFO: Cevap kalitesi deÄŸerlendirme fonksiyonlarÄ± tanÄ±mlandÄ± (manuel BLEURT yÃ¼kleme denemesi ile).\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H9)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H9} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H9} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "RTT1lLOp3U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 10: Ana RAG Pipeline Fonksiyonu (GÃœNCELLENDÄ° - Loglar DÃ¼zenlendi, Cache AnahtarÄ± Ä°yileÅŸtirildi, Metrik YazdÄ±rma DÃ¼zenlendi)\n",
        "\n",
        "CELL_NAME_H10 = \"10: Ana RAG Pipeline Fonksiyonu\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H10)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H10} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H10} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H10} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "# Gerekli fonksiyonlar ve global deÄŸiÅŸkenler Ã¶nceki hÃ¼crelerden gelmeli.\n",
        "import datetime # Zamanlama iÃ§in\n",
        "import os       # Dosya iÅŸlemleri iÃ§in\n",
        "import hashlib  # Hashleme iÃ§in\n",
        "\n",
        "# Lokal loglama yardÄ±mcÄ± fonksiyonlarÄ± (rag_pipeline iÃ§inde kullanÄ±lacak)\n",
        "log_active_rag = 'logger' in globals() # Logger'Ä±n varlÄ±ÄŸÄ±nÄ± bir kere kontrol et (rag_pipeline iÃ§in)\n",
        "def _log_rag_info(message):\n",
        "    if log_active_rag: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_rag_warning(message):\n",
        "    if log_active_rag: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_rag_error(message, exc_info=False):\n",
        "    if log_active_rag: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_rag_debug(message):\n",
        "    if log_active_rag and logger.isEnabledFor(logging.DEBUG):\n",
        "        logger.debug(message)\n",
        "    # else:\n",
        "    #     if LOG_LEVEL == logging.DEBUG: print(f\"DEBUG: {message}\")\n",
        "\n",
        "\n",
        "def rag_pipeline(\n",
        "    pdf_path,\n",
        "    question,\n",
        "    reference_answer,\n",
        "    llm_model_to_use,\n",
        "    llm_tokenizer_to_use,\n",
        "    st_embedding_model_instance,\n",
        "    lc_embedding_model_for_semantic_instance,\n",
        "    use_semantic_chunker=False,\n",
        "    recursive_chunk_size=1000,\n",
        "    recursive_chunk_overlap=150,\n",
        "    semantic_chunker_threshold_type=\"percentile\",\n",
        "    semantic_chunker_threshold_amount=95,\n",
        "    retrieval_top_k=5,\n",
        "    llm_max_new_tokens=512,\n",
        "    llm_temperature=0.3,\n",
        "    llm_do_sample=True,\n",
        "    log_retrieved_chunks_content=False\n",
        "):\n",
        "    pipeline_start_time = datetime.datetime.now()\n",
        "    pdf_basename = os.path.basename(pdf_path) if pdf_path else \"BilinmeyenPDF\"\n",
        "\n",
        "    _log_rag_info(f\"RAG Pipeline baÅŸlatÄ±lÄ±yor. PDF: '{pdf_basename}', Soru (ilk 30): '{str(question)[:30]}...'\")\n",
        "    _log_rag_info(f\"Ayarlar: SemanticChunker={use_semantic_chunker}, RecChunkSize={recursive_chunk_size}, TopK={retrieval_top_k}, LLMMaxTokens={llm_max_new_tokens}\")\n",
        "\n",
        "    if not all([llm_model_to_use, llm_tokenizer_to_use, st_embedding_model_instance]):\n",
        "        error_msg = \"Temel modeller (LLM, Tokenizer, SentenceTransformer Embedding) eksik. Pipeline Ã§alÄ±ÅŸtÄ±rÄ±lamÄ±yor.\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return \"HATA: Modeller eksik.\", {}\n",
        "    if use_semantic_chunker and not lc_embedding_model_for_semantic_instance:\n",
        "        error_msg = \"Anlamsal parÃ§alayÄ±cÄ± (SemanticChunker) istendi ancak Langchain embedding modeli eksik.\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return \"HATA: Semantik parÃ§alayÄ±cÄ± iÃ§in embedding modeli eksik.\", {}\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        error_msg = f\"GeÃ§ersiz PDF yolu veya dosya bulunamadÄ±: {pdf_path}\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return f\"HATA: PDF bulunamadÄ± ({pdf_basename}).\", {}\n",
        "\n",
        "    pdf_content_hash_val = generate_file_hash(pdf_path) # generate_file_hash HÃ¼cre 6'dan\n",
        "    if not pdf_content_hash_val:\n",
        "        try:\n",
        "            pdf_mtime = os.path.getmtime(pdf_path)\n",
        "            pdf_content_hash_val = hashlib.md5(f\"{pdf_basename}_{pdf_mtime}\".encode()).hexdigest()[:16]\n",
        "            _log_rag_warning(f\"PDF iÃ§eriÄŸi iÃ§in MD5 hash alÄ±namadÄ±, dosya adÄ± ve mtime kullanÄ±larak alternatif hash oluÅŸturuldu: {pdf_content_hash_val}\")\n",
        "        except Exception as e_hash_fallback:\n",
        "            pdf_content_hash_val = hashlib.md5(pdf_basename.encode() + os.urandom(8)).hexdigest()[:16]\n",
        "            _log_rag_warning(f\"PDF hash iÃ§in mtime alÄ±nÄ±rken hata ({e_hash_fallback}), dosya adÄ± ve rastgele veri ile alternatif hash: {pdf_content_hash_val}\")\n",
        "\n",
        "    global_embedding_model_id_str = globals().get('EMBEDDING_MODEL_ID', 'unknown_emb_model') # HÃ¼cre 4'ten\n",
        "    embedding_model_name_for_cache_key = global_embedding_model_id_str.split('/')[-1].replace('-', '_')[:20]\n",
        "\n",
        "    chunker_settings_suffix_key = \"\"\n",
        "    if use_semantic_chunker:\n",
        "        chunker_settings_suffix_key = f\"_sem_t{str(semantic_chunker_threshold_type)[:3]}_a{semantic_chunker_threshold_amount}\"\n",
        "    else:\n",
        "        chunker_settings_suffix_key = f\"_rec_cs{recursive_chunk_size}_co{recursive_chunk_overlap}\"\n",
        "\n",
        "    file_identifier_for_cache_key = f\"{pdf_basename}_{pdf_content_hash_val[:8]}_emb_{embedding_model_name_for_cache_key}{chunker_settings_suffix_key}\"\n",
        "    _log_rag_info(f\"OluÅŸturulan Ã¶nbellek (cache) anahtarÄ±: {file_identifier_for_cache_key}\")\n",
        "\n",
        "    global_cache_dir_path = globals().get('CACHE_DIR', './cache_default') # HÃ¼cre 4'ten\n",
        "    processed_chunks_from_cache, faiss_idx_from_cache = load_chunks_and_index(file_identifier_for_cache_key, cache_directory=global_cache_dir_path) # HÃ¼cre 6'dan\n",
        "\n",
        "    if processed_chunks_from_cache is None or faiss_idx_from_cache is None:\n",
        "        _log_rag_info(f\"'{file_identifier_for_cache_key}' iÃ§in Ã¶nbellek bulunamadÄ±/yÃ¼klenemedi. Veri yeniden iÅŸlenecek.\")\n",
        "        raw_text_from_pdf_val = pdf_to_text(pdf_path) # HÃ¼cre 6'dan\n",
        "        if not raw_text_from_pdf_val:\n",
        "            _log_rag_error(f\"PDF ({pdf_basename}) okunamadÄ± veya boÅŸ iÃ§erik.\")\n",
        "            return f\"HATA: PDF ({pdf_basename}) okunamadÄ± veya boÅŸ iÃ§erik.\", {}\n",
        "\n",
        "        if use_semantic_chunker:\n",
        "            raw_chunks_list = chunk_text_semantic( # HÃ¼cre 6'dan\n",
        "                raw_text_from_pdf_val, lc_embedding_model_for_semantic_instance,\n",
        "                semantic_chunker_threshold_type, semantic_chunker_threshold_amount\n",
        "            )\n",
        "        else:\n",
        "            raw_chunks_list = chunk_text_recursive( # HÃ¼cre 6'dan\n",
        "                raw_text_from_pdf_val, recursive_chunk_size, recursive_chunk_overlap\n",
        "            )\n",
        "\n",
        "        if not raw_chunks_list:\n",
        "            _log_rag_error(f\"Metin ({pdf_basename}) parÃ§alanamadÄ± (chunking baÅŸarÄ±sÄ±z).\")\n",
        "            return f\"HATA: Metin ({pdf_basename}) parÃ§alanamadÄ± (chunking baÅŸarÄ±sÄ±z).\", {}\n",
        "        _log_rag_info(f\"Ham metin {len(raw_chunks_list)} parÃ§aya (chunk) ayrÄ±ldÄ±.\")\n",
        "\n",
        "        processed_chunks_list = [\n",
        "            pc for pc in [preprocess_text(chunk) for chunk in raw_chunks_list if chunk and str(chunk).strip()] # HÃ¼cre 6'dan\n",
        "            if pc and str(pc).strip()\n",
        "        ]\n",
        "        if not processed_chunks_list:\n",
        "            _log_rag_error(f\"Ã–n iÅŸleme sonrasÄ± kullanÄ±labilir chunk kalmadÄ± ({pdf_basename}).\")\n",
        "            return f\"HATA: Ã–n iÅŸleme sonrasÄ± kullanÄ±labilir chunk kalmadÄ± ({pdf_basename}).\", {}\n",
        "        _log_rag_info(f\"{len(processed_chunks_list)} adet chunk baÅŸarÄ±yla Ã¶n iÅŸlendi.\")\n",
        "\n",
        "        faiss_idx_built, _ = build_faiss_index(processed_chunks_list, st_embedding_model_instance) # HÃ¼cre 7'den\n",
        "        if faiss_idx_built is None:\n",
        "            _log_rag_error(f\"FAISS indeksi oluÅŸturulamadÄ± ({pdf_basename}).\")\n",
        "            return f\"HATA: FAISS indeksi oluÅŸturulamadÄ± ({pdf_basename}).\", {}\n",
        "\n",
        "        # Yeniden atama yapalÄ±m ki sonraki adÄ±mlarda doÄŸru deÄŸiÅŸkenler kullanÄ±lsÄ±n\n",
        "        processed_chunks = processed_chunks_list\n",
        "        faiss_idx = faiss_idx_built\n",
        "        save_chunks_and_index(processed_chunks, faiss_idx, file_identifier_for_cache_key, cache_directory=global_cache_dir_path) # HÃ¼cre 6'dan\n",
        "    else:\n",
        "        _log_rag_info(f\"'{file_identifier_for_cache_key}' iÃ§in Ã¶nbellekten veriler (chunklar ve FAISS indeksi) baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "        # Cache'den yÃ¼klenenleri doÄŸru deÄŸiÅŸkenlere ata\n",
        "        processed_chunks = processed_chunks_from_cache\n",
        "        faiss_idx = faiss_idx_from_cache\n",
        "\n",
        "\n",
        "    preprocessed_question_text = preprocess_text(question) # HÃ¼cre 6'dan\n",
        "    if not preprocessed_question_text:\n",
        "        _log_rag_warning(f\"Soru ('{str(question)[:30]}...') Ã¶n iÅŸleme sonrasÄ± boÅŸ kaldÄ±. Ham soru kullanÄ±lacak.\")\n",
        "        preprocessed_question_text = str(question)\n",
        "\n",
        "    relevant_context_chunks_list = retrieve_relevant_chunks( # HÃ¼cre 7'den\n",
        "        preprocessed_question_text, faiss_idx, processed_chunks,\n",
        "        st_embedding_model_instance, top_k=retrieval_top_k\n",
        "    )\n",
        "\n",
        "    if log_retrieved_chunks_content:\n",
        "        _log_rag_info(f\"FAISS ARAMA SONUCU: '{preprocessed_question_text[:50]}...' sorgusu iÃ§in {len(relevant_context_chunks_list)} chunk getirildi (top_k={retrieval_top_k}):\")\n",
        "        if relevant_context_chunks_list:\n",
        "            for i, chunk_text_retrieved in enumerate(relevant_context_chunks_list):\n",
        "                _log_rag_info(f\"--- GETÄ°RÄ°LEN CHUNK #{i+1} (ilk 100 karakter) ---\\n{str(chunk_text_retrieved)[:100]}...\\n{'-'*20}\")\n",
        "        else:\n",
        "            _log_rag_info(\"FAISS aramasÄ±ndan bu sorgu iÃ§in chunk getirilmedi.\")\n",
        "    else:\n",
        "        _log_rag_info(f\"FAISS arama: '{preprocessed_question_text[:50]}...' sorgusu iÃ§in {len(relevant_context_chunks_list)} chunk bulundu (iÃ§erik loglanmadÄ±).\")\n",
        "\n",
        "    if relevant_context_chunks_list:\n",
        "        context_for_llm_val = \"\\n\\n---\\n\\n\".join(relevant_context_chunks_list)\n",
        "    else:\n",
        "        context_for_llm_val = \"Bu soruyla ilgili spesifik bir bilgi dokÃ¼manda bulunamadÄ±.\"\n",
        "        _log_rag_warning(f\"Soru ('{preprocessed_question_text[:50]}...') iÃ§in FAISS'ten ilgili chunk bulunamadÄ±. LLM'e 'bilgi yok' mesajÄ± gÃ¶nderiliyor.\")\n",
        "\n",
        "    generated_answer_val = generate_answer_with_llm( # HÃ¼cre 8'den\n",
        "        context_for_llm_val, str(question),\n",
        "        llm_model_to_use, llm_tokenizer_to_use,\n",
        "        max_new_tokens=llm_max_new_tokens,\n",
        "        temperature=llm_temperature,\n",
        "        do_sample=llm_do_sample\n",
        "    )\n",
        "\n",
        "    evaluation_metrics_dict = evaluate_answer_quality( # HÃ¼cre 9'dan\n",
        "        generated_answer_val, str(reference_answer), st_embedding_model_instance\n",
        "    )\n",
        "\n",
        "    pipeline_end_time_val = datetime.datetime.now()\n",
        "    pipeline_duration_val = pipeline_end_time_val - pipeline_start_time\n",
        "\n",
        "    # --- SonuÃ§larÄ± Logla/YazdÄ±r (Daha OkunaklÄ±) ---\n",
        "    _log_rag_info(\"\\n\" + \"=\"*15 + \" RAG PIPELINE SONUÃ‡LARI \" + \"=\"*15)\n",
        "    _log_rag_info(f\"Ä°ÅLENEN PDF:         {pdf_basename}\")\n",
        "    _log_rag_info(f\"SORU:                {str(question)}\")\n",
        "    _log_rag_info(\"-\" * 50)\n",
        "    _log_rag_info(f\"ÃœRETÄ°LEN CEVAP:      {str(generated_answer_val)}\")\n",
        "    _log_rag_info(\"-\" * 50)\n",
        "    _log_rag_info(f\"REFERANS CEVAP:      {str(reference_answer) if reference_answer else 'N/A'}\")\n",
        "    _log_rag_info(\"=\" * 50)\n",
        "    _log_rag_info(\"DEÄERLENDÄ°RME METRÄ°KLERÄ°:\")\n",
        "\n",
        "    if isinstance(evaluation_metrics_dict, dict):\n",
        "        max_key_len = 0\n",
        "        if evaluation_metrics_dict:\n",
        "             max_key_len = max(len(key) for key in evaluation_metrics_dict.keys()) if evaluation_metrics_dict else 0\n",
        "\n",
        "        for metric_name, metric_value in evaluation_metrics_dict.items():\n",
        "            # SayÄ±sal deÄŸerleri formatla, diÄŸerlerini olduÄŸu gibi bÄ±rak\n",
        "            if isinstance(metric_value, float):\n",
        "                formatted_value = f\"{metric_value:.4f}\"\n",
        "            else:\n",
        "                formatted_value = metric_value\n",
        "            _log_rag_info(f\"  {str(metric_name).ljust(max_key_len + 2)}: {formatted_value}\")\n",
        "    else:\n",
        "        _log_rag_info(f\"  Metrikler alÄ±namadÄ±: {evaluation_metrics_dict}\")\n",
        "\n",
        "    _log_rag_info(\"=\" * 50)\n",
        "    _log_rag_info(f\"Pipeline Ã‡alÄ±ÅŸma SÃ¼resi: {pipeline_duration_val}\")\n",
        "    _log_rag_info(\"=\"*15 + \"      SONUÃ‡LAR BÄ°TTÄ°      \" + \"=\"*15 + \"\\n\")\n",
        "\n",
        "    return generated_answer_val, evaluation_metrics_dict\n",
        "\n",
        "\n",
        "if 'logger' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"Ana RAG pipeline fonksiyonu (rag_pipeline) tanÄ±mlandÄ± (metrik yazdÄ±rma gÃ¼ncellendi).\")\n",
        "else:\n",
        "    print(\"INFO: Ana RAG pipeline fonksiyonu (rag_pipeline) tanÄ±mlandÄ± (metrik yazdÄ±rma gÃ¼ncellendi).\")\n",
        "\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H10)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H10} tamamlandÄ±.\")\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H10} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H10} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "UNVDAdHL3vU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 11: Ä°ÅŸlem YapÄ±lacak PDF DosyasÄ±nÄ±n SeÃ§ilmesi (GÃœNCELLENDÄ° - Loglar DÃ¼zenlendi)\n",
        "\n",
        "CELL_NAME_H11 = \"11: Ä°ÅŸlem YapÄ±lacak PDF DosyasÄ±nÄ±n SeÃ§ilmesi\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H11)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H11} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H11} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H11} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "import os\n",
        "# PDF_KLASOR_YOLU global deÄŸiÅŸkeni HÃ¼cre 4'te tanÄ±mlandÄ±.\n",
        "# logger global deÄŸiÅŸkeni HÃ¼cre 0'da tanÄ±mlandÄ± (veya burada varlÄ±ÄŸÄ± kontrol ediliyor).\n",
        "\n",
        "pdf_dosyalari_listesi = []\n",
        "secilen_pdf_tam_yolu = None # Bu hÃ¼crede seÃ§ilecek PDF'in tam yolu\n",
        "secilen_pdf_dosya_adi = None # Bu hÃ¼crede seÃ§ilecek PDF'in sadece adÄ±\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Ä°ÅŸlem yapÄ±lacak PDF dosyasÄ±nÄ± seÃ§me sÃ¼reci baÅŸlatÄ±lÄ±yor.\")\n",
        "else:\n",
        "    print(\"INFO: Ä°ÅŸlem yapÄ±lacak PDF dosyasÄ±nÄ± seÃ§me sÃ¼reci baÅŸlatÄ±lÄ±yor.\")\n",
        "\n",
        "# PDF_KLASOR_YOLU'nun varlÄ±ÄŸÄ±nÄ± ve geÃ§erliliÄŸini kontrol et\n",
        "current_pdf_klasor_yolu = globals().get('PDF_KLASOR_YOLU', None)\n",
        "\n",
        "if not current_pdf_klasor_yolu:\n",
        "    error_msg = \"PDF klasÃ¶r yolu (PDF_KLASOR_YOLU) global deÄŸiÅŸkenlerde tanÄ±mlanmamÄ±ÅŸ veya None. LÃ¼tfen HÃ¼cre 4'Ã¼ Ã§alÄ±ÅŸtÄ±rÄ±n.\"\n",
        "    if 'logger' in globals(): logger.error(error_msg)\n",
        "    else: print(f\"ERROR: {error_msg}\")\n",
        "elif not os.path.exists(current_pdf_klasor_yolu) or not os.path.isdir(current_pdf_klasor_yolu):\n",
        "    error_msg = f\"TanÄ±mlanan PDF klasÃ¶r yolu '{current_pdf_klasor_yolu}' bulunamadÄ± veya geÃ§erli bir dizin deÄŸil.\"\n",
        "    if 'logger' in globals(): logger.error(error_msg)\n",
        "    else: print(f\"ERROR: {error_msg}\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyalarÄ± '{current_pdf_klasor_yolu}' klasÃ¶rÃ¼nden listeleniyor...\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyalarÄ± '{current_pdf_klasor_yolu}' klasÃ¶rÃ¼nden listeleniyor...\")\n",
        "    try:\n",
        "        # Sadece .pdf uzantÄ±lÄ± dosyalarÄ± al ve gizli dosyalarÄ± (Ã¶rn: ._ ile baÅŸlayanlar) hariÃ§ tut\n",
        "        pdf_dosyalari_listesi = [\n",
        "            f for f in os.listdir(current_pdf_klasor_yolu)\n",
        "            if f.lower().endswith(\".pdf\") and not f.startswith('.')\n",
        "        ]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Bulunan PDF dosyalarÄ± ({len(pdf_dosyalari_listesi)} adet): {pdf_dosyalari_listesi}\")\n",
        "        else:\n",
        "            print(f\"INFO: Bulunan PDF dosyalarÄ± ({len(pdf_dosyalari_listesi)} adet): {pdf_dosyalari_listesi}\")\n",
        "    except Exception as e:\n",
        "        error_msg = f\"PDF dosyalarÄ± ('{current_pdf_klasor_yolu}' iÃ§inden) listelenirken HATA oluÅŸtu: {e}\"\n",
        "        if 'logger' in globals(): logger.error(error_msg, exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: {error_msg}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        pdf_dosyalari_listesi = [] # Hata durumunda listeyi boÅŸalt\n",
        "\n",
        "# PDF dosyasÄ± seÃ§imi\n",
        "if not pdf_dosyalari_listesi:\n",
        "    warning_msg = f\"'{current_pdf_klasor_yolu if current_pdf_klasor_yolu else 'TanÄ±msÄ±z PDF klasÃ¶rÃ¼'}' iÃ§inde iÅŸlenecek PDF dosyasÄ± bulunamadÄ±.\"\n",
        "    if 'logger' in globals(): logger.warning(warning_msg)\n",
        "    print(f\"âš ï¸  {warning_msg}\") # KullanÄ±cÄ±ya her zaman gÃ¶ster\n",
        "else:\n",
        "    print(f\"\\nğŸ“‚ '{current_pdf_klasor_yolu}' klasÃ¶rÃ¼nde bulunan PDF dosyalarÄ±:\")\n",
        "    for i, dosya_adi_listede in enumerate(pdf_dosyalari_listesi):\n",
        "        print(f\"   {i+1}. {dosya_adi_listede}\")\n",
        "\n",
        "    if len(pdf_dosyalari_listesi) == 1:\n",
        "        # EÄŸer sadece bir PDF varsa, onu otomatik seÃ§\n",
        "        secilen_pdf_dosya_adi = pdf_dosyalari_listesi[0]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"KlasÃ¶rde tek PDF dosyasÄ± bulundu ve otomatik olarak seÃ§ildi: {secilen_pdf_dosya_adi}\")\n",
        "        print(f\"\\nâœ… Tek PDF dosyasÄ± bulundu ve otomatik olarak seÃ§ildi: {secilen_pdf_dosya_adi}\")\n",
        "    else:\n",
        "        # Birden fazla PDF varsa kullanÄ±cÄ±dan seÃ§im iste\n",
        "        while True:\n",
        "            try:\n",
        "                secim_no_str = input(f\"\\nâ¡ï¸ LÃ¼tfen iÅŸlem yapmak istediÄŸiniz PDF'in numarasÄ±nÄ± girin (1-{len(pdf_dosyalari_listesi)}): \").strip()\n",
        "                if 'logger' in globals(): logger.debug(f\"KullanÄ±cÄ±nÄ±n PDF seÃ§imi iÃ§in girdisi: '{secim_no_str}'\")\n",
        "\n",
        "                if not secim_no_str: # BoÅŸ giriÅŸ\n",
        "                    print(\"âš ï¸ GiriÅŸ yapÄ±lmadÄ±. LÃ¼tfen bir numara girin.\")\n",
        "                    if 'logger' in globals(): logger.warning(\"KullanÄ±cÄ± PDF seÃ§imi iÃ§in boÅŸ giriÅŸ yaptÄ±.\")\n",
        "                    continue\n",
        "\n",
        "                secim_no = int(secim_no_str)\n",
        "                if 1 <= secim_no <= len(pdf_dosyalari_listesi):\n",
        "                    secilen_pdf_dosya_adi = pdf_dosyalari_listesi[secim_no - 1]\n",
        "                    if 'logger' in globals():\n",
        "                        logger.info(f\"KullanÄ±cÄ± tarafÄ±ndan PDF seÃ§ildi. SeÃ§im No: {secim_no}, Dosya AdÄ±: '{secilen_pdf_dosya_adi}'\")\n",
        "                    break # GeÃ§erli seÃ§im yapÄ±ldÄ±, dÃ¶ngÃ¼den Ã§Ä±k\n",
        "                else:\n",
        "                    print(f\"âš ï¸ GeÃ§ersiz numara. LÃ¼tfen 1 ile {len(pdf_dosyalari_listesi)} arasÄ±nda bir numara girin.\")\n",
        "                    if 'logger' in globals(): logger.warning(f\"KullanÄ±cÄ±nÄ±n girdiÄŸi PDF seÃ§im numarasÄ± ({secim_no}) geÃ§ersiz.\")\n",
        "            except ValueError:\n",
        "                print(\"âš ï¸ GeÃ§ersiz giriÅŸ. LÃ¼tfen sayÄ±sal bir deÄŸer (numara) girin.\")\n",
        "                if 'logger' in globals(): logger.warning(\"KullanÄ±cÄ± PDF seÃ§imi iÃ§in sayÄ±sal olmayan bir deÄŸer girdi.\")\n",
        "            except EOFError: # KullanÄ±cÄ± input'u keserse (Ã¶rn: Colab'da stop butonu)\n",
        "                 print(\"\\nğŸš« PDF seÃ§imi iptal edildi.\")\n",
        "                 if 'logger' in globals(): logger.info(\"KullanÄ±cÄ± PDF seÃ§imini EOF ile iptal etti.\")\n",
        "                 secilen_pdf_dosya_adi = None # SeÃ§im yapÄ±lmadÄ±\n",
        "                 break\n",
        "            except Exception as e_input: # Beklenmedik diÄŸer hatalar\n",
        "                error_msg_input = f\"PDF seÃ§imi sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu: {e_input}\"\n",
        "                if 'logger' in globals(): logger.error(error_msg_input, exc_info=True)\n",
        "                else: print(f\"ERROR: {error_msg_input}\")\n",
        "                secilen_pdf_dosya_adi = None # SeÃ§im yapÄ±lamadÄ±\n",
        "                break\n",
        "\n",
        "# SeÃ§ilen PDF iÃ§in tam yolu oluÅŸtur\n",
        "if secilen_pdf_dosya_adi and current_pdf_klasor_yolu:\n",
        "    secilen_pdf_tam_yolu = os.path.join(current_pdf_klasor_yolu, secilen_pdf_dosya_adi)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF seÃ§imi tamamlandÄ±. Ãœzerinde Ã§alÄ±ÅŸÄ±lacak PDF: {secilen_pdf_tam_yolu}\")\n",
        "    print(f\"\\nâœ… PDF seÃ§imi baÅŸarÄ±yla tamamlandÄ±.\")\n",
        "    print(f\"ğŸ“„ Ãœzerinde Ã§alÄ±ÅŸÄ±lacak PDF dosyasÄ±: {secilen_pdf_tam_yolu}\")\n",
        "elif not pdf_dosyalari_listesi:\n",
        "    # Zaten yukarÄ±da uyarÄ± verildi, burada ek bir log yeterli.\n",
        "    if 'logger' in globals(): logger.warning(\"Ä°ÅŸlenecek PDF dosyasÄ± bulunamadÄ±ÄŸÄ± iÃ§in seÃ§im yapÄ±lamadÄ±.\")\n",
        "else: # SeÃ§im iptal edildi veya bir hata oluÅŸtu\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"PDF seÃ§imi yapÄ±lmadÄ± veya bir hata nedeniyle tamamlanamadÄ±.\")\n",
        "    print(\"\\nâŒ PDF seÃ§imi yapÄ±lmadÄ± veya bir hata nedeniyle tamamlanamadÄ±.\")\n",
        "    secilen_pdf_tam_yolu = None # Emin olmak iÃ§in None yapalÄ±m\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H11} tamamlandÄ±.\")\n",
        "    log_cell_end(CELL_NAME_H11)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H11} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H11} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "EsljbV1K4bL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HÃ¼cre 12: Soru GiriÅŸi, Referans Cevap GiriÅŸi ve RAG Pipeline'Ä±nÄ± Ã‡alÄ±ÅŸtÄ±rma (GÃœNCELLENDÄ° - Max_Tokens 65'e Ã‡Ä±karÄ±ldÄ±)\n",
        "\n",
        "CELL_NAME_H12 = \"12: RAG Pipeline Ã‡alÄ±ÅŸtÄ±rma ve DeÄŸerlendirme\"\n",
        "# log_cell_start ve logger'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H12)\n",
        "    logger.info(f\"HÃ¼cre {CELL_NAME_H12} baÅŸlatÄ±ldÄ±.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H12} BAÅLADI (log_cell_start veya logger bulunamadÄ±) ---\")\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H12} baÅŸlatÄ±ldÄ±.\")\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import logging # Logger seviyesini kontrol etmek iÃ§in\n",
        "\n",
        "# Lokal loglama yardÄ±mcÄ± fonksiyonlarÄ± (HÃ¼cre 12 iÃ§in)\n",
        "log_active_h12 = 'logger' in globals()\n",
        "def _log_h12_info(message):\n",
        "    if log_active_h12: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_h12_warning(message):\n",
        "    if log_active_h12: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_h12_error(message, exc_info=False):\n",
        "    if log_active_h12: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_h12_debug(message):\n",
        "    if log_active_h12 and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG): # logging import edildiÄŸinden emin ol\n",
        "        logger.debug(message)\n",
        "\n",
        "ready_to_run_pipeline = False\n",
        "current_selected_pdf_path = globals().get('secilen_pdf_tam_yolu', None)\n",
        "llm_model_instance = globals().get('llm_model', None)\n",
        "llm_tokenizer_instance = globals().get('llm_tokenizer', None)\n",
        "st_embedding_model_instance_main = globals().get('embedding_model_st', None)\n",
        "\n",
        "if current_selected_pdf_path and os.path.exists(current_selected_pdf_path):\n",
        "    if llm_model_instance and llm_tokenizer_instance and st_embedding_model_instance_main:\n",
        "        ready_to_run_pipeline = True\n",
        "        _log_h12_info(f\"Pipeline Ã§alÄ±ÅŸtÄ±rmak iÃ§in Ã¶n koÅŸullar tamam. SeÃ§ilen PDF: {os.path.basename(current_selected_pdf_path)}\")\n",
        "        print(f\"ğŸ“– Ãœzerinde Ã§alÄ±ÅŸÄ±lacak PDF: {current_selected_pdf_path}\")\n",
        "    else:\n",
        "        error_msg_models = \"Gerekli modeller (LLM, Tokenizer, Embedding) yÃ¼klenmemiÅŸ. LÃ¼tfen HÃ¼cre 5'i kontrol edin.\"\n",
        "        _log_h12_error(error_msg_models)\n",
        "        print(\"âŒ Gerekli modeller yÃ¼klenmemiÅŸ. Pipeline Ã§alÄ±ÅŸtÄ±rÄ±lamaz.\")\n",
        "else:\n",
        "    error_msg_pdf = \"Ã‡alÄ±ÅŸtÄ±rÄ±lacak PDF dosyasÄ± seÃ§ilmemiÅŸ veya bulunamadÄ±. LÃ¼tfen HÃ¼cre 11'i kontrol edin.\"\n",
        "    _log_h12_error(error_msg_pdf)\n",
        "    print(\"âŒ Ã‡alÄ±ÅŸtÄ±rÄ±lacak PDF dosyasÄ± seÃ§ilmemiÅŸ/bulunamadÄ±. Pipeline Ã§alÄ±ÅŸtÄ±rÄ±lamaz.\")\n",
        "\n",
        "if ready_to_run_pipeline:\n",
        "    RAG_USE_SEMANTIC_CHUNKER = False\n",
        "    RAG_RECURSIVE_CHUNK_SIZE = 320\n",
        "    RAG_RECURSIVE_CHUNK_OVERLAP = 100\n",
        "    RAG_SEMANTIC_THRESHOLD_TYPE = \"percentile\"\n",
        "    RAG_SEMANTIC_THRESHOLD_AMOUNT = 95\n",
        "    RAG_RETRIEVAL_TOP_K = 5 # Bir Ã¶nceki baÅŸarÄ±lÄ± ayar\n",
        "\n",
        "    # *** DEÄÄ°ÅÄ°KLÄ°K: RAG_LLM_MAX_NEW_TOKENS artÄ±rÄ±ldÄ± ***\n",
        "    RAG_LLM_MAX_NEW_TOKENS = 65 # Ã–nceki 60 idi.\n",
        "    # *****************************************************\n",
        "\n",
        "    RAG_LLM_TEMPERATURE = 0.01\n",
        "    RAG_LLM_DO_SAMPLE = True\n",
        "    RAG_LOG_RETRIEVED_CHUNKS_CONTENT = False\n",
        "\n",
        "    _log_h12_info(\"KullanÄ±lacak RAG Pipeline AyarlarÄ± (GÃ¼ncellendi - Son RÃ¶tuÅŸ):\")\n",
        "    if RAG_USE_SEMANTIC_CHUNKER:\n",
        "        _log_h12_info(f\"  ParÃ§alama (Chunking): Anlamsal (Semantic), Threshold Tipi='{RAG_SEMANTIC_THRESHOLD_TYPE}', Miktar='{RAG_SEMANTIC_THRESHOLD_AMOUNT}'\")\n",
        "    else:\n",
        "        _log_h12_info(f\"  ParÃ§alama (Chunking): Yinelemeli (Recursive), Boyut (Size)={RAG_RECURSIVE_CHUNK_SIZE}, Ã–rtÃ¼ÅŸme (Overlap)={RAG_RECURSIVE_CHUNK_OVERLAP}\")\n",
        "    _log_h12_info(f\"  Getirme (Retrieval): Top K={RAG_RETRIEVAL_TOP_K}\")\n",
        "    _log_h12_info(f\"  LLM Ãœretim (Generation): Max Yeni Token={RAG_LLM_MAX_NEW_TOKENS} (DeÄŸiÅŸtirildi), SÄ±caklÄ±k (Temp)={RAG_LLM_TEMPERATURE if RAG_LLM_DO_SAMPLE else 'N/A'}, Ã–rnekleme (Sample)={RAG_LLM_DO_SAMPLE}\")\n",
        "    _log_h12_info(f\"  Getirilen Chunk Ä°Ã§erikleri LoglansÄ±n mÄ±?: {RAG_LOG_RETRIEVED_CHUNKS_CONTENT}\")\n",
        "\n",
        "    def run_rag_pipeline_with_settings(pdf_file_path, user_question, reference_answer_text):\n",
        "        _log_h12_info(f\"RAG Pipeline Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor. Soru (ilk 50): '{str(user_question)[:50]}...'\")\n",
        "        return rag_pipeline(\n",
        "            pdf_path=pdf_file_path,\n",
        "            question=user_question,\n",
        "            reference_answer=reference_answer_text,\n",
        "            llm_model_to_use=llm_model_instance,\n",
        "            llm_tokenizer_to_use=llm_tokenizer_instance,\n",
        "            st_embedding_model_instance=st_embedding_model_instance_main,\n",
        "            lc_embedding_model_for_semantic_instance=globals().get('langchain_embeddings_for_semantic_chunker', None),\n",
        "            use_semantic_chunker=RAG_USE_SEMANTIC_CHUNKER,\n",
        "            recursive_chunk_size=RAG_RECURSIVE_CHUNK_SIZE,\n",
        "            recursive_chunk_overlap=RAG_RECURSIVE_CHUNK_OVERLAP,\n",
        "            semantic_chunker_threshold_type=RAG_SEMANTIC_THRESHOLD_TYPE,\n",
        "            semantic_chunker_threshold_amount=RAG_SEMANTIC_THRESHOLD_AMOUNT,\n",
        "            retrieval_top_k=RAG_RETRIEVAL_TOP_K,\n",
        "            llm_max_new_tokens=RAG_LLM_MAX_NEW_TOKENS, # GÃ¼ncel deÄŸer\n",
        "            llm_temperature=RAG_LLM_TEMPERATURE,\n",
        "            llm_do_sample=RAG_LLM_DO_SAMPLE,\n",
        "            log_retrieved_chunks_content=RAG_LOG_RETRIEVED_CHUNKS_CONTENT\n",
        "        )\n",
        "\n",
        "    default_question = \"YÃ¼ksek lisans programlarÄ±na Ã¶ÄŸrenci kabulÃ¼ ne zaman yapÄ±lÄ±r?\"\n",
        "    default_reference_answer = \"YÃ¼ksek lisans programlarÄ±na, gÃ¼z ve bahar yarÄ±yÄ±llarÄ± baÅŸÄ±nda Ã¶ÄŸrenci alÄ±nabilir.\"\n",
        "\n",
        "    _log_h12_info(f\"--- VarsayÄ±lan Soru ile RAG Pipeline Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor (Son RÃ¶tuÅŸ AyarlarÄ±) ---\")\n",
        "    _log_h12_info(f\"VarsayÄ±lan Soru: {default_question}\")\n",
        "    _log_h12_info(f\"VarsayÄ±lan Referans Cevap: {default_reference_answer}\")\n",
        "\n",
        "    final_generated_answer, final_evaluation_metrics = run_rag_pipeline_with_settings(\n",
        "        current_selected_pdf_path,\n",
        "        default_question,\n",
        "        default_reference_answer\n",
        "    )\n",
        "    _log_h12_info(f\"VARSAYILAN SORU Ä°Ã‡Ä°N PIPELINE TAMAMLANDI (Son RÃ¶tuÅŸ AyarlarÄ±). Cevap (ilk 50): '{str(final_generated_answer)[:50]}...', BERTScore_F1: {final_evaluation_metrics.get('BERTScore_F1', 'N/A')}, Token_F1: {final_evaluation_metrics.get('Token_F1', 'N/A')}\")\n",
        "\n",
        "    _log_h12_info(\"KullanÄ±cÄ±ya baÅŸka soru sorma dÃ¶ngÃ¼sÃ¼ baÅŸlatÄ±lÄ±yor.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_response_for_new_question = input(\"\\nğŸ”„ BaÅŸka bir soru sormak ister misiniz? (evet/e veya hayÄ±r/h): \").strip().lower()\n",
        "            _log_h12_debug(f\"KullanÄ±cÄ±nÄ±n yeni soru sorma isteÄŸi: '{user_response_for_new_question}'\")\n",
        "\n",
        "            if user_response_for_new_question in [\"evet\", \"e\"]:\n",
        "                new_user_question = input(\"\\nâ“ LÃ¼tfen yeni sorunuzu girin: \").strip()\n",
        "                if not new_user_question:\n",
        "                    print(\"âš ï¸ BoÅŸ soru girdiniz. LÃ¼tfen geÃ§erli bir soru sorun.\")\n",
        "                    _log_h12_warning(\"KullanÄ±cÄ± yeni soru iÃ§in boÅŸ giriÅŸ yaptÄ±.\")\n",
        "                    continue\n",
        "\n",
        "                new_reference_answer = input(\"ğŸ¯ Bu soru iÃ§in ideal (referans) cevabÄ± girin (yoksa boÅŸ bÄ±rakÄ±n veya 'yok' yazÄ±n): \").strip()\n",
        "                if not new_reference_answer or new_reference_answer.lower() == 'yok':\n",
        "                    new_reference_answer = \"\"\n",
        "                    _log_h12_debug(\"Yeni soru iÃ§in referans cevap girilmedi.\")\n",
        "                else:\n",
        "                    _log_h12_debug(f\"Yeni soru iÃ§in referans cevap: '{str(new_reference_answer)[:50]}...'\")\n",
        "\n",
        "                _log_h12_info(f\"--- KullanÄ±cÄ±nÄ±n Yeni Sorusu ile RAG Pipeline Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor (Son RÃ¶tuÅŸ AyarlarÄ±) ---\")\n",
        "                final_generated_answer, final_evaluation_metrics = run_rag_pipeline_with_settings(\n",
        "                    current_selected_pdf_path,\n",
        "                    new_user_question,\n",
        "                    new_reference_answer\n",
        "                )\n",
        "                _log_h12_info(f\"YENÄ° SORU Ä°Ã‡Ä°N PIPELINE TAMAMLANDI (Son RÃ¶tuÅŸ AyarlarÄ±). Cevap (ilk 50): '{str(final_generated_answer)[:50]}...', BERTScore_F1: {final_evaluation_metrics.get('BERTScore_F1', 'N/A')}, Token_F1: {final_evaluation_metrics.get('Token_F1', 'N/A')}\")\n",
        "\n",
        "            elif user_response_for_new_question in [\"hayÄ±r\", \"h\"]:\n",
        "                _log_h12_info(\"KullanÄ±cÄ± iÅŸlemi sonlandÄ±rdÄ±.\")\n",
        "                print(\"\\nğŸ›‘ Ä°ÅŸlem sonlandÄ±rÄ±ldÄ±. Ä°yi gÃ¼nler!\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"âš ï¸ GeÃ§ersiz yanÄ±t. LÃ¼tfen 'evet' (veya 'e') ya da 'hayÄ±r' (veya 'h') girin.\")\n",
        "                _log_h12_warning(f\"KullanÄ±cÄ±nÄ±n yeni soru sorma isteÄŸi iÃ§in geÃ§ersiz yanÄ±t: '{user_response_for_new_question}'.\")\n",
        "        except EOFError:\n",
        "            _log_h12_info(\"KullanÄ±cÄ± soru sorma dÃ¶ngÃ¼sÃ¼nÃ¼ EOF ile sonlandÄ±rdÄ±.\")\n",
        "            print(\"\\nğŸ›‘ Ä°ÅŸlem (EOF ile) sonlandÄ±rÄ±ldÄ±.\")\n",
        "            break\n",
        "        except Exception as e_loop:\n",
        "            error_msg_loop = f\"Soru sorma dÃ¶ngÃ¼sÃ¼nde beklenmedik bir hata oluÅŸtu: {e_loop}\"\n",
        "            _log_h12_error(error_msg_loop, exc_info=True)\n",
        "            print(\"ğŸ” DÃ¶ngÃ¼de bir sorun oluÅŸtu, lÃ¼tfen tekrar deneyin veya 'hayÄ±r' ile Ã§Ä±kÄ±n.\")\n",
        "\n",
        "if not ready_to_run_pipeline:\n",
        "    _log_h12_warning(\"Pipeline Ã¶n koÅŸullarÄ± saÄŸlanamadÄ±ÄŸÄ± iÃ§in RAG iÅŸlemi Ã§alÄ±ÅŸtÄ±rÄ±lmadÄ±.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H12)\n",
        "else:\n",
        "    print(f\"HÃ¼cre {CELL_NAME_H12} tamamlandÄ±.\")\n",
        "    print(f\"--- {CELL_NAME_H12} TAMAMLANDI (log_cell_end veya logger bulunamadÄ±) ---\")"
      ],
      "metadata": {
        "id": "8x4Rrw0j4tQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}