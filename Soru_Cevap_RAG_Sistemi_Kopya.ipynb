{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOKYLRuRoqu5vGtioTSFmZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehmetaksoy/Soru-Cevap-Sistemi-RAG/blob/main/Soru_Cevap_RAG_Sistemi_Kopya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUrhtORDz3qv"
      },
      "outputs": [],
      "source": [
        "# Hücre 1: Gerekli Kütüphanelerin Kurulumu (GÜNCELLENDİ - Loglama Eklendi)\n",
        "\n",
        "# Hücre 0'daki log_cell_start ve log_cell_end fonksiyonlarının bu hücrede de erişilebilir olduğunu varsayıyoruz.\n",
        "# Güvenlik için, eğer fonksiyonlar bulunamazsa diye bir kontrol eklenebilir.\n",
        "# Şimdilik Hücre 0'ın çalıştırıldığını ve logger'ın ayarlandığını varsayıyoruz.\n",
        "\n",
        "CELL_NAME_H1 = \"1: Gerekli Kütüphanelerin Kurulumu\"\n",
        "if 'log_cell_start' in globals() and 'logger' in globals(): # logger'ın da varlığını kontrol et\n",
        "    log_cell_start(CELL_NAME_H1)\n",
        "    logger.info(\"Hücre 1 başlatıldı: Gerekli kütüphanelerin kurulumu.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H1} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(\"Hücre 1 başlatıldı: Gerekli kütüphanelerin kurulumu.\")\n",
        "\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"pip paket yöneticisi güncelleniyor...\")\n",
        "else:\n",
        "    print(\"pip paket yöneticisi güncelleniyor...\")\n",
        "!pip install --upgrade pip -q\n",
        "\n",
        "# Temel RAG ve LLM kütüphaneleri\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel RAG ve LLM kütüphaneleri kuruluyor: transformers, sentence-transformers, pdfplumber, faiss-cpu, nltk, rouge-score, scikit-learn, bert_score, langchain, torch, accelerate, bitsandbytes\")\n",
        "else:\n",
        "    print(\"Temel RAG ve LLM kütüphaneleri kuruluyor...\")\n",
        "!pip install transformers sentence-transformers pdfplumber faiss-cpu nltk rouge-score scikit-learn bert_score langchain torch accelerate bitsandbytes -q\n",
        "\n",
        "# Langchain topluluk ve deneysel kütüphaneleri\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Langchain topluluk ve deneysel kütüphaneleri kuruluyor: langchain_community, langchain_experimental\")\n",
        "else:\n",
        "    print(\"Langchain topluluk ve deneysel kütüphaneleri kuruluyor...\")\n",
        "!pip install langchain_community langchain_experimental -q\n",
        "\n",
        "# BLEURT Kurulumu\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BLEURT ve tf-slim (GitHub'dan) kuruluyor...\")\n",
        "else:\n",
        "    print(\"🔄 BLEURT ve tf-slim kuruluyor (GitHub'dan)...\")\n",
        "!pip install git+https://github.com/google-research/bleurt.git tf-slim -q\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"BLEURT ve tf-slim kurulum denemesi tamamlandı.\")\n",
        "else:\n",
        "    print(\"✅ BLEURT ve tf-slim kurulum denemesi tamamlandı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(\"Kütüphane kurulumları tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H1)\n",
        "else:\n",
        "    print(\"Kütüphane kurulumları tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H1} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 2: Hugging Face Girişi (Sabit Token ile) (GÜNCELLENDİ - Loglama Eklendi)\n",
        "\n",
        "CELL_NAME_H2 = \"2: Hugging Face Girişi\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H2)\n",
        "    logger.info(\"Hücre 2 başlatıldı: Hugging Face girişi.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H2} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(\"Hücre 2 başlatıldı: Hugging Face girişi.\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os # os import'u burada tekrar edilse de zararı olmaz, zaten edilmiş olabilir.\n",
        "\n",
        "# Sabit token'ı doğrudan kullanıyoruz\n",
        "hf_token_sabit = \"hf_**************************************************************\" # <--- BU TOKEN GENEL KULLANIMA AÇIK OLMAMALI, GEREKİRSE ORTAM DEĞİŞKENİNDEN ALINMALI\n",
        "hf_token_to_use = None # Kullanılacak nihai token\n",
        "\n",
        "if hf_token_sabit and hf_token_sabit != \"YOUR_PLACEHOLDER_TOKEN\" and hf_token_sabit.startswith(\"hf_\"):\n",
        "    hf_token_to_use = hf_token_sabit\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Sabit Hugging Face tokenı kullanılacak.\")\n",
        "    else:\n",
        "        print(\"INFO: Sabit Hugging Face tokenı kullanılacak.\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"Geçerli bir sabit Hugging Face tokenı (hf_token_sabit) tanımlanmamış veya formatı yanlış.\")\n",
        "    else:\n",
        "        print(\"WARNING: Geçerli bir sabit Hugging Face tokenı (hf_token_sabit) tanımlanmamış veya formatı yanlış.\")\n",
        "\n",
        "if hf_token_to_use:\n",
        "    try:\n",
        "        login(token=hf_token_to_use)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"Hugging Face'e başarıyla giriş yapıldı.\")\n",
        "        else:\n",
        "            print(\"INFO: Hugging Face'e başarıyla giriş yapıldı.\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Hugging Face girişi başarısız oldu. Hata: {e}\")\n",
        "        else:\n",
        "            print(f\"ERROR: Hugging Face girişi başarısız oldu. Hata: {e}\")\n",
        "else:\n",
        "    # Yukarıdaki if bloğunda zaten uyarı verildi, burada ek bir loglama yapılabilir veya pas geçilebilir.\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Hugging Face tokenı sağlanmadığı için giriş denemesi yapılmadı.\")\n",
        "    else:\n",
        "        print(\"INFO: Hugging Face tokenı sağlanmadığı için giriş denemesi yapılmadı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(\"Hugging Face giriş işlemleri tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H2)\n",
        "else:\n",
        "    print(\"Hugging Face giriş işlemleri tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H2} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "I34YPyNY0nyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 3: Kütüphane İçe Aktarma, NLTK Kurulumu, BLEURT Modül Kontrolü (GÜNCELLENDİ - Loglama Eklendi)\n",
        "\n",
        "CELL_NAME_H3 = \"3: Kütüphane İçe Aktarma, NLTK ve BLEURT Yönetimi\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H3)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H3} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H3} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H3} başlatıldı.\")\n",
        "\n",
        "# --- BÖLÜM A: Gerekli Kütüphanelerin İçe Aktarılması ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- Bölüm A: Kütüphaneler İçe Aktarılıyor ---\")\n",
        "else:\n",
        "    print(\"--- Bölüm A: Kütüphaneler İçe Aktarılıyor ---\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "import nltk # nltk.download kullanmadan önce import edilmeli\n",
        "import torch\n",
        "import faiss\n",
        "import hashlib\n",
        "import pickle\n",
        "import zipfile # Manuel zip açma için\n",
        "\n",
        "# NLTK alt modüllerini, NLTK ana veri yolu ayarlandıktan ve kaynaklar indirildikten SONRA import etmek daha güvenli olabilir.\n",
        "# Ancak stopwords ve tokenizers genellikle temel nltk kurulumu ile gelir veya erken import edilebilir.\n",
        "# Şimdilik burada bırakıyoruz, sorun olursa yerlerini değiştirebiliriz.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from rouge_score import rouge_scorer # nltk.translate'den farklı, kendi başına bir kütüphane\n",
        "from bert_score import score as bert_score_L # İsim çakışmasını önlemek için bert_score_L olarak bırakıldı\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Langchain importları\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel kütüphaneler içe aktarıldı.\")\n",
        "else:\n",
        "    print(\"Temel kütüphaneler içe aktarıldı.\")\n",
        "\n",
        "# BLEURT kütüphanesini koşullu olarak içe aktarma\n",
        "bleurt_available = False\n",
        "bleurt_score_module_global_for_h3_and_h9 = None # Hücre 9'da scorer'ı başlatmak için kullanılacak, global yaptık\n",
        "try:\n",
        "    from bleurt import score as bleurt_score_module_temp # geçici bir isimle al\n",
        "    bleurt_score_module_global_for_h3_and_h9 = bleurt_score_module_temp # globale ata\n",
        "    bleurt_available = True\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"BLEURT kütüphanesi (bleurt.score modülü) başarıyla içe aktarıldı ve global değişkene atandı.\")\n",
        "    else:\n",
        "        print(\"INFO: BLEURT kütüphanesi (bleurt.score modülü) başarıyla içe aktarıldı ve global değişkene atandı.\")\n",
        "except ImportError:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"BLEURT kütüphanesi bulunamadı veya içe aktarılamadı. Kurulumda bir sorun olabilir. BLEURT skorlaması atlanacak.\")\n",
        "    else:\n",
        "        print(\"WARNING: BLEURT kütüphanesi bulunamadı veya içe aktarılamadı. Kurulumda bir sorun olabilir. BLEURT skorlaması atlanacak.\")\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"BLEURT içe aktarılırken beklenmedik bir hata: {e}. BLEURT skorlaması atlanacak.\")\n",
        "    else:\n",
        "        print(f\"ERROR: BLEURT içe aktarılırken beklenmedik bir hata: {e}. BLEURT skorlaması atlanacak.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Bölüm A (Kütüphane İçe Aktarma) tamamlandı.\")\n",
        "else:\n",
        "    print(\"Bölüm A (Kütüphane İçe Aktarma) tamamlandı.\")\n",
        "\n",
        "# --- BÖLÜM B: NLTK Kaynaklarının İndirilmesi ve Hazırlanması ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- Bölüm B: NLTK Kaynak Yönetimi Başlatılıyor ---\")\n",
        "else:\n",
        "    print(\"--- Bölüm B: NLTK Kaynak Yönetimi Başlatılıyor ---\")\n",
        "\n",
        "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK ana veri dizini olarak '{nltk_data_dir}' kullanılacak/oluşturulacak.\")\n",
        "else:\n",
        "    print(f\"NLTK ana veri dizini olarak '{nltk_data_dir}' kullanılacak/oluşturulacak.\")\n",
        "\n",
        "# Gerekli alt dizinlerin varlığını kontrol et/oluştur\n",
        "# nltk.download() bu dizinleri zaten oluşturur, ancak manuel kontrol de zararsızdır.\n",
        "os.makedirs(os.path.join(nltk_data_dir, 'corpora'), exist_ok=True)\n",
        "os.makedirs(os.path.join(nltk_data_dir, 'tokenizers'), exist_ok=True)\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK için gerekli alt dizinler ('corpora', 'tokenizers') '{nltk_data_dir}' altında kontrol edildi/oluşturuldu.\")\n",
        "else:\n",
        "    print(f\"NLTK için gerekli alt dizinler ('corpora', 'tokenizers') '{nltk_data_dir}' altında kontrol edildi/oluşturuldu.\")\n",
        "\n",
        "# NLTK'nın arama yolunu güncelle (nltk_data_dir'i başa ekle)\n",
        "if nltk_data_dir not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, nltk_data_dir)\n",
        "elif nltk.data.path[0] != nltk_data_dir: # Eğer listede var ama başta değilse, silip başa ekle\n",
        "    nltk.data.path.remove(nltk_data_dir)\n",
        "    nltk.data.path.insert(0, nltk_data_dir)\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"NLTK veri yolu güncellendi. Aktif yollar: {nltk.data.path}\")\n",
        "else:\n",
        "    print(f\"NLTK veri yolu güncellendi. Aktif yollar: {nltk.data.path}\")\n",
        "\n",
        "resource_configs = {\n",
        "    'wordnet':   {'zip_filename': 'wordnet.zip',   'type_dir_name': 'corpora',    'verification_path': 'corpora/wordnet',   'manual_unzip_needed': True, 'unzip_target_name': 'wordnet'},\n",
        "    'omw-1.4':   {'zip_filename': 'omw-1.4.zip',   'type_dir_name': 'corpora',    'verification_path': 'corpora/omw-1.4',   'manual_unzip_needed': True, 'unzip_target_name': 'omw-1.4'},\n",
        "    'punkt':     {'zip_filename': 'punkt.zip',     'type_dir_name': 'tokenizers', 'verification_path': 'tokenizers/punkt',  'manual_unzip_needed': False}, # NLTK kendi açar\n",
        "    'stopwords': {'zip_filename': 'stopwords.zip', 'type_dir_name': 'corpora',    'verification_path': 'corpora/stopwords', 'manual_unzip_needed': False}, # NLTK kendi açar\n",
        "    # 'punkt_tab' genellikle 'punkt' ile birlikte gelir veya daha az yaygındır. Sorun çıkarırsa kaldırılabilir.\n",
        "    # Şimdilik koruyoruz, çıktıda başarılı görünüyordu.\n",
        "    'punkt_tab': {'zip_filename': 'punkt_tab.zip', 'type_dir_name': 'tokenizers', 'verification_path': 'tokenizers/punkt_tab', 'manual_unzip_needed': False}\n",
        "}\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- NLTK kaynakları indirme ve doğrulama işlemi başlıyor ---\")\n",
        "else:\n",
        "    print(\"--- NLTK kaynakları indirme ve doğrulama işlemi başlıyor ---\")\n",
        "\n",
        "for resource_id, config in resource_configs.items():\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"'{resource_id}' NLTK kaynağı işleniyor...\")\n",
        "    print(f\"\\n🔄 '{resource_id}' NLTK kaynağı için işlem yapılıyor...\")\n",
        "\n",
        "    type_dir_absolute_path = os.path.join(nltk_data_dir, config['type_dir_name'])\n",
        "    # zip dosyasının tam yolu (eğer manuel açma gerekiyorsa)\n",
        "    actual_zip_file_path = os.path.join(type_dir_absolute_path, config['zip_filename'])\n",
        "    # açılmış dizinin tam yolu (eğer manuel açma gerekiyorsa)\n",
        "    unzipped_target_dir_absolute_path = os.path.join(type_dir_absolute_path, config.get('unzip_target_name', resource_id))\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 1. Adım: NLTK ile indirmeyi dene\n",
        "        print(f\"   -> '{resource_id}' NLTK ile '{nltk_data_dir}' dizinine indiriliyor/kontrol ediliyor...\")\n",
        "        # quiet=False NLTK'nın kendi loglarını gösterir, raise_on_error=True hata durumunda exception fırlatır.\n",
        "        nltk.download(resource_id, download_dir=nltk_data_dir, quiet=False, raise_on_error=True)\n",
        "        print(f\"   -> '{resource_id}' için NLTK download komutu başarıyla tamamlandı (veya kaynak zaten günceldi).\")\n",
        "\n",
        "        # 2. Adım: Manuel açma (eğer gerekiyorsa ve NLTK otomatik yapmadıysa)\n",
        "        # NLTK'nın download fonksiyonu genellikle zip dosyalarını kendisi açar (extract=True varsayılan).\n",
        "        # Bu yüzden manuel_unzip_needed=True olanlar için bile, önce açılıp açılmadığını kontrol etmek iyi bir fikir.\n",
        "        # Şimdiki kodunuzda NLTK'nın açmasına güveniliyor (manuel_unzip=False olanlar için) veya üzerine yazılıyor (manuel_unzip=True olanlar için).\n",
        "        # Bu yaklaşımı koruyalım, çünkü önceki çıktıda çalışmıştı.\n",
        "        if config.get('manual_unzip_needed', False):\n",
        "            print(f\"   -> '{resource_id}' için manuel ZIP açma işlemi kontrol ediliyor/yapılıyor...\")\n",
        "            if not os.path.exists(actual_zip_file_path):\n",
        "                # Bu durum, nltk.download'ın zip'i indirip sildiği veya hiç indirmediği anlamına gelebilir.\n",
        "                # Eğer nltk.download başarılı olduysa ve kaynak doğrulandıysa, zip'in olmaması sorun değil.\n",
        "                # Ancak manuel açma için zip dosyası gerekiyorsa bu bir sorundur.\n",
        "                # 'punkt_tab' gibi bazı kaynaklar için ayrı zip olmayabilir, NLTK'nın kendi yönetimine bırakmak daha iyi.\n",
        "                if 'logger' in globals():\n",
        "                    logger.warning(f\"Manuel açma için beklenen '{actual_zip_file_path}' ZIP dosyası bulunamadı. NLTK'nın kaynağı zaten açmış veya farklı bir şekilde yönetmiş olması umuluyor.\")\n",
        "                print(f\"   -> ⚠️ UYARI: Manuel açma için '{actual_zip_file_path}' ZIP dosyası bulunamadı. Kaynağın zaten NLTK tarafından açıldığı varsayılıyor.\")\n",
        "            elif os.path.exists(actual_zip_file_path): # Zip dosyası varsa ve manuel açma gerekiyorsa\n",
        "                print(f\"   -> '{actual_zip_file_path}' ZIP dosyası bulundu.\")\n",
        "                # Hedef dizin zaten varsa ve üzerine yazılacaksa, önce silmek daha güvenli olabilir (kodunuzda vardı).\n",
        "                if os.path.exists(unzipped_target_dir_absolute_path):\n",
        "                    print(f\"   -> '{unzipped_target_dir_absolute_path}' hedef dizini zaten mevcut. Manuel açma öncesi temizleniyor...\")\n",
        "                    # os.system(f\"rm -rf '{unzipped_target_dir_absolute_path}'\") # Bu riskli olabilir, shutil.rmtree daha iyi\n",
        "                    import shutil\n",
        "                    try:\n",
        "                        shutil.rmtree(unzipped_target_dir_absolute_path)\n",
        "                        print(f\"   -> '{unzipped_target_dir_absolute_path}' başarıyla silindi.\")\n",
        "                    except Exception as e_rm:\n",
        "                        if 'logger' in globals(): logger.error(f\"'{unzipped_target_dir_absolute_path}' silinirken hata: {e_rm}\")\n",
        "                        print(f\"   -> ❌ HATA: '{unzipped_target_dir_absolute_path}' silinirken: {e_rm}\")\n",
        "\n",
        "                print(f\"   -> '{actual_zip_file_path}' dosyası manuel olarak '{type_dir_absolute_path}' dizinine açılıyor...\")\n",
        "                try:\n",
        "                    with zipfile.ZipFile(actual_zip_file_path, 'r') as zip_ref:\n",
        "                        zip_ref.extractall(type_dir_absolute_path)\n",
        "                    print(f\"   -> '{resource_id}' zipfile modülü ile '{type_dir_absolute_path}' içine başarıyla açıldı.\")\n",
        "                    if not os.path.exists(unzipped_target_dir_absolute_path):\n",
        "                         if 'logger' in globals(): logger.warning(f\"Manuel açma sonrası '{unzipped_target_dir_absolute_path}' hala bulunamadı. Zip içeriği veya hedef isim kontrol edilmeli.\")\n",
        "                         print(f\"   -> ⚠️ UYARI: Manuel açma sonrası '{unzipped_target_dir_absolute_path}' hala bulunamadı.\")\n",
        "                except Exception as e_unzip:\n",
        "                    if 'logger' in globals(): logger.error(f\"'{resource_id}' manuel olarak açılırken (zipfile) hata: {e_unzip}\")\n",
        "                    print(f\"   -> ❌ HATA: '{resource_id}' manuel olarak (zipfile) açılırken: {e_unzip}\")\n",
        "        else:\n",
        "            print(f\"   -> '{resource_id}' için NLTK'nın otomatik açma işlemine güveniliyor (manual_unzip_needed=False).\")\n",
        "\n",
        "        # 3. Adım: Kaynağın NLTK tarafından bulunabildiğini doğrula\n",
        "        print(f\"   -> '{resource_id}' kaynağının NLTK tarafından ('{config['verification_path']}') bulunabilirliği doğrulanıyor...\")\n",
        "        nltk.data.find(config['verification_path']) # Bu satır hata fırlatırsa, kaynak bulunamamıştır.\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"'{resource_id}' NLTK kaynağı başarıyla doğrulandı (yol: {config['verification_path']}).\")\n",
        "        print(f\"   -> ✅ '{resource_id}' NLTK tarafından başarıyla bulundu ve doğrulandı.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"'{resource_id}' NLTK kaynağı işlenirken genel bir HATA oluştu: {e}\", exc_info=True) # exc_info=True ile traceback loglanır\n",
        "        print(f\"   -> ❌ '{resource_id}' NLTK kaynağı işlenirken genel bir HATA oluştu: {e}\")\n",
        "        print(f\"         Lütfen '{resource_id}' kaynağının durumunu ve NLTK veri dizinini ('{nltk_data_dir}') manuel olarak kontrol edin.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- NLTK Kaynak Kurulum İşlemleri Tamamlandı ---\")\n",
        "    logger.info(\"Bölüm B (NLTK Kaynak Yönetimi) tamamlandı.\")\n",
        "else:\n",
        "    print(\"--- NLTK Kaynak Kurulum İşlemleri Tamamlandı ---\")\n",
        "    print(\"Bölüm B (NLTK Kaynak Yönetimi) tamamlandı.\")\n",
        "\n",
        "\n",
        "# --- BÖLÜM C: Kurulum Sonrası NLTK Fonksiyonellik Testi ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"--- Bölüm C: Kurulum Sonrası NLTK Fonksiyonellik Testi Başlatılıyor ---\")\n",
        "else:\n",
        "    print(\"--- Bölüm C: Kurulum Sonrası NLTK Fonksiyonellik Testi Başlatılıyor ---\")\n",
        "\n",
        "try:\n",
        "    # `nltk.translate` importları, bu metrikler burada veya skorlama hücresinde kullanılacaksa gerekli.\n",
        "    # Şimdilik test amaçlı burada bırakalım. Asıl kullanım Hücre 9'da.\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "    from nltk.translate.meteor_score import meteor_score # meteor_score için wordnet gerekli olabilir.\n",
        "\n",
        "    # Test 1: Stopwords\n",
        "    test_en_stopwords = stopwords.words('english')\n",
        "    test_tr_stopwords = stopwords.words('turkish')\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Örnek İngilizce stopword: '{test_en_stopwords[0]}' (Toplam: {len(test_en_stopwords)})\")\n",
        "        logger.info(f\"Örnek Türkçe stopword: '{test_tr_stopwords[0]}' (Toplam: {len(test_tr_stopwords)})\")\n",
        "    else:\n",
        "        print(f\"INFO: Örnek İngilizce stopword: '{test_en_stopwords[0]}' (Toplam: {len(test_en_stopwords)})\")\n",
        "        print(f\"INFO: Örnek Türkçe stopword: '{test_tr_stopwords[0]}' (Toplam: {len(test_tr_stopwords)})\")\n",
        "\n",
        "    # Test 2: Tokenization (Punkt)\n",
        "    test_en_sent = \"Hello world. NLTK is fun.\"\n",
        "    test_tr_sent = \"Merhaba dünya. NLTK eğlencelidir.\"\n",
        "    tokenized_en_sents = sent_tokenize(test_en_sent)\n",
        "    tokenized_tr_words = word_tokenize(test_tr_sent, language='turkish')\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Örnek İngilizce cümle tokenizasyonu ('{test_en_sent}'): {tokenized_en_sents}\")\n",
        "        logger.info(f\"Örnek Türkçe kelime tokenizasyonu ('{test_tr_sent}'): {tokenized_tr_words}\")\n",
        "    else:\n",
        "        print(f\"INFO: Örnek İngilizce cümle tokenizasyonu ('{test_en_sent}'): {tokenized_en_sents}\")\n",
        "        print(f\"INFO: Örnek Türkçe kelime tokenizasyonu ('{test_tr_sent}'): {tokenized_tr_words}\")\n",
        "\n",
        "    # Test 3: WordNet ve OMW (Open Multilingual WordNet)\n",
        "    # Bu testler 'wordnet' ve 'omw-1.4' kaynaklarının doğru yüklendiğini kontrol eder.\n",
        "    from nltk.corpus import wordnet as wn\n",
        "    if 'logger' in globals(): logger.info(\"--- WordNet ve OMW-1.4 Testi ---\")\n",
        "    else: print(\"--- WordNet ve OMW-1.4 Testi ---\")\n",
        "\n",
        "    dog_synsets_eng = wn.synsets('dog', lang='eng')\n",
        "    if dog_synsets_eng:\n",
        "        if 'logger' in globals(): logger.info(f\"WordNet'ten 'dog' (İngilizce) için ilk synset: {dog_synsets_eng[0].name()} - Tanım: {dog_synsets_eng[0].definition()}\")\n",
        "        else: print(f\"INFO: WordNet'ten 'dog' (İngilizce) için ilk synset: {dog_synsets_eng[0].name()} - Tanım: {dog_synsets_eng[0].definition()}\")\n",
        "    else:\n",
        "        if 'logger' in globals(): logger.warning(\"WordNet'ten 'dog' (İngilizce) için synset bulunamadı.\")\n",
        "        else: print(\"WARNING: WordNet'ten 'dog' (İngilizce) için synset bulunamadı.\")\n",
        "\n",
        "    # OMW-1.4 kullanarak başka bir dilde lemma bulma testi (örneğin İspanyolca)\n",
        "    # 'cat' kelimesinin İngilizce synset'ini bul, sonra İspanyolca karşılıklarını ara\n",
        "    cat_synsets_eng_noun = wn.synsets('cat', pos=wn.NOUN, lang='eng')\n",
        "    if cat_synsets_eng_noun:\n",
        "        first_cat_synset = cat_synsets_eng_noun[0]\n",
        "        lemmas_spanish = first_cat_synset.lemmas(lang='spa') # 'spa' OMW-1.4'ten gelmeli\n",
        "        if lemmas_spanish:\n",
        "            if 'logger' in globals(): logger.info(f\"WordNet/OMW-1.4: 'cat' (İngilizce synset: {first_cat_synset.name()}) için İspanyolca lemmalar: {[lemma.name() for lemma in lemmas_spanish]}\")\n",
        "            else: print(f\"INFO: WordNet/OMW-1.4: 'cat' (İngilizce synset: {first_cat_synset.name()}) için İspanyolca lemmalar: {[lemma.name() for lemma in lemmas_spanish]}\")\n",
        "        else:\n",
        "            if 'logger' in globals(): logger.warning(f\"'cat' için İspanyolca lemma bulunamadı (OMW-1.4 kontrol edilmeli). Synset: {first_cat_synset.name()}\")\n",
        "            else: print(f\"WARNING: 'cat' için İspanyolca lemma bulunamadı (OMW-1.4 kontrol edilmeli). Synset: {first_cat_synset.name()}\")\n",
        "    else:\n",
        "        if 'logger' in globals(): logger.warning(\"'cat' (isim, İngilizce) için WordNet'te synset bulunamadı.\")\n",
        "        else: print(\"WARNING: 'cat' (isim, İngilizce) için WordNet'te synset bulunamadı.\")\n",
        "\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"NLTK fonksiyonellik testleri başarıyla tamamlandı.\")\n",
        "    else:\n",
        "        print(\"INFO: NLTK fonksiyonellik testleri başarıyla tamamlandı.\")\n",
        "\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"NLTK kaynakları test edilirken bir hata oluştu: {e}\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: NLTK kaynakları test edilirken bir hata oluştu: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Bölüm C (NLTK Fonksiyonellik Testi) tamamlandı.\")\n",
        "else:\n",
        "    print(\"Bölüm C (NLTK Fonksiyonellik Testi) tamamlandı.\")\n",
        "\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H3} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H3)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H3} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H3} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "aFKZtxnW0-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 4: Temel Konfigürasyonlar ve Google Drive Bağlantısı (Embedding Modeli: paraphrase-multilingual-mpnet-base-v2)\n",
        "\n",
        "CELL_NAME_H4 = \"4: Temel Konfigürasyonlar ve Google Drive Bağlantısı\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H4)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H4} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H4} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H4} başlatıldı.\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch # torch import'u Hücre 3'te zaten var, burada tekrar edilmesinde sakınca yok\n",
        "# BitsAndBytesConfig ve stopwords Hücre 3'te import edildi, burada tekrar etmeye gerek yok.\n",
        "# from transformers import BitsAndBytesConfig\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# --- Google Drive Bağlantısı ve Temel Dizin Yapılandırması ---\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Google Drive bağlantısı deneniyor...\")\n",
        "else:\n",
        "    print(\"INFO: Google Drive bağlantısı deneniyor...\")\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount=True her zaman yeniden bağlanmayı zorlar\n",
        "    if 'logger' in globals():\n",
        "        logger.info(\"Google Drive başarıyla bağlandı: /content/drive\")\n",
        "    else:\n",
        "        print(\"INFO: Google Drive başarıyla bağlandı: /content/drive\")\n",
        "\n",
        "    # Ana çalışma dizinini tanımla (Google Drive üzerinde)\n",
        "    BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab_RAG_Projesi\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ana çalışma dizini (BASE_DRIVE_PATH) olarak '{BASE_DRIVE_PATH}' ayarlandı.\")\n",
        "    else:\n",
        "        print(f\"INFO: Ana çalışma dizini (BASE_DRIVE_PATH) olarak '{BASE_DRIVE_PATH}' ayarlandı.\")\n",
        "\n",
        "    if not os.path.exists(BASE_DRIVE_PATH):\n",
        "        os.makedirs(BASE_DRIVE_PATH)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana çalışma dizini oluşturuldu: {BASE_DRIVE_PATH}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana çalışma dizini oluşturuldu: {BASE_DRIVE_PATH}\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana çalışma dizini zaten mevcut: {BASE_DRIVE_PATH}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana çalışma dizini zaten mevcut: {BASE_DRIVE_PATH}\")\n",
        "\n",
        "    # PDF dosyaları için klasör\n",
        "    PDF_KLASORU_ADI = \"PDF_Dosyalari\"\n",
        "    PDF_KLASOR_YOLU = os.path.join(BASE_DRIVE_PATH, PDF_KLASORU_ADI)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyaları klasörü (PDF_KLASOR_YOLU) olarak '{PDF_KLASOR_YOLU}' ayarlandı.\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyaları klasörü (PDF_KLASOR_YOLU) olarak '{PDF_KLASOR_YOLU}' ayarlandı.\")\n",
        "\n",
        "    if not os.path.exists(PDF_KLASOR_YOLU):\n",
        "        os.makedirs(PDF_KLASOR_YOLU)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF dosyaları için klasör oluşturuldu: {PDF_KLASOR_YOLU}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF dosyaları için klasör oluşturuldu: {PDF_KLASOR_YOLU}\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF dosyaları klasörü zaten mevcut: {PDF_KLASOR_YOLU}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF dosyaları klasörü zaten mevcut: {PDF_KLASOR_YOLU}\")\n",
        "\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"Google Drive bağlanırken veya ana dizinler oluşturulurken HATA: {e}\", exc_info=True)\n",
        "        logger.warning(\"Google Drive bağlanamadığı için lokal dizinler kullanılacak. Kalıcı depolama olmayacaktır.\")\n",
        "    else:\n",
        "        print(f\"ERROR: Google Drive bağlanırken veya ana dizinler oluşturulurken HATA: {e}\")\n",
        "        print(\"WARNING: Google Drive bağlanamadığı için lokal dizinler kullanılacak. Kalıcı depolama olmayacaktır.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Google Drive bağlanamazsa fallback olarak lokal dizinler\n",
        "    BASE_DRIVE_PATH = \"./colab_rag_data_local\" # Lokalde çalışacaksa, bu dizin oturum sonunda silinir.\n",
        "    PDF_KLASOR_ADI = \"PDF_Dosyalari_local\"\n",
        "    PDF_KLASOR_YOLU = os.path.join(BASE_DRIVE_PATH, PDF_KLASOR_ADI)\n",
        "\n",
        "    if not os.path.exists(BASE_DRIVE_PATH): os.makedirs(BASE_DRIVE_PATH)\n",
        "    if not os.path.exists(PDF_KLASOR_YOLU): os.makedirs(PDF_KLASOR_YOLU)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Lokal ana çalışma dizini: {BASE_DRIVE_PATH}\")\n",
        "        logger.info(f\"Lokal PDF dosyaları klasörü: {PDF_KLASOR_YOLU}\")\n",
        "    else:\n",
        "        print(f\"INFO: Lokal ana çalışma dizini: {BASE_DRIVE_PATH}\")\n",
        "        print(f\"INFO: Lokal PDF dosyaları klasörü: {PDF_KLASOR_YOLU}\")\n",
        "\n",
        "\n",
        "# --- Model ve Diğer Konfigürasyonlar ---\n",
        "LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "EMBEDDING_MODEL_ID = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' # Bu model zaten çok dilli ve iyi bir başlangıç\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"Kullanılacak LLM Model ID (LLM_MODEL_ID): {LLM_MODEL_ID}\")\n",
        "    logger.info(f\"Kullanılacak Embedding Model ID (EMBEDDING_MODEL_ID): {EMBEDDING_MODEL_ID}\")\n",
        "else:\n",
        "    print(f\"INFO: Kullanılacak LLM Model ID (LLM_MODEL_ID): {LLM_MODEL_ID}\")\n",
        "    print(f\"INFO: Kullanılacak Embedding Model ID (EMBEDDING_MODEL_ID): {EMBEDDING_MODEL_ID}\")\n",
        "\n",
        "# Cache dizini (BASE_DRIVE_PATH altında)\n",
        "CACHE_DIR_NAME = \"cache_data\" # Önceki \"cache\" ile karışmaması için veya aynı kalabilir\n",
        "CACHE_DIR = os.path.join(BASE_DRIVE_PATH, CACHE_DIR_NAME)\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"Cache dizini (CACHE_DIR) olarak '{CACHE_DIR}' ayarlandı.\")\n",
        "else:\n",
        "    print(f\"INFO: Cache dizini (CACHE_DIR) olarak '{CACHE_DIR}' ayarlandı.\")\n",
        "\n",
        "if not os.path.exists(CACHE_DIR):\n",
        "    os.makedirs(CACHE_DIR)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Cache dizini oluşturuldu: {CACHE_DIR}\")\n",
        "    else:\n",
        "        print(f\"INFO: Cache dizini oluşturuldu: {CACHE_DIR}\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Cache dizini zaten mevcut: {CACHE_DIR}. ÖNEMLİ: Eğer embedding modeli, chunking stratejisi gibi temel ayarlar değiştiyse, bu cache klasörünün içeriğini manuel olarak temizlemeniz önerilir.\")\n",
        "    else:\n",
        "        print(f\"INFO: Cache dizini zaten mevcut: {CACHE_DIR}. ÖNEMLİ: Eğer embedding modeli, chunking stratejisi gibi temel ayarlar değiştiyse, bu cache klasörünün içeriğini manuel olarak temizlemeniz önerilir.\")\n",
        "\n",
        "\n",
        "# Cihaz Ayarı\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if 'logger' in globals():\n",
        "    logger.info(f\"Hesaplamalar için kullanılacak cihaz (DEVICE): {DEVICE}\")\n",
        "else:\n",
        "    print(f\"INFO: Hesaplamalar için kullanılacak cihaz (DEVICE): {DEVICE}\")\n",
        "\n",
        "# Türkçe Stopwords (Hücre 3'te nltk.corpus.stopwords import edildi)\n",
        "turkish_stopwords = []\n",
        "try:\n",
        "    # Hücre 3'te NLTK kaynakları indirildiği için burada doğrudan kullanılabilir olmalı.\n",
        "    from nltk.corpus import stopwords as nltk_stopwords_corpus\n",
        "    turkish_stopwords = list(set(nltk_stopwords_corpus.words('turkish')))\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Türkçe stopwords listesi başarıyla yüklendi ({len(turkish_stopwords)} adet). İlk 5: {turkish_stopwords[:5] if turkish_stopwords else 'Liste boş'}\")\n",
        "    else:\n",
        "        print(f\"INFO: Türkçe stopwords listesi başarıyla yüklendi ({len(turkish_stopwords)} adet). İlk 5: {turkish_stopwords[:5] if turkish_stopwords else 'Liste boş'}\")\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"Türkçe stopwords yüklenirken HATA: {e}. Stopword kaldırma işlemi etkilenemeyebilir veya hatalı olabilir.\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: Türkçe stopwords yüklenirken HATA: {e}. Stopword kaldırma işlemi etkilenemeyebilir veya hatalı olabilir.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    turkish_stopwords = [] # Hata durumunda boş liste\n",
        "\n",
        "# Quantization Configuration (Hücre 3'te BitsAndBytesConfig import edildi)\n",
        "quantization_config = None # Varsayılan olarak None\n",
        "try:\n",
        "    if DEVICE.type == 'cuda':\n",
        "        from transformers import BitsAndBytesConfig # Burada tekrar import etmek güvenli\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16, # veya torch.bfloat16 (GPU destekliyorsa)\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"CUDA cihazı için 4-bit quantization (BitsAndBytesConfig) ayarlandı.\")\n",
        "        else:\n",
        "            print(\"INFO: CUDA cihazı için 4-bit quantization (BitsAndBytesConfig) ayarlandı.\")\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(\"Cihaz CUDA değil, bu yüzden quantization config (BitsAndBytesConfig) ayarlanmadı (None olarak kaldı).\")\n",
        "        else:\n",
        "            print(\"INFO: Cihaz CUDA değil, bu yüzden quantization config (BitsAndBytesConfig) ayarlanmadı (None olarak kaldı).\")\n",
        "except ImportError:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(\"BitsAndBytesConfig import edilemedi. Quantization config ayarlanamadı.\")\n",
        "    else:\n",
        "        print(\"WARNING: BitsAndBytesConfig import edilemedi. Quantization config ayarlanamadı.\")\n",
        "    quantization_config = None\n",
        "except Exception as e:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"Quantization config (BitsAndBytesConfig) ayarlanırken bir HATA oluştu: {e}\", exc_info=True)\n",
        "    else:\n",
        "        print(f\"ERROR: Quantization config (BitsAndBytesConfig) ayarlanırken bir HATA oluştu: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    quantization_config = None\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Temel konfigürasyonlar ve Google Drive bağlantı/ayarları tamamlandı.\")\n",
        "else:\n",
        "    print(\"INFO: Temel konfigürasyonlar ve Google Drive bağlantı/ayarları tamamlandı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H4} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H4)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H4} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H4} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "K6w7h8Ct1UbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 5: Model Yükleme (LLM ve Güncel Embedding Modeli)\n",
        "\n",
        "CELL_NAME_H5 = \"5: Model Yükleme (LLM ve Embedding)\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H5)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H5} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H5} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H5} başlatıldı.\")\n",
        "\n",
        "# Gerekli importlar (Hücre 3'te zaten yapıldı, burada tekrar edilmeleri genellikle sorun olmaz ama gereksiz olabilir)\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "# torch ve BitsAndBytesConfig Hücre 3 ve 4'te zaten import edildi.\n",
        "\n",
        "# Global değişkenlerin (LLM_MODEL_ID, hf_token_to_use, quantization_config, DEVICE, EMBEDDING_MODEL_ID)\n",
        "# önceki hücrelerde tanımlandığını varsayıyoruz.\n",
        "\n",
        "# --- LLM Tokenizer Yükleme ---\n",
        "llm_tokenizer = None\n",
        "if 'LLM_MODEL_ID' in globals() and LLM_MODEL_ID:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"LLM Tokenizer yükleniyor: {LLM_MODEL_ID}\")\n",
        "    else:\n",
        "        print(f\"INFO: LLM Tokenizer yükleniyor: {LLM_MODEL_ID}\")\n",
        "    try:\n",
        "        # Hugging Face token'ının adını Hücre 2'deki ile tutarlı hale getiriyoruz (hf_token_to_use)\n",
        "        current_hf_token = globals().get('hf_token_to_use', None)\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            LLM_MODEL_ID,\n",
        "            token=current_hf_token\n",
        "        )\n",
        "        # Pad token'ı kontrolü ve ayarlanması\n",
        "        if llm_tokenizer.pad_token is None:\n",
        "            if 'logger' in globals():\n",
        "                logger.warning(f\"LLM Tokenizer ({LLM_MODEL_ID}) için 'pad_token' tanımsız. 'eos_token' ({llm_tokenizer.eos_token}) 'pad_token' olarak atanıyor.\")\n",
        "            else:\n",
        "                print(f\"WARNING: LLM Tokenizer ({LLM_MODEL_ID}) için 'pad_token' tanımsız. 'eos_token' ({llm_tokenizer.eos_token}) 'pad_token' olarak atanıyor.\")\n",
        "            llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"LLM Tokenizer ({LLM_MODEL_ID}) başarıyla yüklendi. Pad token: {llm_tokenizer.pad_token}\")\n",
        "        else:\n",
        "            print(f\"INFO: LLM Tokenizer ({LLM_MODEL_ID}) başarıyla yüklendi. Pad token: {llm_tokenizer.pad_token}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"LLM Tokenizer ({LLM_MODEL_ID}) yüklenirken HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: LLM Tokenizer ({LLM_MODEL_ID}) yüklenirken HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        llm_tokenizer = None # Hata durumunda None olarak kalsın\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"LLM_MODEL_ID tanımlanmamış. LLM Tokenizer yüklenemiyor.\")\n",
        "    else:\n",
        "        print(\"ERROR: LLM_MODEL_ID tanımlanmamış. LLM Tokenizer yüklenemiyor.\")\n",
        "\n",
        "# --- LLM Model Yükleme ---\n",
        "llm_model = None\n",
        "if llm_tokenizer and 'LLM_MODEL_ID' in globals() and LLM_MODEL_ID: # Tokenizer başarıyla yüklendiyse devam et\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"LLM ({LLM_MODEL_ID}) yükleniyor... Bu işlem zaman alabilir.\")\n",
        "    else:\n",
        "        print(f\"INFO: LLM ({LLM_MODEL_ID}) yükleniyor... Bu işlem zaman alabilir.\")\n",
        "    try:\n",
        "        current_hf_token = globals().get('hf_token_to_use', None)\n",
        "        # Quantization config ve DEVICE Hücre 4'ten gelmeli\n",
        "        current_quantization_config = globals().get('quantization_config', None)\n",
        "        current_device = globals().get('DEVICE', torch.device(\"cpu\")) # Fallback to CPU if not defined\n",
        "\n",
        "        effective_quant_config = None\n",
        "        if current_device.type == 'cuda' and current_quantization_config:\n",
        "            effective_quant_config = current_quantization_config\n",
        "            if 'logger' in globals(): logger.info(\"CUDA cihazı algılandı ve quantization_config mevcut, LLM için kullanılacak.\")\n",
        "            else: print(\"INFO: CUDA cihazı algılandı ve quantization_config mevcut, LLM için kullanılacak.\")\n",
        "        elif current_device.type == 'cuda' and not current_quantization_config:\n",
        "            if 'logger' in globals(): logger.warning(\"CUDA cihazı algılandı ancak quantization_config mevcut değil. Model quantize edilmeden yüklenecek.\")\n",
        "            else: print(\"WARNING: CUDA cihazı algılandı ancak quantization_config mevcut değil. Model quantize edilmeden yüklenecek.\")\n",
        "        else: # CPU\n",
        "             if 'logger' in globals(): logger.info(\"Cihaz CPU, quantization_config LLM yüklemesinde kullanılmayacak.\")\n",
        "             else: print(\"INFO: Cihaz CPU, quantization_config LLM yüklemesinde kullanılmayacak.\")\n",
        "\n",
        "\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_ID,\n",
        "            torch_dtype=torch.float16, # Genellikle iyi bir denge sağlar\n",
        "            device_map=\"auto\", # Modeli uygun cihazlara (GPU/CPU) otomatik dağıtır\n",
        "            quantization_config=effective_quant_config, # Sadece CUDA'da ve tanımlıysa kullanılır\n",
        "            token=current_hf_token\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"LLM ({LLM_MODEL_ID}) başarıyla yüklendi.\")\n",
        "            if hasattr(llm_model, 'device') and next(llm_model.parameters(), None) is not None:\n",
        "                 logger.info(f\"LLM'in yüklendiği ana cihaz: {next(llm_model.parameters()).device}\")\n",
        "            elif hasattr(llm_model, 'hf_device_map'):\n",
        "                 logger.info(f\"LLM cihaz haritası (device_map): {llm_model.hf_device_map}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"INFO: LLM ({LLM_MODEL_ID}) başarıyla yüklendi.\")\n",
        "            if hasattr(llm_model, 'device') and next(llm_model.parameters(), None) is not None:\n",
        "                 print(f\"INFO: LLM'in yüklendiği ana cihaz: {next(llm_model.parameters()).device}\")\n",
        "            elif hasattr(llm_model, 'hf_device_map'):\n",
        "                 print(f\"INFO: LLM cihaz haritası (device_map): {llm_model.hf_device_map}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"LLM ({LLM_MODEL_ID}) yüklenirken HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: LLM ({LLM_MODEL_ID}) yüklenirken HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        llm_model = None # Hata durumunda None olarak kalsın\n",
        "elif not llm_tokenizer :\n",
        "    if 'logger' in globals():\n",
        "        logger.error(f\"LLM Tokenizer yüklenemediği için LLM ({LLM_MODEL_ID if 'LLM_MODEL_ID' in globals() else 'Bilinmeyen'}) yükleme işlemi atlandı.\")\n",
        "    else:\n",
        "        print(f\"ERROR: LLM Tokenizer yüklenemediği için LLM ({LLM_MODEL_ID if 'LLM_MODEL_ID' in globals() else 'Bilinmeyen'}) yükleme işlemi atlandı.\")\n",
        "\n",
        "# --- Ana Embedding Modeli (SentenceTransformer) Yükleme ---\n",
        "embedding_model_st = None # SentenceTransformer instance\n",
        "embedding_dimension = None\n",
        "if 'EMBEDDING_MODEL_ID' in globals() and EMBEDDING_MODEL_ID:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Ana Embedding Modeli (SentenceTransformer) yükleniyor: {EMBEDDING_MODEL_ID}\")\n",
        "    else:\n",
        "        print(f\"INFO: Ana Embedding Modeli (SentenceTransformer) yükleniyor: {EMBEDDING_MODEL_ID}\")\n",
        "    try:\n",
        "        current_device_for_emb = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "        embedding_model_st = SentenceTransformer(EMBEDDING_MODEL_ID, device=current_device_for_emb)\n",
        "        # Embedding boyutunu al\n",
        "        if hasattr(embedding_model_st, 'get_sentence_embedding_dimension'):\n",
        "            embedding_dimension = embedding_model_st.get_sentence_embedding_dimension()\n",
        "        else: # Eski versiyonlar veya farklı SentenceTransformer modelleri için fallback\n",
        "            try:\n",
        "                test_emb = embedding_model_st.encode(\"test\")\n",
        "                embedding_dimension = test_emb.shape[-1]\n",
        "            except Exception as e_dim:\n",
        "                 if 'logger' in globals(): logger.warning(f\"Embedding boyutu otomatik algılanamadı: {e_dim}\")\n",
        "                 else: print(f\"WARNING: Embedding boyutu otomatik algılanamadı: {e_dim}\")\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) başarıyla yüklendi. Cihaz: {embedding_model_st.device}, Embedding Boyutu: {embedding_dimension if embedding_dimension else 'Bilinmiyor'}\")\n",
        "        else:\n",
        "            print(f\"INFO: Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) başarıyla yüklendi. Cihaz: {embedding_model_st.device}, Embedding Boyutu: {embedding_dimension if embedding_dimension else 'Bilinmiyor'}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) yüklenirken HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Ana Embedding Modeli ({EMBEDDING_MODEL_ID}) yüklenirken HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        embedding_model_st = None\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"EMBEDDING_MODEL_ID tanımlanmamış. Ana Embedding Modeli yüklenemiyor.\")\n",
        "    else:\n",
        "        print(\"ERROR: EMBEDDING_MODEL_ID tanımlanmamış. Ana Embedding Modeli yüklenemiyor.\")\n",
        "\n",
        "\n",
        "# --- Langchain uyumlu Embedding Modeli Oluşturma (SemanticChunker için) ---\n",
        "langchain_embeddings_for_semantic_chunker = None\n",
        "if embedding_model_st and 'EMBEDDING_MODEL_ID' in globals() and EMBEDDING_MODEL_ID: # Ana model başarıyla yüklendiyse\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Langchain uyumlu Embedding Modeli '{EMBEDDING_MODEL_ID}' kullanılarak SentenceTransformerEmbeddings ile oluşturuluyor.\")\n",
        "    else:\n",
        "        print(f\"INFO: Langchain uyumlu Embedding Modeli '{EMBEDDING_MODEL_ID}' kullanılarak SentenceTransformerEmbeddings ile oluşturuluyor.\")\n",
        "    try:\n",
        "        from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings # Burada tekrar import güvenli\n",
        "        current_device_for_lc_emb = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "        langchain_embeddings_for_semantic_chunker = SentenceTransformerEmbeddings(\n",
        "            model_name=EMBEDDING_MODEL_ID,\n",
        "            model_kwargs={'device': current_device_for_lc_emb}\n",
        "            # encode_kwargs={'normalize_embeddings': True} # Bazı durumlarda faydalı olabilir, cosine similarity için normalizasyon önerilir.\n",
        "        )\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Langchain uyumlu Embedding Modeli (SentenceTransformerEmbeddings) başarıyla oluşturuldu. Cihaz: {current_device_for_lc_emb}\")\n",
        "        else:\n",
        "            print(f\"INFO: Langchain uyumlu Embedding Modeli (SentenceTransformerEmbeddings) başarıyla oluşturuldu. Cihaz: {current_device_for_lc_emb}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Langchain uyumlu Embedding Modeli oluşturulurken HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Langchain uyumlu Embedding Modeli oluşturulurken HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        langchain_embeddings_for_semantic_chunker = None\n",
        "elif not embedding_model_st:\n",
        "    if 'logger' in globals():\n",
        "        logger.warning(f\"Ana embedding modeli (embedding_model_st) yüklenemediği için Langchain uyumlu embedding modeli oluşturulamadı.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Ana embedding modeli (embedding_model_st) yüklenemediği için Langchain uyumlu embedding modeli oluşturulamadı.\")\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Tüm model yükleme/oluşturma denemeleri tamamlandı.\")\n",
        "else:\n",
        "    print(\"INFO: Tüm model yükleme/oluşturma denemeleri tamamlandı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H5} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H5)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H5} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H5} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "Hj9BcCuv1sfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 6: Yardımcı Fonksiyonlar (GÜNCELLENDİ - Loglar Düzenlendi)\n",
        "\n",
        "CELL_NAME_H6 = \"6: Yardımcı Fonksiyonlar\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H6)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H6} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H6} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H6} başlatıldı.\")\n",
        "\n",
        "import hashlib\n",
        "import pickle\n",
        "import os\n",
        "import pdfplumber\n",
        "import re\n",
        "# nltk.tokenize.word_tokenize, RecursiveCharacterTextSplitter, SemanticChunker Hücre 3'te import edildi.\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# --- Dosya Hashleme Fonksiyonu ---\n",
        "def generate_file_hash(file_path):\n",
        "    \"\"\"Verilen dosyanın MD5 hash'ini oluşturur.\"\"\"\n",
        "    hasher = hashlib.md5()\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            buf = f.read()\n",
        "            hasher.update(buf)\n",
        "        file_hash = hasher.hexdigest()\n",
        "        if 'logger' in globals():\n",
        "            logger.debug(f\"'{os.path.basename(file_path)}' için MD5 hash oluşturuldu: {file_hash}\")\n",
        "        return file_hash\n",
        "    except FileNotFoundError:\n",
        "        if 'logger' in globals():\n",
        "            logger.warning(f\"Hash oluşturmak için dosya bulunamadı: {file_path}\")\n",
        "        else:\n",
        "            print(f\"WARNING: Hash oluşturmak için dosya bulunamadı: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"'{file_path}' için dosya hash'i oluşturulurken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: '{file_path}' için dosya hash'i oluşturulurken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- Cache Kaydetme ve Yükleme Fonksiyonları ---\n",
        "def save_chunks_and_index(chunks_to_save, faiss_index_to_save, file_identifier_prefix, cache_directory=None):\n",
        "    \"\"\"Parçaları (chunks) ve FAISS indeksini önbelleğe kaydeder.\"\"\"\n",
        "    final_cache_dir = cache_directory if cache_directory else globals().get('CACHE_DIR', './cache_default')\n",
        "    if not os.path.exists(final_cache_dir):\n",
        "        try:\n",
        "            os.makedirs(final_cache_dir)\n",
        "            if 'logger' in globals(): logger.info(f\"Önbellek dizini oluşturuldu: {final_cache_dir}\")\n",
        "            else: print(f\"INFO: Önbellek dizini oluşturuldu: {final_cache_dir}\")\n",
        "        except Exception as e_mkdir:\n",
        "            if 'logger' in globals(): logger.error(f\"Önbellek dizini ({final_cache_dir}) oluşturulamadı: {e_mkdir}. Kayıt yapılamayacak.\", exc_info=True)\n",
        "            else: print(f\"ERROR: Önbellek dizini ({final_cache_dir}) oluşturulamadı: {e_mkdir}. Kayıt yapılamayacak.\")\n",
        "            return\n",
        "\n",
        "    chunk_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_chunks.pkl\")\n",
        "    index_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_index.faiss\")\n",
        "\n",
        "    try:\n",
        "        with open(chunk_file_path, 'wb') as f:\n",
        "            pickle.dump(chunks_to_save, f)\n",
        "        faiss.write_index(faiss_index_to_save, index_file_path)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Parçalar ve FAISS indeksi başarıyla önbelleğe kaydedildi. Prefix: '{file_identifier_prefix}', Dizine: {final_cache_dir}\")\n",
        "        else:\n",
        "            print(f\"INFO: Parçalar ve FAISS indeksi başarıyla önbelleğe kaydedildi. Prefix: '{file_identifier_prefix}', Dizine: {final_cache_dir}\")\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Önbellek (prefix: {file_identifier_prefix}) kaydedilirken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Önbellek (prefix: {file_identifier_prefix}) kaydedilirken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "def load_chunks_and_index(file_identifier_prefix, cache_directory=None):\n",
        "    \"\"\"Parçaları (chunks) ve FAISS indeksini önbellekten yükler.\"\"\"\n",
        "    final_cache_dir = cache_directory if cache_directory else globals().get('CACHE_DIR', './cache_default')\n",
        "    chunk_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_chunks.pkl\")\n",
        "    index_file_path = os.path.join(final_cache_dir, f\"{file_identifier_prefix}_index.faiss\")\n",
        "\n",
        "    if os.path.exists(chunk_file_path) and os.path.exists(index_file_path):\n",
        "        try:\n",
        "            with open(chunk_file_path, 'rb') as f:\n",
        "                loaded_chunks = pickle.load(f)\n",
        "            loaded_index = faiss.read_index(index_file_path)\n",
        "            if 'logger' in globals():\n",
        "                logger.info(f\"Önbellek (prefix: {file_identifier_prefix}) başarıyla yüklendi. {len(loaded_chunks) if loaded_chunks else 0} chunk, İndeks yüklendi: {'Evet' if loaded_index else 'Hayır'}.\")\n",
        "            else:\n",
        "                print(f\"INFO: Önbellek (prefix: {file_identifier_prefix}) başarıyla yüklendi. {len(loaded_chunks) if loaded_chunks else 0} chunk, İndeks yüklendi: {'Evet' if loaded_index else 'Hayır'}.\")\n",
        "            return loaded_chunks, loaded_index\n",
        "        except Exception as e:\n",
        "            if 'logger' in globals():\n",
        "                logger.error(f\"Önbellek (prefix: {file_identifier_prefix}) yüklenirken HATA: {e}. Bozuk önbellek dosyaları silinip yeniden oluşturulacak.\", exc_info=True)\n",
        "            else:\n",
        "                print(f\"ERROR: Önbellek (prefix: {file_identifier_prefix}) yüklenirken HATA: {e}. Bozuk önbellek dosyaları silinip yeniden oluşturulacak.\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            # Hata durumunda bozuk olabilecek cache dosyalarını silmeyi dene\n",
        "            if os.path.exists(chunk_file_path):\n",
        "                try: os.remove(chunk_file_path)\n",
        "                except Exception as e_rm_c:\n",
        "                    if 'logger' in globals(): logger.error(f\"Bozuk cache chunk dosyası ({chunk_file_path}) silinirken HATA: {e_rm_c}\")\n",
        "                    else: print(f\"ERROR: Bozuk cache chunk dosyası ({chunk_file_path}) silinirken HATA: {e_rm_c}\")\n",
        "            if os.path.exists(index_file_path):\n",
        "                try: os.remove(index_file_path)\n",
        "                except Exception as e_rm_i:\n",
        "                    if 'logger' in globals(): logger.error(f\"Bozuk cache index dosyası ({index_file_path}) silinirken HATA: {e_rm_i}\")\n",
        "                    else: print(f\"ERROR: Bozuk cache index dosyası ({index_file_path}) silinirken HATA: {e_rm_i}\")\n",
        "            return None, None\n",
        "    else:\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Önbellek (prefix: {file_identifier_prefix}) bulunamadı. Veri yeniden işlenecek.\")\n",
        "        else:\n",
        "            print(f\"INFO: Önbellek (prefix: {file_identifier_prefix}) bulunamadı. Veri yeniden işlenecek.\")\n",
        "        return None, None\n",
        "\n",
        "# --- PDF Metin Çıkarma Fonksiyonu ---\n",
        "def pdf_to_text(pdf_path):\n",
        "    \"\"\"Verilen PDF dosyasından metin içeriğini çıkarır.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyasından metin çıkarılıyor: {os.path.basename(pdf_path)}\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyasından metin çıkarılıyor: {os.path.basename(pdf_path)}\")\n",
        "    extracted_text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    extracted_text += page_text + \"\\n\" # Sayfalar arasına yeni satır ekle\n",
        "                else:\n",
        "                    if 'logger' in globals(): logger.debug(f\"PDF: {os.path.basename(pdf_path)}, Sayfa {i+1} metin içermiyor veya çıkarılamadı.\")\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"PDF metin çıkarma tamamlandı. '{os.path.basename(pdf_path)}' için toplam karakter sayısı: {len(extracted_text)}\")\n",
        "        else:\n",
        "            print(f\"INFO: PDF metin çıkarma tamamlandı. '{os.path.basename(pdf_path)}' için toplam karakter sayısı: {len(extracted_text)}\")\n",
        "        return extracted_text\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"PDF ({os.path.basename(pdf_path)}) okunurken/işlenirken HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: PDF ({os.path.basename(pdf_path)}) okunurken/işlenirken HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return \"\" # Hata durumunda boş string dön\n",
        "\n",
        "# --- Metin Ön İşleme Fonksiyonu ---\n",
        "def preprocess_text(text_input):\n",
        "    \"\"\"Giriş metnini ön işler: küçük harf, boşluk normalizasyonu, URL/hashtag temizliği, tokenizasyon ve stopword kaldırma.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Ön işleme için geçersiz metin girişi (boş veya string değil). Boş string döndürülüyor.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Küçük harfe çevirme\n",
        "    processed_text = text_input.lower()\n",
        "    # Çoklu boşlukları tek boşluğa indirgeme ve baştaki/sondaki boşlukları temizleme\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "    # URL, e-posta ve hashtag'leri kaldırma (daha kapsamlı regex'ler eklenebilir)\n",
        "    processed_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', processed_text, flags=re.MULTILINE)\n",
        "    processed_text = re.sub(r'\\S*@\\S*\\s?', '', processed_text) # E-posta\n",
        "    processed_text = re.sub(r'#\\S+', '', processed_text) # Hashtag\n",
        "    # Sayısal olmayan karakterleri ve temel noktalama işaretlerini koru, diğerlerini temizle (isteğe bağlı)\n",
        "    # processed_text = re.sub(r'[^a-z0-9ğüşıöç\\s\\.,!?]', '', processed_text) # Bu çok agresif olabilir.\n",
        "\n",
        "    # Tokenize etme (Hücre 3'te nltk.tokenize.word_tokenize import edildi)\n",
        "    try:\n",
        "        from nltk.tokenize import word_tokenize # Güvenlik için tekrar import\n",
        "        tokens = word_tokenize(processed_text, language='turkish')\n",
        "    except Exception as e_tok:\n",
        "        if 'logger' in globals(): logger.error(f\"Metin tokenizasyonu sırasında hata: {e_tok}. Ham metin kullanılacak.\", exc_info=True)\n",
        "        else: print(f\"ERROR: Metin tokenizasyonu sırasında hata: {e_tok}. Ham metin kullanılacak.\")\n",
        "        tokens = processed_text.split() # Fallback olarak basit split\n",
        "\n",
        "    # Stopword'leri ve kısa/alfanümerik olmayan token'ları kaldır\n",
        "    # turkish_stopwords global değişkeni Hücre 4'te tanımlandı.\n",
        "    current_turkish_stopwords = globals().get('turkish_stopwords', [])\n",
        "    if not isinstance(current_turkish_stopwords, list): current_turkish_stopwords = []\n",
        "\n",
        "    filtered_tokens = []\n",
        "    if current_turkish_stopwords: # Eğer stopword listesi varsa\n",
        "        for word in tokens:\n",
        "            if word.isalpha() and len(word) > 1 and word not in current_turkish_stopwords:\n",
        "                filtered_tokens.append(word)\n",
        "    else: # Stopword listesi yoksa, sadece alfanümerik ve uzunluk kontrolü\n",
        "        for word in tokens:\n",
        "            if word.isalpha() and len(word) > 1:\n",
        "                filtered_tokens.append(word)\n",
        "\n",
        "    final_text = ' '.join(filtered_tokens)\n",
        "    if 'logger' in globals():\n",
        "        logger.debug(f\"Metin ön işleme tamamlandı. Orijinal (ilk 50): '{text_input[:50]}...' -> İşlenmiş (ilk 50): '{final_text[:50]}...'\")\n",
        "    return final_text\n",
        "\n",
        "# --- Metin Parçalama Fonksiyonları ---\n",
        "def chunk_text_recursive(text_to_chunk, chunk_size=1000, chunk_overlap=150):\n",
        "    \"\"\"Metni RecursiveCharacterTextSplitter kullanarak parçalar.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Recursive metin parçalama başlatılıyor. Ayarlar: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
        "    else:\n",
        "        print(f\"INFO: Recursive metin parçalama başlatılıyor. Ayarlar: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
        "\n",
        "    if not text_to_chunk or not isinstance(text_to_chunk, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Recursive parçalama için geçersiz metin (boş veya string değil). Boş liste döndürülüyor.\")\n",
        "        return []\n",
        "    try:\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter # Güvenlik için tekrar import\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"] # Daha fazla ayırıcı eklenebilir\n",
        "        )\n",
        "        chunks = text_splitter.split_text(text_to_chunk)\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Metin (Recursive) {len(chunks)} parçaya bölündü.\")\n",
        "        else:\n",
        "            print(f\"INFO: Metin (Recursive) {len(chunks)} parçaya bölündü.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Recursive metin parçalama sırasında HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Recursive metin parçalama sırasında HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "def chunk_text_semantic(text_to_chunk, langchain_embedding_model_instance,\n",
        "                        breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=95):\n",
        "    \"\"\"Metni SemanticChunker kullanarak anlamsal olarak parçalar.\"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"Anlamsal metin parçalama başlatılıyor. Ayarlar: threshold_type='{breakpoint_threshold_type}', threshold_amount='{breakpoint_threshold_amount}'\")\n",
        "    else:\n",
        "        print(f\"INFO: Anlamsal metin parçalama başlatılıyor. Ayarlar: threshold_type='{breakpoint_threshold_type}', threshold_amount='{breakpoint_threshold_amount}'\")\n",
        "\n",
        "    if not text_to_chunk or not isinstance(text_to_chunk, str):\n",
        "        if 'logger' in globals(): logger.warning(\"Anlamsal parçalama için geçersiz metin (boş veya string değil). Boş liste döndürülüyor.\")\n",
        "        return []\n",
        "    if langchain_embedding_model_instance is None:\n",
        "        if 'logger' in globals(): logger.error(\"Anlamsal parçalama için Langchain embedding modeli (langchain_embedding_model_instance) sağlanmadı. Boş liste döndürülüyor.\")\n",
        "        else: print(\"ERROR: Anlamsal parçalama için Langchain embedding modeli (langchain_embedding_model_instance) sağlanmadı. Boş liste döndürülüyor.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        from langchain_experimental.text_splitter import SemanticChunker # Güvenlik için tekrar import\n",
        "        # breakpoint_threshold_type için \"standard_deviation\", \"interquartile\" gibi seçenekler de mevcut.\n",
        "        # \"percentile\" genellikle iyi bir başlangıçtır.\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=langchain_embedding_model_instance,\n",
        "            breakpoint_threshold_type=breakpoint_threshold_type\n",
        "            # Eğer percentile ise, SemanticChunker bunu __init__'te doğrudan almıyor olabilir,\n",
        "            # doğrudan _breakpoint_threshold_amount olarak ayarlanabilir veya instance oluşturulduktan sonra.\n",
        "            # Dökümantasyona göre, breakpoint_threshold_type=\"percentile\" ise,\n",
        "            # percentile_threshold parametresi __init__'e verilebilir veya sonradan ayarlanabilir.\n",
        "            # Şimdiki kodunuzda (Hücre 10'da) create_documents sonrası set ediliyordu, bu da bir yöntem.\n",
        "            # Ya da doğrudan __init__ içinde deneyelim, eğer varsa.\n",
        "            # **{'percentile_threshold': breakpoint_threshold_amount} if breakpoint_threshold_type == \"percentile\" else {} # Bu şekilde denenebilir\n",
        "        )\n",
        "        # Eğer __init__ percentile_threshold almıyorsa, sonradan ayarlama (önceki kodunuzdaki gibi)\n",
        "        if breakpoint_threshold_type == \"percentile\" and hasattr(semantic_splitter, 'percentile_threshold'):\n",
        "           semantic_splitter.percentile_threshold = breakpoint_threshold_amount\n",
        "        elif breakpoint_threshold_type != \"percentile\" and hasattr(semantic_splitter, breakpoint_threshold_type + '_threshold'):\n",
        "            # Diğer threshold tipleri için (örn: standard_deviation_threshold)\n",
        "            # setattr(semantic_splitter, breakpoint_threshold_type + '_threshold', breakpoint_threshold_amount)\n",
        "            # Ancak bu SemanticChunker'ın iç yapısına bağlı, dökümantasyon kontrol edilmeli.\n",
        "            # Şimdilik sadece percentile için açık ayar bırakalım.\n",
        "            pass\n",
        "\n",
        "\n",
        "        # SemanticChunker Langchain Document objeleri listesi bekler ve Document listesi döndürür.\n",
        "        # Biz tek bir metinle çalışıyorsak, onu bir Document içine almalıyız.\n",
        "        from langchain_core.documents import Document\n",
        "        documents_to_split = [Document(page_content=text_to_chunk)]\n",
        "\n",
        "        # split_documents metodu kullanılmalı\n",
        "        semantic_docs = semantic_splitter.split_documents(documents_to_split)\n",
        "        chunks = [doc.page_content for doc in semantic_docs]\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Metin (Anlamsal) {len(chunks)} parçaya bölündü.\")\n",
        "        else:\n",
        "            print(f\"INFO: Metin (Anlamsal) {len(chunks)} parçaya bölündü.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"Anlamsal metin parçalama sırasında HATA: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: Anlamsal metin parçalama sırasında HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"Yardımcı fonksiyonlar tanımlandı.\")\n",
        "else:\n",
        "    print(\"INFO: Yardımcı fonksiyonlar tanımlandı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H6} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H6)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H6} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H6} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "KgYshIl02cLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 7: FAISS İndeksi Oluşturma ve Arama Fonksiyonları (GÜNCELLENDİ - Loglar Düzenlendi)\n",
        "\n",
        "CELL_NAME_H7 = \"7: FAISS İndeksi Oluşturma ve Arama Fonksiyonları\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H7)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H7} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H7} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H7} başlatıldı.\")\n",
        "\n",
        "import faiss\n",
        "import numpy as np # Hücre 3'te import edildi, burada tekrar edilmesinde sakınca yok\n",
        "\n",
        "# --- FAISS İndeksi Oluşturma Fonksiyonu ---\n",
        "def build_faiss_index(chunks_to_index, sentence_transformer_embedding_model):\n",
        "    \"\"\"\n",
        "    Verilen metin parçalarından (chunks) bir FAISS indeksi oluşturur.\n",
        "    Kullanılan embedding modeli bir SentenceTransformer instance olmalıdır.\n",
        "    \"\"\"\n",
        "    num_chunks = len(chunks_to_index) if chunks_to_index else 0\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"FAISS indeksi oluşturma işlemi başlatılıyor ({num_chunks} adet chunk için)...\")\n",
        "    else:\n",
        "        print(f\"INFO: FAISS indeksi oluşturma işlemi başlatılıyor ({num_chunks} adet chunk için)...\")\n",
        "\n",
        "    if not chunks_to_index:\n",
        "        if 'logger' in globals():\n",
        "            logger.warning(\"FAISS indeksi oluşturmak için chunk listesi boş. İşlem durduruldu.\")\n",
        "        else:\n",
        "            print(\"WARNING: FAISS indeksi oluşturmak için chunk listesi boş. İşlem durduruldu.\")\n",
        "        return None, None # index, embeddings_np\n",
        "\n",
        "    if sentence_transformer_embedding_model is None:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(\"FAISS indeksi oluşturmak için embedding modeli (sentence_transformer_embedding_model) sağlanmadı. İşlem durduruldu.\")\n",
        "        else:\n",
        "            print(\"ERROR: FAISS indeksi oluşturmak için embedding modeli (sentence_transformer_embedding_model) sağlanmadı. İşlem durduruldu.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # 1. Adım: Chunk'ların embedding'lerini al\n",
        "        # SentenceTransformer.encode() metodu numpy array veya tensor döndürebilir.\n",
        "        # convert_to_tensor=True ile tensor alıp sonra numpy'a çevirmek tutarlı bir yol.\n",
        "        if 'logger' in globals(): logger.debug(\"Chunk embedding'leri hesaplanıyor...\")\n",
        "        chunk_embeddings_tensor = sentence_transformer_embedding_model.encode(\n",
        "            chunks_to_index,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True # Uzun listeler için ilerleme çubuğu faydalı olabilir\n",
        "        )\n",
        "        chunk_embeddings_np = chunk_embeddings_tensor.cpu().numpy().astype('float32') # FAISS float32 bekler\n",
        "        if 'logger' in globals(): logger.debug(f\"Chunk embedding'leri hesaplandı. Şekil: {chunk_embeddings_np.shape}\")\n",
        "\n",
        "\n",
        "        # Embedding'lerin geçerliliğini kontrol et\n",
        "        if chunk_embeddings_np.shape[0] == 0 or \\\n",
        "           len(chunk_embeddings_np.shape) < 2 or \\\n",
        "           chunk_embeddings_np.shape[1] == 0:\n",
        "            if 'logger' in globals():\n",
        "                logger.error(f\"Hesaplanan chunk embedding'lerinin boyutu geçersiz: {chunk_embeddings_np.shape}. FAISS indeksi kurulamıyor.\")\n",
        "            else:\n",
        "                print(f\"ERROR: Hesaplanan chunk embedding'lerinin boyutu geçersiz: {chunk_embeddings_np.shape}. FAISS indeksi kurulamıyor.\")\n",
        "            return None, None\n",
        "\n",
        "        embedding_dimension = chunk_embeddings_np.shape[1]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Chunk embedding'leri başarıyla oluşturuldu. Boyut (dimension): {embedding_dimension}, Vektör sayısı: {chunk_embeddings_np.shape[0]}\")\n",
        "        else:\n",
        "            print(f\"INFO: Chunk embedding'leri başarıyla oluşturuldu. Boyut (dimension): {embedding_dimension}, Vektör sayısı: {chunk_embeddings_np.shape[0]}\")\n",
        "\n",
        "        # 2. Adım: FAISS İndeksi Oluştur\n",
        "        # IndexFlatL2, L2 mesafesini (Öklid) kullanır. Normalize edilmiş embedding'ler için kosinüs benzerliğine denktir.\n",
        "        # SentenceTransformer modelleri genellikle normalize edilmiş embedding'ler döndürür.\n",
        "        faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
        "        faiss_index.add(chunk_embeddings_np) # Embedding'leri indekse ekle\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"FAISS indeksi (IndexFlatL2) başarıyla oluşturuldu. Toplam {faiss_index.ntotal} vektör eklendi.\")\n",
        "        else:\n",
        "            print(f\"INFO: FAISS indeksi (IndexFlatL2) başarıyla oluşturuldu. Toplam {faiss_index.ntotal} vektör eklendi.\")\n",
        "\n",
        "        return faiss_index, chunk_embeddings_np # İndeksi ve ayrıca embedding'leri döndür (bazı senaryolarda gerekebilir)\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"FAISS indeksi oluşturulurken HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: FAISS indeksi oluşturulurken HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# --- FAISS İndeksinde Arama Yapma Fonksiyonu ---\n",
        "def retrieve_relevant_chunks(query_text, faiss_index_instance, original_indexed_chunks,\n",
        "                             sentence_transformer_embedding_model, top_k=5):\n",
        "    \"\"\"\n",
        "    Verilen sorgu metni için FAISS indeksinde arama yaparak en ilgili chunk'ları getirir.\n",
        "    \"\"\"\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"FAISS indeksinde ilgili chunk'lar aranıyor. Top K: {top_k}, Sorgu (ilk 50 karakter): '{query_text[:50]}...'\")\n",
        "    else:\n",
        "        print(f\"INFO: FAISS indeksinde ilgili chunk'lar aranıyor. Top K: {top_k}, Sorgu (ilk 50 karakter): '{query_text[:50]}...'\")\n",
        "\n",
        "    if not faiss_index_instance:\n",
        "        if 'logger' in globals(): logger.error(\"Arama için FAISS indeksi (faiss_index_instance) sağlanmadı.\")\n",
        "        else: print(\"ERROR: Arama için FAISS indeksi (faiss_index_instance) sağlanmadı.\")\n",
        "        return []\n",
        "    if not sentence_transformer_embedding_model:\n",
        "        if 'logger' in globals(): logger.error(\"Arama için embedding modeli (sentence_transformer_embedding_model) sağlanmadı.\")\n",
        "        else: print(\"ERROR: Arama için embedding modeli (sentence_transformer_embedding_model) sağlanmadı.\")\n",
        "        return []\n",
        "    if not original_indexed_chunks:\n",
        "        if 'logger' in globals(): logger.warning(\"Arama sonuçlarını eşleştirmek için orijinal chunk listesi (original_indexed_chunks) boş.\")\n",
        "        # Boş liste döndürmek yerine, bu durumda da arama yapılabilir, ancak sonuçlar sadece indis olurdu.\n",
        "        # Mevcut mantık, orijinal chunk'ları döndürmek üzerine kurulu olduğu için boş liste daha uygun.\n",
        "        else: print(\"WARNING: Arama sonuçlarını eşleştirmek için orijinal chunk listesi (original_indexed_chunks) boş.\")\n",
        "        return []\n",
        "    if not query_text or not query_text.strip():\n",
        "        if 'logger' in globals(): logger.warning(\"Arama sorgusu (query_text) boş veya sadece boşluk içeriyor.\")\n",
        "        else: print(\"WARNING: Arama sorgusu (query_text) boş veya sadece boşluk içeriyor.\")\n",
        "        return []\n",
        "    if top_k <= 0:\n",
        "        if 'logger' in globals(): logger.warning(f\"İstenen top_k ({top_k}) geçersiz. En az 1 olmalı. Boş liste döndürülüyor.\")\n",
        "        else: print(f\"WARNING: İstenen top_k ({top_k}) geçersiz. En az 1 olmalı. Boş liste döndürülüyor.\")\n",
        "        return[]\n",
        "\n",
        "    try:\n",
        "        # 1. Adım: Sorgu metninin embedding'ini al\n",
        "        if 'logger' in globals(): logger.debug(\"Sorgu embedding'i hesaplanıyor...\")\n",
        "        query_embedding_tensor = sentence_transformer_embedding_model.encode(\n",
        "            [query_text], # encode metodu liste bekler\n",
        "            convert_to_tensor=True\n",
        "        )\n",
        "        query_embedding_np = query_embedding_tensor.cpu().numpy().astype('float32')\n",
        "        if 'logger' in globals(): logger.debug(f\"Sorgu embedding'i hesaplandı. Şekil: {query_embedding_np.shape}\")\n",
        "\n",
        "\n",
        "        # 2. Adım: FAISS indeksinde arama yap\n",
        "        # .search metodu mesafeleri (distances) ve en yakın komşuların indislerini (indices) döndürür.\n",
        "        if 'logger' in globals(): logger.debug(f\"FAISS indeksinde {top_k} en yakın komşu için arama yapılıyor...\")\n",
        "        distances, indices = faiss_index_instance.search(query_embedding_np, k=top_k)\n",
        "        if 'logger' in globals(): logger.debug(f\"FAISS arama sonuçları alındı. İndisler: {indices}, Mesafeler: {distances}\")\n",
        "\n",
        "\n",
        "        # 3. Adım: Bulunan indislerle orijinal chunk'ları eşleştir\n",
        "        relevant_chunks_found = []\n",
        "        if indices.size > 0: # Eğer en az bir sonuç bulunduysa\n",
        "            for i, idx in enumerate(indices[0]): # indices[0] çünkü tek bir sorgu için arama yaptık\n",
        "                if idx == -1: # FAISS bazen -1 dönebilir (özellikle k > index.ntotal ise veya bazı özel index türlerinde)\n",
        "                    if 'logger' in globals(): logger.warning(f\"FAISS aramasında geçersiz indeks (-1) bulundu. Bu sonuç atlanıyor. Sıra: {i}\")\n",
        "                    continue\n",
        "                if 0 <= idx < len(original_indexed_chunks):\n",
        "                    relevant_chunks_found.append(original_indexed_chunks[idx])\n",
        "                else:\n",
        "                    if 'logger' in globals():\n",
        "                        logger.warning(f\"FAISS aramasından gelen indeks ({idx}) orijinal chunk listesi sınırları dışında. Toplam chunk: {len(original_indexed_chunks)}. Bu sonuç atlanıyor.\")\n",
        "                    else:\n",
        "                        print(f\"WARNING: FAISS aramasından gelen indeks ({idx}) orijinal chunk listesi sınırları dışında. Toplam chunk: {len(original_indexed_chunks)}. Bu sonuç atlanıyor.\")\n",
        "        else:\n",
        "             if 'logger' in globals(): logger.info(\"FAISS aramasından herhangi bir sonuç (indis) bulunamadı.\")\n",
        "             else: print(\"INFO: FAISS aramasından herhangi bir sonuç (indis) bulunamadı.\")\n",
        "\n",
        "\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"FAISS arama tamamlandı. '{query_text[:50]}...' sorgusu için {len(relevant_chunks_found)} adet ilgili chunk bulundu.\")\n",
        "        else:\n",
        "            print(f\"INFO: FAISS arama tamamlandı. '{query_text[:50]}...' sorgusu için {len(relevant_chunks_found)} adet ilgili chunk bulundu.\")\n",
        "\n",
        "        return relevant_chunks_found\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'logger' in globals():\n",
        "            logger.error(f\"FAISS arama sırasında HATA oluştu: {e}\", exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: FAISS arama sırasında HATA oluştu: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"FAISS indeksi oluşturma ve arama fonksiyonları tanımlandı.\")\n",
        "else:\n",
        "    print(\"INFO: FAISS indeksi oluşturma ve arama fonksiyonları tanımlandı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H7} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H7)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H7} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H7} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "KAkXa2s12s9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 8: LLM ile Cevap Üretme Fonksiyonu (GÜNCELLENDİ - Son Prompt Ayarı)\n",
        "\n",
        "CELL_NAME_H8 = \"8: LLM ile Cevap Üretme Fonksiyonu\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H8)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H8} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H8} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H8} başlatıldı.\")\n",
        "\n",
        "import torch\n",
        "import logging # Logger seviyesini kontrol etmek için\n",
        "\n",
        "def generate_answer_with_llm(\n",
        "    context_text,\n",
        "    question_text,\n",
        "    llm_model_instance,\n",
        "    tokenizer_instance,\n",
        "    max_new_tokens=512, # Bu değer Hücre 12'deki RAG_LLM_MAX_NEW_TOKENS ile override edilecek\n",
        "    temperature=0.3,    # Bu değer Hücre 12'deki RAG_LLM_TEMPERATURE ile override edilecek\n",
        "    do_sample=True      # Bu değer Hücre 12'deki RAG_LLM_DO_SAMPLE ile override edilecek\n",
        "):\n",
        "    current_device = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "    log_active = 'logger' in globals()\n",
        "\n",
        "    def _log_info(message):\n",
        "        if log_active: logger.info(message)\n",
        "        else: print(f\"INFO: {message}\")\n",
        "    def _log_warning(message):\n",
        "        if log_active: logger.warning(message)\n",
        "        else: print(f\"WARNING: {message}\")\n",
        "    def _log_error(message, exc_info=False):\n",
        "        if log_active: logger.error(message, exc_info=exc_info)\n",
        "        else:\n",
        "            print(f\"ERROR: {message}\")\n",
        "            if exc_info:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "    def _log_debug(message):\n",
        "        if log_active and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG):\n",
        "            logger.debug(message)\n",
        "    def _log_critical(message, exc_info=False):\n",
        "        if log_active: logger.critical(message, exc_info=exc_info)\n",
        "        else:\n",
        "            print(f\"CRITICAL: {message}\")\n",
        "            if exc_info:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    _log_info(f\"LLM ile cevap üretme işlemi başlatılıyor. Soru (ilk 50 krktr): '{str(question_text)[:50]}...'\")\n",
        "    _log_info(f\"LLM 'generate' ayarları (fonksiyona gelen): max_new_tokens={max_new_tokens}, temperature={temperature if do_sample else 'N/A (do_sample=False)'}, do_sample={do_sample}\")\n",
        "\n",
        "    if llm_model_instance is None or tokenizer_instance is None:\n",
        "        error_msg = \"LLM (llm_model_instance) veya Tokenizer (tokenizer_instance) sağlanmadı. Cevap üretilemiyor.\"\n",
        "        _log_error(error_msg)\n",
        "        return f\"HATA: {error_msg}\"\n",
        "\n",
        "   # Prompt'a ekleme: \"Cevabının tam, eksiksiz ve dilbilgisi açısından doğru bir cümle olmasına özen göster.\"\n",
        "    prompt = f\"\"\"<s>[INST] Senin görevin, yalnızca sana verilen DOKÜMAN BAĞLAMI'nı kullanarak SORU'yu yanıtlamaktır.\n",
        "Kesinlikle DOKÜMAN BAĞLAMI dışından bilgi kullanma.\n",
        "Eğer cevap DOKÜMAN BAĞLAMI'nda açıkça bulunmuyorsa, \"Sağlanan bilgilerde bu soruya net bir cevap bulunmamaktadır.\" şeklinde yanıt ver.\n",
        "Cevabın, sorulan sorunun en doğrudan ve en kısa yanıtı olmalıdır. Cevabının tam, eksiksiz ve dilbilgisi açısından doğru bir cümle olmasına özen göster. Yanıta herhangi bir ek açıklama, giriş cümlesi, madde numarası, referans veya yorum EKLEME. Sadece ve sadece saf bilgiyi, yani direkt cevabı ver. Örneğin, soru \"X ne zaman yapılır?\" ise ve bağlamda \"X, Y zamanında yapılır.\" bilgisi varsa, cevabın sadece \"X, Y zamanında yapılır.\" olmalıdır. Başka hiçbir ek ifade kullanma. Cevabını kısa ve öz tut. [/INST]\n",
        "DOKÜMAN BAĞLAMI:\n",
        "{context_text}\n",
        "\n",
        "SORU:\n",
        "{question_text}\n",
        "YANITIN:\"\"\"\n",
        "    # *** PROMPT GÜNCELLEMESİ SONU ***\n",
        "\n",
        "    _log_debug(f\"LLM'e gönderilecek Prompt (ilk 300 karakter): {prompt[:300]}...\") # Daha fazla görelim\n",
        "    _log_debug(f\"LLM'e gönderilecek Prompt (son 100 karakter): ...{prompt[-100:]}\")\n",
        "\n",
        "    try:\n",
        "        PRACTICAL_MAX_CONTEXT_LEN = 4096\n",
        "        tokenizer_reported_max_len = getattr(tokenizer_instance, 'model_max_length', PRACTICAL_MAX_CONTEXT_LEN)\n",
        "\n",
        "        if not isinstance(tokenizer_reported_max_len, int) or \\\n",
        "           tokenizer_reported_max_len <= 0 or \\\n",
        "           tokenizer_reported_max_len > 1000000:\n",
        "            _log_warning(f\"tokenizer.model_max_length ({tokenizer_reported_max_len}) geçersiz veya çok büyük. Pratik üst sınır {PRACTICAL_MAX_CONTEXT_LEN} kullanılıyor.\")\n",
        "            effective_model_max_len = PRACTICAL_MAX_CONTEXT_LEN\n",
        "        else:\n",
        "            effective_model_max_len = min(tokenizer_reported_max_len, PRACTICAL_MAX_CONTEXT_LEN)\n",
        "        _log_debug(f\"Kullanılacak efektif model maksimum token uzunluğu: {effective_model_max_len}\")\n",
        "\n",
        "        # Hücre 12'den gelen max_new_tokens kullanılır, buradaki varsayılan değil.\n",
        "        # Bu yüzden fonksiyona gelen max_new_tokens değerini alalım.\n",
        "        current_max_new_tokens_param = int(globals().get('RAG_LLM_MAX_NEW_TOKENS', max_new_tokens)) # Hücre 12'deki ayarı al\n",
        "        if not(isinstance(current_max_new_tokens_param, (int, float)) and current_max_new_tokens_param > 0):\n",
        "            current_max_new_tokens_param = 65 # Güvenli bir varsayılan (önceki başarılı değer)\n",
        "        current_max_new_tokens = int(current_max_new_tokens_param)\n",
        "        if current_max_new_tokens <= 0: current_max_new_tokens = 10\n",
        "\n",
        "        safety_margin = 20\n",
        "        truncation_target_length = effective_model_max_len - current_max_new_tokens - safety_margin\n",
        "\n",
        "        if truncation_target_length <= 0:\n",
        "            original_max_new_tokens_val = current_max_new_tokens\n",
        "            min_prompt_len_for_calc = 100\n",
        "            if effective_model_max_len - min_prompt_len_for_calc - safety_margin > 10:\n",
        "                current_max_new_tokens = max(10, effective_model_max_len - min_prompt_len_for_calc - safety_margin)\n",
        "            else:\n",
        "                current_max_new_tokens = max(10, int(effective_model_max_len * 0.15))\n",
        "\n",
        "            truncation_target_length = effective_model_max_len - current_max_new_tokens - safety_margin\n",
        "            _log_warning(f\"İlk max_new_tokens ({original_max_new_tokens_val}) çok büyük veya effective_model_max_len çok küçük. \"\n",
        "                         f\"Model kapasitesine göre max_new_tokens {current_max_new_tokens}'e ayarlandı. \"\n",
        "                         f\"truncation_target_length: {truncation_target_length}\")\n",
        "\n",
        "        if truncation_target_length <= 0:\n",
        "            error_msg = (f\"Model kapasitesi yetersiz. truncate_len ({truncation_target_length}) < 0. \"\n",
        "                         f\"eff_max_len: {effective_model_max_len}, final_max_new_tokens: {current_max_new_tokens}\")\n",
        "            _log_critical(error_msg)\n",
        "            return f\"HATA: {error_msg}\"\n",
        "\n",
        "        final_max_length_for_tokenizer = int(truncation_target_length)\n",
        "        if final_max_length_for_tokenizer <= 0:\n",
        "            error_msg = f\"Hesaplanan final_max_length_for_tokenizer ({final_max_length_for_tokenizer}) pozitif değil. Tokenizer'a gönderilemiyor.\"\n",
        "            _log_critical(error_msg)\n",
        "            return f\"HATA: {error_msg}\"\n",
        "\n",
        "        _log_debug(f\"Tokenizer'a gönderilecek max_length: {final_max_length_for_tokenizer}\")\n",
        "        inputs = tokenizer_instance(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=final_max_length_for_tokenizer\n",
        "        ).to(current_device)\n",
        "\n",
        "        input_token_length = inputs.input_ids.shape[1]\n",
        "        _log_info(f\"Tokenize edilmiş giriş (prompt) uzunluğu: {input_token_length} token.\")\n",
        "        _log_debug(f\"Giriş token ID'leri (ilk 10): {inputs.input_ids[0, :10].tolist()}\")\n",
        "\n",
        "        available_space_for_generation = effective_model_max_len - input_token_length - safety_margin\n",
        "        if current_max_new_tokens > available_space_for_generation:\n",
        "            original_max_new_tokens_val_2 = current_max_new_tokens\n",
        "            current_max_new_tokens = max(10, available_space_for_generation)\n",
        "            _log_warning(f\"Tokenize edilmiş prompt sonrası max_new_tokens ({original_max_new_tokens_val_2}) çok büyük. \"\n",
        "                         f\"Kalan alana göre {current_max_new_tokens}'e düşürüldü.\")\n",
        "\n",
        "        final_max_new_tokens_for_generation = int(current_max_new_tokens)\n",
        "        if final_max_new_tokens_for_generation <= 0:\n",
        "            error_msg = (f\"Tokenize edilmiş prompt sonrası cevap üretimi için yer kalmadı \"\n",
        "                         f\"(final_max_new_tokens: {final_max_new_tokens_for_generation}). Prompt çok uzun veya model kapasitesi düşük.\")\n",
        "            _log_error(error_msg)\n",
        "            return f\"HATA: Prompt çok uzun, cevap üretilemedi.\"\n",
        "\n",
        "        # LLM'e gönderilecek nihai max_new_tokens değerini logla (Hücre 12'deki ayardan gelmeli)\n",
        "        final_temp_for_generation = float(globals().get('RAG_LLM_TEMPERATURE', temperature))\n",
        "        final_do_sample_for_generation = bool(globals().get('RAG_LLM_DO_SAMPLE', do_sample))\n",
        "\n",
        "        _log_info(f\"LLM.generate çağrılıyor. max_new_tokens: {final_max_new_tokens_for_generation}, temperature: {final_temp_for_generation if final_do_sample_for_generation else 'N/A'}, do_sample: {final_do_sample_for_generation}\")\n",
        "\n",
        "\n",
        "        output_sequences = llm_model_instance.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=final_max_new_tokens_for_generation,\n",
        "            temperature=final_temp_for_generation if final_do_sample_for_generation else 1.0,\n",
        "            top_k=50 if final_do_sample_for_generation else None,\n",
        "            top_p=0.95 if final_do_sample_for_generation else None,\n",
        "            do_sample=final_do_sample_for_generation,\n",
        "            pad_token_id=tokenizer_instance.eos_token_id,\n",
        "        )\n",
        "\n",
        "        generated_answer = tokenizer_instance.decode(output_sequences[0, input_token_length:], skip_special_tokens=True).strip()\n",
        "        _log_info(f\"LLM'den ham cevap üretildi (ilk 100 karakter): '{str(generated_answer)[:100]}...'\")\n",
        "\n",
        "        if generated_answer.lower().startswith(\"yanitin:\"):\n",
        "            generated_answer = generated_answer[len(\"yanitin:\"):].strip()\n",
        "            _log_debug(\"Cevabın başındaki 'yanitin:' ifadesi temizlendi.\")\n",
        "\n",
        "        return generated_answer\n",
        "\n",
        "    except OverflowError as oe:\n",
        "        error_msg_overflow = f\"LLM ile cevap üretirken OverflowError oluştu (muhtemelen token uzunluğuyla ilgili): {oe}\"\n",
        "        _log_critical(error_msg_overflow, exc_info=True)\n",
        "        return f\"HATA: {error_msg_overflow}\"\n",
        "    except Exception as e:\n",
        "        _log_error(f\"LLM ile cevap üretirken genel bir HATA oluştu: {e}\", exc_info=True)\n",
        "        return \"HATA: Cevap üretimi sırasında bir sorun oluştu.\"\n",
        "\n",
        "if 'logger' in globals() and 'logging' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"LLM ile cevap üretme fonksiyonu (generate_answer_with_llm) tanımlandı (Son Prompt Ayarı ile).\")\n",
        "else:\n",
        "    print(\"INFO: LLM ile cevap üretme fonksiyonu (generate_answer_with_llm) tanımlandı (Son Prompt Ayarı ile).\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H8)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H8} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H8} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "GylzW2cG3AAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 9: Cevap Kalitesi Değerlendirme Fonksiyonları (GÜNCELLENDİ - Manuel BLEURT Checkpoint Yükleme)\n",
        "\n",
        "CELL_NAME_H9 = \"9: Cevap Kalitesi Değerlendirme Fonksiyonları\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H9)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H9} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H9} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H9} başlatıldı.\")\n",
        "\n",
        "# Gerekli importlar\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score_L\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import torch\n",
        "import os # Dosya ve dizin işlemleri için\n",
        "import zipfile # Zip dosyalarını açmak için\n",
        "import logging # Logger seviyesini kontrol etmek için\n",
        "\n",
        "# Lokal loglama yardımcı fonksiyonları (Hücre 9 için)\n",
        "log_active_h9 = 'logger' in globals()\n",
        "def _log_h9_info(message):\n",
        "    if log_active_h9: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_h9_warning(message):\n",
        "    if log_active_h9: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_h9_error(message, exc_info=False):\n",
        "    if log_active_h9: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_h9_debug(message):\n",
        "    if log_active_h9 and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG):\n",
        "        logger.debug(message)\n",
        "\n",
        "\n",
        "# --- BLEURT Scorer Yükleme Girişimi (Manuel Checkpoint ile) ---\n",
        "bleurt_scorer_instance_global = None\n",
        "# bleurt_available ve bleurt_score_module_global_for_h3_and_h9 Hücre 3'ten gelmeli\n",
        "\n",
        "# Google Drive'daki zip dosyasının yolu ve Colab'da açılacağı hedef yol\n",
        "drive_bleurt_zip_path = \"/content/drive/MyDrive/Colab_RAG_Projesi/bleurt_resources/BLEURT-20.zip\"\n",
        "colab_extract_base_path = \"/content/bleurt_checkpoint_extracted\" # Checkpoint'in açılacağı ana klasör\n",
        "# BLEURT-20.zip açıldığında genellikle içinde \"BLEURT-20\" adında bir klasör olur.\n",
        "# Bu yüzden scorer'a verilecek asıl checkpoint yolu /content/bleurt_checkpoint_extracted/BLEURT-20/ olmalı.\n",
        "# Ancak zip dosyasının içeriğine göre bu değişebilir. Şimdilik zip'in doğrudan hedef klasöre açıldığını varsayalım\n",
        "# ve BleurtScorer'a açılmış ana klasörün (örn: BLEURT-20) yolunu verelim.\n",
        "# Eğer zip iç yapısı farklıysa, bu yolun ayarlanması gerekebilir.\n",
        "# Örneğin, zip içinde doğrudan dosyalar varsa colab_extract_base_path + \"BLEURT-20_unzipped_content\" gibi bir yol olabilir.\n",
        "# VEYA zip zaten \"BLEURT-20\" adlı bir klasör içeriyorsa, o zaman:\n",
        "final_bleurt_checkpoint_path_in_colab = os.path.join(colab_extract_base_path, \"BLEURT-20\")\n",
        "\n",
        "\n",
        "if 'bleurt_available' in globals() and bleurt_available and \\\n",
        "   'bleurt_score_module_global_for_h3_and_h9' in globals() and bleurt_score_module_global_for_h3_and_h9 is not None:\n",
        "\n",
        "    _log_h9_info(f\"Manuel BLEURT checkpoint işlemi başlatılıyor.\")\n",
        "    _log_h9_info(f\"Google Drive'da beklenen ZIP yolu: {drive_bleurt_zip_path}\")\n",
        "    _log_h9_info(f\"Colab'da açılacak hedef checkpoint yolu: {final_bleurt_checkpoint_path_in_colab}\")\n",
        "\n",
        "    manual_checkpoint_ready = False\n",
        "    if os.path.exists(drive_bleurt_zip_path):\n",
        "        _log_h9_info(f\"'{drive_bleurt_zip_path}' Google Drive'da bulundu.\")\n",
        "        # Hedef açma klasörünün (örn: /content/bleurt_checkpoint_extracted/BLEURT-20)\n",
        "        # varlığını kontrol et. Eğer yoksa veya boşsa zip'i aç.\n",
        "        # Bu, her çalıştırmada zip'i tekrar tekrar açmayı önler.\n",
        "        # Ancak zip içeriği değişirse veya Colab oturumu yeniden başlarsa açma işlemi gerekebilir.\n",
        "        # Şimdilik, eğer final_bleurt_checkpoint_path_in_colab yoksa açalım.\n",
        "        if not os.path.exists(final_bleurt_checkpoint_path_in_colab) or \\\n",
        "           not os.listdir(final_bleurt_checkpoint_path_in_colab if os.path.exists(final_bleurt_checkpoint_path_in_colab) else colab_extract_base_path): # Klasör varsa ama boşsa da aç\n",
        "            _log_h9_info(f\"Hedef checkpoint '{final_bleurt_checkpoint_path_in_colab}' Colab'da bulunamadı veya boş. ZIP dosyası açılıyor...\")\n",
        "            try:\n",
        "                os.makedirs(colab_extract_base_path, exist_ok=True) # Ana açma klasörünü oluştur\n",
        "                with zipfile.ZipFile(drive_bleurt_zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(colab_extract_base_path) # ZIP'i ana klasöre aç\n",
        "                _log_h9_info(f\"ZIP dosyası '{colab_extract_base_path}' dizinine başarıyla açıldı.\")\n",
        "                # Açıldıktan sonra final_bleurt_checkpoint_path_in_colab'ın varlığını kontrol et\n",
        "                if os.path.exists(final_bleurt_checkpoint_path_in_colab):\n",
        "                    _log_h9_info(f\"Açılan checkpoint dizini '{final_bleurt_checkpoint_path_in_colab}' doğrulandı.\")\n",
        "                    # Dizin içeriğini loglayalım (ilk birkaç dosya/klasör)\n",
        "                    try:\n",
        "                        content_list = os.listdir(final_bleurt_checkpoint_path_in_colab)\n",
        "                        _log_h9_debug(f\"'{final_bleurt_checkpoint_path_in_colab}' içeriği (ilk 5): {content_list[:5]}\")\n",
        "                    except Exception as e_ls:\n",
        "                        _log_h9_warning(f\"Açılan checkpoint dizin içeriği listelenirken hata: {e_ls}\")\n",
        "                    manual_checkpoint_ready = True\n",
        "                else:\n",
        "                    _log_h9_error(f\"ZIP açıldı ancak beklenen '{final_bleurt_checkpoint_path_in_colab}' dizini bulunamadı. Lütfen ZIP dosyasının içeriğini kontrol edin. Belki farklı bir alt klasör adı vardır veya dosyalar doğrudan '{colab_extract_base_path}' altına açılmıştır.\")\n",
        "                    # Bu durumda, colab_extract_base_path'in içeriğini kontrol edebiliriz\n",
        "                    try:\n",
        "                        content_list_base = os.listdir(colab_extract_base_path)\n",
        "                        _log_h9_info(f\"'{colab_extract_base_path}' (ana açma dizini) içeriği: {content_list_base}\")\n",
        "                        # Eğer BLEURT-20.zip dosyaları doğrudan buraya açıyorsa, final_bleurt_checkpoint_path_in_colab'ı buna göre ayarlamamız gerekebilir.\n",
        "                        # Örneğin, eğer \"saved_model.pb\" bu dizindeyse, path doğru olabilir.\n",
        "                        # Şimdilik hata olarak bırakıyoruz.\n",
        "                    except Exception as e_ls_base:\n",
        "                         _log_h9_warning(f\"Ana açma dizin içeriği listelenirken hata: {e_ls_base}\")\n",
        "\n",
        "            except Exception as e_unzip:\n",
        "                _log_h9_error(f\"ZIP dosyası ('{drive_bleurt_zip_path}') açılırken HATA: {e_unzip}\", exc_info=True)\n",
        "        else:\n",
        "            _log_h9_info(f\"Hedef checkpoint '{final_bleurt_checkpoint_path_in_colab}' Colab'da zaten mevcut görünüyor. ZIP açma işlemi atlandı.\")\n",
        "            manual_checkpoint_ready = True\n",
        "    else:\n",
        "        _log_h9_error(f\"BLEURT ZIP dosyası ('{drive_bleurt_zip_path}') Google Drive'da bulunamadı. Lütfen yüklediğinizden ve yolun doğru olduğundan emin olun.\")\n",
        "\n",
        "    # Manuel checkpoint hazırsa scorer'ı yüklemeyi dene\n",
        "    if manual_checkpoint_ready:\n",
        "        _log_h9_info(f\"BLEURT scorer manuel olarak belirtilen yoldan yükleniyor: '{final_bleurt_checkpoint_path_in_colab}'\")\n",
        "        try:\n",
        "            bleurt_scorer_instance_global = bleurt_score_module_global_for_h3_and_h9.BleurtScorer(final_bleurt_checkpoint_path_in_colab)\n",
        "            _log_h9_info(f\"BLEURT scorer ('{final_bleurt_checkpoint_path_in_colab}') başarıyla yüklendi.\")\n",
        "        except AssertionError as e_assert: # Checkpoint yapısı beklenen gibi değilse bu hata alınabilir\n",
        "             _log_h9_error(f\"BLEURT checkpoint ('{final_bleurt_checkpoint_path_in_colab}') yüklenirken AssertionError (muhtemelen checkpoint yapısı/dosyaları eksik veya yanlış): {e_assert}. Lütfen açılan dizinin içeriğini ve ZIP dosyasının doğruluğunu kontrol edin.\", exc_info=True)\n",
        "             bleurt_scorer_instance_global = None\n",
        "        except Exception as e_bleurt_manual:\n",
        "            _log_h9_error(f\"Manuel BLEURT checkpoint ('{final_bleurt_checkpoint_path_in_colab}') yüklenirken genel bir HATA oluştu: {e_bleurt_manual}\", exc_info=True)\n",
        "            bleurt_scorer_instance_global = None\n",
        "    else:\n",
        "        if os.path.exists(drive_bleurt_zip_path): # Zip bulundu ama açılamadı veya doğrulanamadı\n",
        "             _log_h9_warning(f\"BLEURT ZIP dosyası bulundu ancak checkpoint hazırlanamadı. BLEURT skorlaması atlanacak.\")\n",
        "        # else: Zip zaten bulunamadı, yukarıda hata loglandı.\n",
        "        bleurt_scorer_instance_global = None\n",
        "else:\n",
        "    _log_h9_warning(\"BLEURT kütüphanesi/modülü (Hücre 3'ten) bulunamadı veya içe aktarılamadı. BLEURT skorlaması atlanacak.\")\n",
        "    bleurt_scorer_instance_global = None\n",
        "\n",
        "\n",
        "# --- Kosinüs Benzerliği Hesaplama Fonksiyonu ---\n",
        "def compute_cosine_similarity_for_eval(\n",
        "    sentence_transformer_model_instance,\n",
        "    candidate_text,\n",
        "    reference_text\n",
        "):\n",
        "    if sentence_transformer_model_instance is None:\n",
        "        _log_h9_warning(\"Kosinüs benzerliği için embedding modeli sağlanmadı. 0.0 döndürülüyor.\")\n",
        "        return 0.0\n",
        "    if not candidate_text or not isinstance(candidate_text, str) or not str(candidate_text).strip() or \\\n",
        "       not reference_text or not isinstance(reference_text, str) or not str(reference_text).strip():\n",
        "        _log_h9_debug(\"Kosinüs benzerliği için aday veya referans metin boş/geçersiz. 0.0 döndürülüyor.\")\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        embeddings = sentence_transformer_model_instance.encode(\n",
        "            [str(candidate_text), str(reference_text)],\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        similarity_matrix = cosine_similarity(\n",
        "            embeddings[0].reshape(1, -1).cpu().numpy(),\n",
        "            embeddings[1].reshape(1, -1).cpu().numpy()\n",
        "        )\n",
        "        similarity_score = similarity_matrix[0][0]\n",
        "        return float(similarity_score)\n",
        "    except Exception as e:\n",
        "        _log_h9_error(f\"Kosinüs benzerliği hesaplanırken HATA: {e}\", exc_info=False)\n",
        "        return 0.0\n",
        "\n",
        "# --- Ana Değerlendirme Fonksiyonu ---\n",
        "def evaluate_answer_quality(\n",
        "    generated_answer_text,\n",
        "    reference_answer_text,\n",
        "    main_embedding_model_instance\n",
        "):\n",
        "    _log_h9_info(\"Cevap kalitesi metrikleri hesaplanıyor...\")\n",
        "    _log_h9_debug(f\"Değerlendirilecek Cevap (ilk 50): '{str(generated_answer_text)[:50]}...'\")\n",
        "    _log_h9_debug(f\"Referans Cevap (ilk 50): '{str(reference_answer_text)[:50]}...'\")\n",
        "\n",
        "    metrics = {}\n",
        "    current_device_for_metrics = globals().get('DEVICE', torch.device(\"cpu\"))\n",
        "\n",
        "    ref_for_token = str(reference_answer_text) if reference_answer_text and isinstance(reference_answer_text, str) and str(reference_answer_text).strip() else \"[REFERANS_YOK]\"\n",
        "    gen_for_token = str(generated_answer_text) if generated_answer_text and isinstance(generated_answer_text, str) and str(generated_answer_text).strip() else \"[CEVAP_YOK]\"\n",
        "\n",
        "    try:\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        reference_tokens_lower = word_tokenize(ref_for_token.lower(), language='turkish')\n",
        "        generated_tokens_lower = word_tokenize(gen_for_token.lower(), language='turkish')\n",
        "    except Exception as e_tokenize:\n",
        "        _log_h9_error(f\"Değerlendirme için tokenizasyon sırasında HATA: {e_tokenize}\", exc_info=False)\n",
        "        reference_tokens_lower, generated_tokens_lower = [], []\n",
        "\n",
        "    try:\n",
        "        rouge_calculator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rs = rouge_calculator.score(str(reference_answer_text) or \"\", str(generated_answer_text) or \"\")\n",
        "        metrics.update({\n",
        "            'ROUGE1_F': rs['rouge1'].fmeasure,\n",
        "            'ROUGE2_F': rs['rouge2'].fmeasure,\n",
        "            'ROUGEL_F': rs['rougeL'].fmeasure\n",
        "        })\n",
        "    except Exception as e_rouge:\n",
        "        _log_h9_error(f\"ROUGE skorları hesaplanırken HATA: {e_rouge}\", exc_info=False)\n",
        "        metrics.update({'ROUGE1_F': \"HATA\", 'ROUGE2_F': \"HATA\", 'ROUGEL_F': \"HATA\"})\n",
        "\n",
        "    try:\n",
        "        bleu_val = 0.0\n",
        "        if reference_tokens_lower and generated_tokens_lower:\n",
        "            bleu_val = sentence_bleu([reference_tokens_lower], generated_tokens_lower, smoothing_function=SmoothingFunction().method4)\n",
        "        elif not reference_tokens_lower and not generated_tokens_lower:\n",
        "            if not (str(reference_answer_text) or \"\").strip() and not (str(generated_answer_text) or \"\").strip():\n",
        "                bleu_val = 1.0\n",
        "        metrics['BLEU'] = bleu_val\n",
        "    except Exception as e_bleu:\n",
        "        _log_h9_error(f\"BLEU skoru hesaplanırken HATA: {e_bleu}\", exc_info=False)\n",
        "        metrics['BLEU'] = \"HATA\"\n",
        "\n",
        "    try:\n",
        "        meteor_val = 0.0\n",
        "        if reference_tokens_lower and generated_tokens_lower:\n",
        "            meteor_val = meteor_score([reference_tokens_lower], generated_tokens_lower)\n",
        "        elif not reference_tokens_lower and not generated_tokens_lower:\n",
        "            if not (str(reference_answer_text) or \"\").strip() and not (str(generated_answer_text) or \"\").strip():\n",
        "                meteor_val = 1.0\n",
        "        metrics['METEOR'] = meteor_val\n",
        "    except Exception as e_meteor:\n",
        "        _log_h9_error(f\"METEOR skoru hesaplanırken HATA: {e_meteor}\", exc_info=False)\n",
        "        metrics['METEOR'] = \"HATA\"\n",
        "\n",
        "    try:\n",
        "        P, R, F1_bs = bert_score_L(\n",
        "            [gen_for_token], [ref_for_token],\n",
        "            lang=\"tr\",\n",
        "            model_type='bert-base-multilingual-cased',\n",
        "            device=current_device_for_metrics,\n",
        "            verbose=False\n",
        "        )\n",
        "        metrics['BERTScore_F1'] = F1_bs.mean().item()\n",
        "    except Exception as e_bertscore:\n",
        "        _log_h9_error(f\"BERTScore hesaplanırken HATA: {e_bertscore}\", exc_info=True)\n",
        "        metrics['BERTScore_F1'] = \"HATA\"\n",
        "\n",
        "    metrics['Cosine_Similarity'] = compute_cosine_similarity_for_eval(\n",
        "        main_embedding_model_instance,\n",
        "        generated_answer_text, # Orijinal stringleri gönderiyoruz\n",
        "        reference_answer_text\n",
        "    )\n",
        "\n",
        "    if bleurt_scorer_instance_global:\n",
        "        try:\n",
        "            bleurt_scores_raw = bleurt_scorer_instance_global.score(\n",
        "                references=[ref_for_token],\n",
        "                candidates=[gen_for_token]\n",
        "            )\n",
        "            metrics['BLEURT'] = bleurt_scores_raw[0] if bleurt_scores_raw and isinstance(bleurt_scores_raw[0], (float, np.number)) else \"HATA (Tip)\" # np.number daha genel\n",
        "        except Exception as e_bleurt_score:\n",
        "            _log_h9_error(f\"BLEURT skoru hesaplanırken HATA: {e_bleurt_score}\", exc_info=False)\n",
        "            metrics['BLEURT'] = \"HATA (Hesaplama)\"\n",
        "    else:\n",
        "        metrics['BLEURT'] = \"N/A (Yüklenemedi)\"\n",
        "\n",
        "    try:\n",
        "        f1_token = 0.0\n",
        "        if not generated_tokens_lower and not reference_tokens_lower:\n",
        "            if not (str(generated_answer_text) or \"\").strip() and not (str(reference_answer_text) or \"\").strip():\n",
        "                f1_token = 1.0\n",
        "        elif generated_tokens_lower and reference_tokens_lower:\n",
        "            common_tokens = set(reference_tokens_lower) & set(generated_tokens_lower)\n",
        "            precision = len(common_tokens) / len(generated_tokens_lower) if len(generated_tokens_lower) > 0 else 0\n",
        "            recall = len(common_tokens) / len(reference_tokens_lower) if len(reference_tokens_lower) > 0 else 0\n",
        "            if (precision + recall) > 0:\n",
        "                f1_token = 2 * (precision * recall) / (precision + recall)\n",
        "        metrics['Token_F1'] = f1_token\n",
        "    except Exception as e_token_f1:\n",
        "        _log_h9_error(f\"Token F1 skoru hesaplanırken HATA: {e_token_f1}\", exc_info=False)\n",
        "        metrics['Token_F1'] = \"HATA\"\n",
        "\n",
        "    final_metrics = {}\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, (float, np.number)): # np.number daha genel\n",
        "            final_metrics[key] = round(float(value), 4) # float'a çevirip yuvarla\n",
        "        else:\n",
        "            final_metrics[key] = value\n",
        "\n",
        "    _log_h9_info(f\"Değerlendirme metrikleri hesaplandı: {final_metrics}\")\n",
        "    return final_metrics\n",
        "\n",
        "if 'logger' in globals() and 'logging' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"Cevap kalitesi değerlendirme fonksiyonları tanımlandı (manuel BLEURT yükleme denemesi ile).\")\n",
        "else:\n",
        "    print(\"INFO: Cevap kalitesi değerlendirme fonksiyonları tanımlandı (manuel BLEURT yükleme denemesi ile).\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H9)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H9} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H9} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "RTT1lLOp3U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 10: Ana RAG Pipeline Fonksiyonu (GÜNCELLENDİ - Loglar Düzenlendi, Cache Anahtarı İyileştirildi, Metrik Yazdırma Düzenlendi)\n",
        "\n",
        "CELL_NAME_H10 = \"10: Ana RAG Pipeline Fonksiyonu\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H10)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H10} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H10} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H10} başlatıldı.\")\n",
        "\n",
        "# Gerekli fonksiyonlar ve global değişkenler önceki hücrelerden gelmeli.\n",
        "import datetime # Zamanlama için\n",
        "import os       # Dosya işlemleri için\n",
        "import hashlib  # Hashleme için\n",
        "\n",
        "# Lokal loglama yardımcı fonksiyonları (rag_pipeline içinde kullanılacak)\n",
        "log_active_rag = 'logger' in globals() # Logger'ın varlığını bir kere kontrol et (rag_pipeline için)\n",
        "def _log_rag_info(message):\n",
        "    if log_active_rag: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_rag_warning(message):\n",
        "    if log_active_rag: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_rag_error(message, exc_info=False):\n",
        "    if log_active_rag: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_rag_debug(message):\n",
        "    if log_active_rag and logger.isEnabledFor(logging.DEBUG):\n",
        "        logger.debug(message)\n",
        "    # else:\n",
        "    #     if LOG_LEVEL == logging.DEBUG: print(f\"DEBUG: {message}\")\n",
        "\n",
        "\n",
        "def rag_pipeline(\n",
        "    pdf_path,\n",
        "    question,\n",
        "    reference_answer,\n",
        "    llm_model_to_use,\n",
        "    llm_tokenizer_to_use,\n",
        "    st_embedding_model_instance,\n",
        "    lc_embedding_model_for_semantic_instance,\n",
        "    use_semantic_chunker=False,\n",
        "    recursive_chunk_size=1000,\n",
        "    recursive_chunk_overlap=150,\n",
        "    semantic_chunker_threshold_type=\"percentile\",\n",
        "    semantic_chunker_threshold_amount=95,\n",
        "    retrieval_top_k=5,\n",
        "    llm_max_new_tokens=512,\n",
        "    llm_temperature=0.3,\n",
        "    llm_do_sample=True,\n",
        "    log_retrieved_chunks_content=False\n",
        "):\n",
        "    pipeline_start_time = datetime.datetime.now()\n",
        "    pdf_basename = os.path.basename(pdf_path) if pdf_path else \"BilinmeyenPDF\"\n",
        "\n",
        "    _log_rag_info(f\"RAG Pipeline başlatılıyor. PDF: '{pdf_basename}', Soru (ilk 30): '{str(question)[:30]}...'\")\n",
        "    _log_rag_info(f\"Ayarlar: SemanticChunker={use_semantic_chunker}, RecChunkSize={recursive_chunk_size}, TopK={retrieval_top_k}, LLMMaxTokens={llm_max_new_tokens}\")\n",
        "\n",
        "    if not all([llm_model_to_use, llm_tokenizer_to_use, st_embedding_model_instance]):\n",
        "        error_msg = \"Temel modeller (LLM, Tokenizer, SentenceTransformer Embedding) eksik. Pipeline çalıştırılamıyor.\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return \"HATA: Modeller eksik.\", {}\n",
        "    if use_semantic_chunker and not lc_embedding_model_for_semantic_instance:\n",
        "        error_msg = \"Anlamsal parçalayıcı (SemanticChunker) istendi ancak Langchain embedding modeli eksik.\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return \"HATA: Semantik parçalayıcı için embedding modeli eksik.\", {}\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        error_msg = f\"Geçersiz PDF yolu veya dosya bulunamadı: {pdf_path}\"\n",
        "        _log_rag_error(error_msg)\n",
        "        return f\"HATA: PDF bulunamadı ({pdf_basename}).\", {}\n",
        "\n",
        "    pdf_content_hash_val = generate_file_hash(pdf_path) # generate_file_hash Hücre 6'dan\n",
        "    if not pdf_content_hash_val:\n",
        "        try:\n",
        "            pdf_mtime = os.path.getmtime(pdf_path)\n",
        "            pdf_content_hash_val = hashlib.md5(f\"{pdf_basename}_{pdf_mtime}\".encode()).hexdigest()[:16]\n",
        "            _log_rag_warning(f\"PDF içeriği için MD5 hash alınamadı, dosya adı ve mtime kullanılarak alternatif hash oluşturuldu: {pdf_content_hash_val}\")\n",
        "        except Exception as e_hash_fallback:\n",
        "            pdf_content_hash_val = hashlib.md5(pdf_basename.encode() + os.urandom(8)).hexdigest()[:16]\n",
        "            _log_rag_warning(f\"PDF hash için mtime alınırken hata ({e_hash_fallback}), dosya adı ve rastgele veri ile alternatif hash: {pdf_content_hash_val}\")\n",
        "\n",
        "    global_embedding_model_id_str = globals().get('EMBEDDING_MODEL_ID', 'unknown_emb_model') # Hücre 4'ten\n",
        "    embedding_model_name_for_cache_key = global_embedding_model_id_str.split('/')[-1].replace('-', '_')[:20]\n",
        "\n",
        "    chunker_settings_suffix_key = \"\"\n",
        "    if use_semantic_chunker:\n",
        "        chunker_settings_suffix_key = f\"_sem_t{str(semantic_chunker_threshold_type)[:3]}_a{semantic_chunker_threshold_amount}\"\n",
        "    else:\n",
        "        chunker_settings_suffix_key = f\"_rec_cs{recursive_chunk_size}_co{recursive_chunk_overlap}\"\n",
        "\n",
        "    file_identifier_for_cache_key = f\"{pdf_basename}_{pdf_content_hash_val[:8]}_emb_{embedding_model_name_for_cache_key}{chunker_settings_suffix_key}\"\n",
        "    _log_rag_info(f\"Oluşturulan önbellek (cache) anahtarı: {file_identifier_for_cache_key}\")\n",
        "\n",
        "    global_cache_dir_path = globals().get('CACHE_DIR', './cache_default') # Hücre 4'ten\n",
        "    processed_chunks_from_cache, faiss_idx_from_cache = load_chunks_and_index(file_identifier_for_cache_key, cache_directory=global_cache_dir_path) # Hücre 6'dan\n",
        "\n",
        "    if processed_chunks_from_cache is None or faiss_idx_from_cache is None:\n",
        "        _log_rag_info(f\"'{file_identifier_for_cache_key}' için önbellek bulunamadı/yüklenemedi. Veri yeniden işlenecek.\")\n",
        "        raw_text_from_pdf_val = pdf_to_text(pdf_path) # Hücre 6'dan\n",
        "        if not raw_text_from_pdf_val:\n",
        "            _log_rag_error(f\"PDF ({pdf_basename}) okunamadı veya boş içerik.\")\n",
        "            return f\"HATA: PDF ({pdf_basename}) okunamadı veya boş içerik.\", {}\n",
        "\n",
        "        if use_semantic_chunker:\n",
        "            raw_chunks_list = chunk_text_semantic( # Hücre 6'dan\n",
        "                raw_text_from_pdf_val, lc_embedding_model_for_semantic_instance,\n",
        "                semantic_chunker_threshold_type, semantic_chunker_threshold_amount\n",
        "            )\n",
        "        else:\n",
        "            raw_chunks_list = chunk_text_recursive( # Hücre 6'dan\n",
        "                raw_text_from_pdf_val, recursive_chunk_size, recursive_chunk_overlap\n",
        "            )\n",
        "\n",
        "        if not raw_chunks_list:\n",
        "            _log_rag_error(f\"Metin ({pdf_basename}) parçalanamadı (chunking başarısız).\")\n",
        "            return f\"HATA: Metin ({pdf_basename}) parçalanamadı (chunking başarısız).\", {}\n",
        "        _log_rag_info(f\"Ham metin {len(raw_chunks_list)} parçaya (chunk) ayrıldı.\")\n",
        "\n",
        "        processed_chunks_list = [\n",
        "            pc for pc in [preprocess_text(chunk) for chunk in raw_chunks_list if chunk and str(chunk).strip()] # Hücre 6'dan\n",
        "            if pc and str(pc).strip()\n",
        "        ]\n",
        "        if not processed_chunks_list:\n",
        "            _log_rag_error(f\"Ön işleme sonrası kullanılabilir chunk kalmadı ({pdf_basename}).\")\n",
        "            return f\"HATA: Ön işleme sonrası kullanılabilir chunk kalmadı ({pdf_basename}).\", {}\n",
        "        _log_rag_info(f\"{len(processed_chunks_list)} adet chunk başarıyla ön işlendi.\")\n",
        "\n",
        "        faiss_idx_built, _ = build_faiss_index(processed_chunks_list, st_embedding_model_instance) # Hücre 7'den\n",
        "        if faiss_idx_built is None:\n",
        "            _log_rag_error(f\"FAISS indeksi oluşturulamadı ({pdf_basename}).\")\n",
        "            return f\"HATA: FAISS indeksi oluşturulamadı ({pdf_basename}).\", {}\n",
        "\n",
        "        # Yeniden atama yapalım ki sonraki adımlarda doğru değişkenler kullanılsın\n",
        "        processed_chunks = processed_chunks_list\n",
        "        faiss_idx = faiss_idx_built\n",
        "        save_chunks_and_index(processed_chunks, faiss_idx, file_identifier_for_cache_key, cache_directory=global_cache_dir_path) # Hücre 6'dan\n",
        "    else:\n",
        "        _log_rag_info(f\"'{file_identifier_for_cache_key}' için önbellekten veriler (chunklar ve FAISS indeksi) başarıyla yüklendi.\")\n",
        "        # Cache'den yüklenenleri doğru değişkenlere ata\n",
        "        processed_chunks = processed_chunks_from_cache\n",
        "        faiss_idx = faiss_idx_from_cache\n",
        "\n",
        "\n",
        "    preprocessed_question_text = preprocess_text(question) # Hücre 6'dan\n",
        "    if not preprocessed_question_text:\n",
        "        _log_rag_warning(f\"Soru ('{str(question)[:30]}...') ön işleme sonrası boş kaldı. Ham soru kullanılacak.\")\n",
        "        preprocessed_question_text = str(question)\n",
        "\n",
        "    relevant_context_chunks_list = retrieve_relevant_chunks( # Hücre 7'den\n",
        "        preprocessed_question_text, faiss_idx, processed_chunks,\n",
        "        st_embedding_model_instance, top_k=retrieval_top_k\n",
        "    )\n",
        "\n",
        "    if log_retrieved_chunks_content:\n",
        "        _log_rag_info(f\"FAISS ARAMA SONUCU: '{preprocessed_question_text[:50]}...' sorgusu için {len(relevant_context_chunks_list)} chunk getirildi (top_k={retrieval_top_k}):\")\n",
        "        if relevant_context_chunks_list:\n",
        "            for i, chunk_text_retrieved in enumerate(relevant_context_chunks_list):\n",
        "                _log_rag_info(f\"--- GETİRİLEN CHUNK #{i+1} (ilk 100 karakter) ---\\n{str(chunk_text_retrieved)[:100]}...\\n{'-'*20}\")\n",
        "        else:\n",
        "            _log_rag_info(\"FAISS aramasından bu sorgu için chunk getirilmedi.\")\n",
        "    else:\n",
        "        _log_rag_info(f\"FAISS arama: '{preprocessed_question_text[:50]}...' sorgusu için {len(relevant_context_chunks_list)} chunk bulundu (içerik loglanmadı).\")\n",
        "\n",
        "    if relevant_context_chunks_list:\n",
        "        context_for_llm_val = \"\\n\\n---\\n\\n\".join(relevant_context_chunks_list)\n",
        "    else:\n",
        "        context_for_llm_val = \"Bu soruyla ilgili spesifik bir bilgi dokümanda bulunamadı.\"\n",
        "        _log_rag_warning(f\"Soru ('{preprocessed_question_text[:50]}...') için FAISS'ten ilgili chunk bulunamadı. LLM'e 'bilgi yok' mesajı gönderiliyor.\")\n",
        "\n",
        "    generated_answer_val = generate_answer_with_llm( # Hücre 8'den\n",
        "        context_for_llm_val, str(question),\n",
        "        llm_model_to_use, llm_tokenizer_to_use,\n",
        "        max_new_tokens=llm_max_new_tokens,\n",
        "        temperature=llm_temperature,\n",
        "        do_sample=llm_do_sample\n",
        "    )\n",
        "\n",
        "    evaluation_metrics_dict = evaluate_answer_quality( # Hücre 9'dan\n",
        "        generated_answer_val, str(reference_answer), st_embedding_model_instance\n",
        "    )\n",
        "\n",
        "    pipeline_end_time_val = datetime.datetime.now()\n",
        "    pipeline_duration_val = pipeline_end_time_val - pipeline_start_time\n",
        "\n",
        "    # --- Sonuçları Logla/Yazdır (Daha Okunaklı) ---\n",
        "    _log_rag_info(\"\\n\" + \"=\"*15 + \" RAG PIPELINE SONUÇLARI \" + \"=\"*15)\n",
        "    _log_rag_info(f\"İŞLENEN PDF:         {pdf_basename}\")\n",
        "    _log_rag_info(f\"SORU:                {str(question)}\")\n",
        "    _log_rag_info(\"-\" * 50)\n",
        "    _log_rag_info(f\"ÜRETİLEN CEVAP:      {str(generated_answer_val)}\")\n",
        "    _log_rag_info(\"-\" * 50)\n",
        "    _log_rag_info(f\"REFERANS CEVAP:      {str(reference_answer) if reference_answer else 'N/A'}\")\n",
        "    _log_rag_info(\"=\" * 50)\n",
        "    _log_rag_info(\"DEĞERLENDİRME METRİKLERİ:\")\n",
        "\n",
        "    if isinstance(evaluation_metrics_dict, dict):\n",
        "        max_key_len = 0\n",
        "        if evaluation_metrics_dict:\n",
        "             max_key_len = max(len(key) for key in evaluation_metrics_dict.keys()) if evaluation_metrics_dict else 0\n",
        "\n",
        "        for metric_name, metric_value in evaluation_metrics_dict.items():\n",
        "            # Sayısal değerleri formatla, diğerlerini olduğu gibi bırak\n",
        "            if isinstance(metric_value, float):\n",
        "                formatted_value = f\"{metric_value:.4f}\"\n",
        "            else:\n",
        "                formatted_value = metric_value\n",
        "            _log_rag_info(f\"  {str(metric_name).ljust(max_key_len + 2)}: {formatted_value}\")\n",
        "    else:\n",
        "        _log_rag_info(f\"  Metrikler alınamadı: {evaluation_metrics_dict}\")\n",
        "\n",
        "    _log_rag_info(\"=\" * 50)\n",
        "    _log_rag_info(f\"Pipeline Çalışma Süresi: {pipeline_duration_val}\")\n",
        "    _log_rag_info(\"=\"*15 + \"      SONUÇLAR BİTTİ      \" + \"=\"*15 + \"\\n\")\n",
        "\n",
        "    return generated_answer_val, evaluation_metrics_dict\n",
        "\n",
        "\n",
        "if 'logger' in globals() and logger.isEnabledFor(logging.INFO):\n",
        "    logger.info(\"Ana RAG pipeline fonksiyonu (rag_pipeline) tanımlandı (metrik yazdırma güncellendi).\")\n",
        "else:\n",
        "    print(\"INFO: Ana RAG pipeline fonksiyonu (rag_pipeline) tanımlandı (metrik yazdırma güncellendi).\")\n",
        "\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H10)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H10} tamamlandı.\")\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H10} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H10} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "UNVDAdHL3vU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 11: İşlem Yapılacak PDF Dosyasının Seçilmesi (GÜNCELLENDİ - Loglar Düzenlendi)\n",
        "\n",
        "CELL_NAME_H11 = \"11: İşlem Yapılacak PDF Dosyasının Seçilmesi\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H11)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H11} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H11} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H11} başlatıldı.\")\n",
        "\n",
        "import os\n",
        "# PDF_KLASOR_YOLU global değişkeni Hücre 4'te tanımlandı.\n",
        "# logger global değişkeni Hücre 0'da tanımlandı (veya burada varlığı kontrol ediliyor).\n",
        "\n",
        "pdf_dosyalari_listesi = []\n",
        "secilen_pdf_tam_yolu = None # Bu hücrede seçilecek PDF'in tam yolu\n",
        "secilen_pdf_dosya_adi = None # Bu hücrede seçilecek PDF'in sadece adı\n",
        "\n",
        "if 'logger' in globals():\n",
        "    logger.info(\"İşlem yapılacak PDF dosyasını seçme süreci başlatılıyor.\")\n",
        "else:\n",
        "    print(\"INFO: İşlem yapılacak PDF dosyasını seçme süreci başlatılıyor.\")\n",
        "\n",
        "# PDF_KLASOR_YOLU'nun varlığını ve geçerliliğini kontrol et\n",
        "current_pdf_klasor_yolu = globals().get('PDF_KLASOR_YOLU', None)\n",
        "\n",
        "if not current_pdf_klasor_yolu:\n",
        "    error_msg = \"PDF klasör yolu (PDF_KLASOR_YOLU) global değişkenlerde tanımlanmamış veya None. Lütfen Hücre 4'ü çalıştırın.\"\n",
        "    if 'logger' in globals(): logger.error(error_msg)\n",
        "    else: print(f\"ERROR: {error_msg}\")\n",
        "elif not os.path.exists(current_pdf_klasor_yolu) or not os.path.isdir(current_pdf_klasor_yolu):\n",
        "    error_msg = f\"Tanımlanan PDF klasör yolu '{current_pdf_klasor_yolu}' bulunamadı veya geçerli bir dizin değil.\"\n",
        "    if 'logger' in globals(): logger.error(error_msg)\n",
        "    else: print(f\"ERROR: {error_msg}\")\n",
        "else:\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF dosyaları '{current_pdf_klasor_yolu}' klasöründen listeleniyor...\")\n",
        "    else:\n",
        "        print(f\"INFO: PDF dosyaları '{current_pdf_klasor_yolu}' klasöründen listeleniyor...\")\n",
        "    try:\n",
        "        # Sadece .pdf uzantılı dosyaları al ve gizli dosyaları (örn: ._ ile başlayanlar) hariç tut\n",
        "        pdf_dosyalari_listesi = [\n",
        "            f for f in os.listdir(current_pdf_klasor_yolu)\n",
        "            if f.lower().endswith(\".pdf\") and not f.startswith('.')\n",
        "        ]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Bulunan PDF dosyaları ({len(pdf_dosyalari_listesi)} adet): {pdf_dosyalari_listesi}\")\n",
        "        else:\n",
        "            print(f\"INFO: Bulunan PDF dosyaları ({len(pdf_dosyalari_listesi)} adet): {pdf_dosyalari_listesi}\")\n",
        "    except Exception as e:\n",
        "        error_msg = f\"PDF dosyaları ('{current_pdf_klasor_yolu}' içinden) listelenirken HATA oluştu: {e}\"\n",
        "        if 'logger' in globals(): logger.error(error_msg, exc_info=True)\n",
        "        else:\n",
        "            print(f\"ERROR: {error_msg}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        pdf_dosyalari_listesi = [] # Hata durumunda listeyi boşalt\n",
        "\n",
        "# PDF dosyası seçimi\n",
        "if not pdf_dosyalari_listesi:\n",
        "    warning_msg = f\"'{current_pdf_klasor_yolu if current_pdf_klasor_yolu else 'Tanımsız PDF klasörü'}' içinde işlenecek PDF dosyası bulunamadı.\"\n",
        "    if 'logger' in globals(): logger.warning(warning_msg)\n",
        "    print(f\"⚠️  {warning_msg}\") # Kullanıcıya her zaman göster\n",
        "else:\n",
        "    print(f\"\\n📂 '{current_pdf_klasor_yolu}' klasöründe bulunan PDF dosyaları:\")\n",
        "    for i, dosya_adi_listede in enumerate(pdf_dosyalari_listesi):\n",
        "        print(f\"   {i+1}. {dosya_adi_listede}\")\n",
        "\n",
        "    if len(pdf_dosyalari_listesi) == 1:\n",
        "        # Eğer sadece bir PDF varsa, onu otomatik seç\n",
        "        secilen_pdf_dosya_adi = pdf_dosyalari_listesi[0]\n",
        "        if 'logger' in globals():\n",
        "            logger.info(f\"Klasörde tek PDF dosyası bulundu ve otomatik olarak seçildi: {secilen_pdf_dosya_adi}\")\n",
        "        print(f\"\\n✅ Tek PDF dosyası bulundu ve otomatik olarak seçildi: {secilen_pdf_dosya_adi}\")\n",
        "    else:\n",
        "        # Birden fazla PDF varsa kullanıcıdan seçim iste\n",
        "        while True:\n",
        "            try:\n",
        "                secim_no_str = input(f\"\\n➡️ Lütfen işlem yapmak istediğiniz PDF'in numarasını girin (1-{len(pdf_dosyalari_listesi)}): \").strip()\n",
        "                if 'logger' in globals(): logger.debug(f\"Kullanıcının PDF seçimi için girdisi: '{secim_no_str}'\")\n",
        "\n",
        "                if not secim_no_str: # Boş giriş\n",
        "                    print(\"⚠️ Giriş yapılmadı. Lütfen bir numara girin.\")\n",
        "                    if 'logger' in globals(): logger.warning(\"Kullanıcı PDF seçimi için boş giriş yaptı.\")\n",
        "                    continue\n",
        "\n",
        "                secim_no = int(secim_no_str)\n",
        "                if 1 <= secim_no <= len(pdf_dosyalari_listesi):\n",
        "                    secilen_pdf_dosya_adi = pdf_dosyalari_listesi[secim_no - 1]\n",
        "                    if 'logger' in globals():\n",
        "                        logger.info(f\"Kullanıcı tarafından PDF seçildi. Seçim No: {secim_no}, Dosya Adı: '{secilen_pdf_dosya_adi}'\")\n",
        "                    break # Geçerli seçim yapıldı, döngüden çık\n",
        "                else:\n",
        "                    print(f\"⚠️ Geçersiz numara. Lütfen 1 ile {len(pdf_dosyalari_listesi)} arasında bir numara girin.\")\n",
        "                    if 'logger' in globals(): logger.warning(f\"Kullanıcının girdiği PDF seçim numarası ({secim_no}) geçersiz.\")\n",
        "            except ValueError:\n",
        "                print(\"⚠️ Geçersiz giriş. Lütfen sayısal bir değer (numara) girin.\")\n",
        "                if 'logger' in globals(): logger.warning(\"Kullanıcı PDF seçimi için sayısal olmayan bir değer girdi.\")\n",
        "            except EOFError: # Kullanıcı input'u keserse (örn: Colab'da stop butonu)\n",
        "                 print(\"\\n🚫 PDF seçimi iptal edildi.\")\n",
        "                 if 'logger' in globals(): logger.info(\"Kullanıcı PDF seçimini EOF ile iptal etti.\")\n",
        "                 secilen_pdf_dosya_adi = None # Seçim yapılmadı\n",
        "                 break\n",
        "            except Exception as e_input: # Beklenmedik diğer hatalar\n",
        "                error_msg_input = f\"PDF seçimi sırasında beklenmedik bir hata oluştu: {e_input}\"\n",
        "                if 'logger' in globals(): logger.error(error_msg_input, exc_info=True)\n",
        "                else: print(f\"ERROR: {error_msg_input}\")\n",
        "                secilen_pdf_dosya_adi = None # Seçim yapılamadı\n",
        "                break\n",
        "\n",
        "# Seçilen PDF için tam yolu oluştur\n",
        "if secilen_pdf_dosya_adi and current_pdf_klasor_yolu:\n",
        "    secilen_pdf_tam_yolu = os.path.join(current_pdf_klasor_yolu, secilen_pdf_dosya_adi)\n",
        "    if 'logger' in globals():\n",
        "        logger.info(f\"PDF seçimi tamamlandı. Üzerinde çalışılacak PDF: {secilen_pdf_tam_yolu}\")\n",
        "    print(f\"\\n✅ PDF seçimi başarıyla tamamlandı.\")\n",
        "    print(f\"📄 Üzerinde çalışılacak PDF dosyası: {secilen_pdf_tam_yolu}\")\n",
        "elif not pdf_dosyalari_listesi:\n",
        "    # Zaten yukarıda uyarı verildi, burada ek bir log yeterli.\n",
        "    if 'logger' in globals(): logger.warning(\"İşlenecek PDF dosyası bulunamadığı için seçim yapılamadı.\")\n",
        "else: # Seçim iptal edildi veya bir hata oluştu\n",
        "    if 'logger' in globals():\n",
        "        logger.error(\"PDF seçimi yapılmadı veya bir hata nedeniyle tamamlanamadı.\")\n",
        "    print(\"\\n❌ PDF seçimi yapılmadı veya bir hata nedeniyle tamamlanamadı.\")\n",
        "    secilen_pdf_tam_yolu = None # Emin olmak için None yapalım\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    logger.info(f\"Hücre {CELL_NAME_H11} tamamlandı.\")\n",
        "    log_cell_end(CELL_NAME_H11)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H11} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H11} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "EsljbV1K4bL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 12: Soru Girişi, Referans Cevap Girişi ve RAG Pipeline'ını Çalıştırma (GÜNCELLENDİ - Max_Tokens 65'e Çıkarıldı)\n",
        "\n",
        "CELL_NAME_H12 = \"12: RAG Pipeline Çalıştırma ve Değerlendirme\"\n",
        "# log_cell_start ve logger'ın varlığını kontrol et\n",
        "if 'log_cell_start' in globals() and 'logger' in globals():\n",
        "    log_cell_start(CELL_NAME_H12)\n",
        "    logger.info(f\"Hücre {CELL_NAME_H12} başlatıldı.\")\n",
        "else:\n",
        "    print(f\"--- {CELL_NAME_H12} BAŞLADI (log_cell_start veya logger bulunamadı) ---\")\n",
        "    print(f\"Hücre {CELL_NAME_H12} başlatıldı.\")\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import logging # Logger seviyesini kontrol etmek için\n",
        "\n",
        "# Lokal loglama yardımcı fonksiyonları (Hücre 12 için)\n",
        "log_active_h12 = 'logger' in globals()\n",
        "def _log_h12_info(message):\n",
        "    if log_active_h12: logger.info(message)\n",
        "    else: print(f\"INFO: {message}\")\n",
        "def _log_h12_warning(message):\n",
        "    if log_active_h12: logger.warning(message)\n",
        "    else: print(f\"WARNING: {message}\")\n",
        "def _log_h12_error(message, exc_info=False):\n",
        "    if log_active_h12: logger.error(message, exc_info=exc_info)\n",
        "    else:\n",
        "        print(f\"ERROR: {message}\")\n",
        "        if exc_info:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "def _log_h12_debug(message):\n",
        "    if log_active_h12 and 'logging' in globals() and logger.isEnabledFor(logging.DEBUG): # logging import edildiğinden emin ol\n",
        "        logger.debug(message)\n",
        "\n",
        "ready_to_run_pipeline = False\n",
        "current_selected_pdf_path = globals().get('secilen_pdf_tam_yolu', None)\n",
        "llm_model_instance = globals().get('llm_model', None)\n",
        "llm_tokenizer_instance = globals().get('llm_tokenizer', None)\n",
        "st_embedding_model_instance_main = globals().get('embedding_model_st', None)\n",
        "\n",
        "if current_selected_pdf_path and os.path.exists(current_selected_pdf_path):\n",
        "    if llm_model_instance and llm_tokenizer_instance and st_embedding_model_instance_main:\n",
        "        ready_to_run_pipeline = True\n",
        "        _log_h12_info(f\"Pipeline çalıştırmak için ön koşullar tamam. Seçilen PDF: {os.path.basename(current_selected_pdf_path)}\")\n",
        "        print(f\"📖 Üzerinde çalışılacak PDF: {current_selected_pdf_path}\")\n",
        "    else:\n",
        "        error_msg_models = \"Gerekli modeller (LLM, Tokenizer, Embedding) yüklenmemiş. Lütfen Hücre 5'i kontrol edin.\"\n",
        "        _log_h12_error(error_msg_models)\n",
        "        print(\"❌ Gerekli modeller yüklenmemiş. Pipeline çalıştırılamaz.\")\n",
        "else:\n",
        "    error_msg_pdf = \"Çalıştırılacak PDF dosyası seçilmemiş veya bulunamadı. Lütfen Hücre 11'i kontrol edin.\"\n",
        "    _log_h12_error(error_msg_pdf)\n",
        "    print(\"❌ Çalıştırılacak PDF dosyası seçilmemiş/bulunamadı. Pipeline çalıştırılamaz.\")\n",
        "\n",
        "if ready_to_run_pipeline:\n",
        "    RAG_USE_SEMANTIC_CHUNKER = False\n",
        "    RAG_RECURSIVE_CHUNK_SIZE = 320\n",
        "    RAG_RECURSIVE_CHUNK_OVERLAP = 100\n",
        "    RAG_SEMANTIC_THRESHOLD_TYPE = \"percentile\"\n",
        "    RAG_SEMANTIC_THRESHOLD_AMOUNT = 95\n",
        "    RAG_RETRIEVAL_TOP_K = 5 # Bir önceki başarılı ayar\n",
        "\n",
        "    # *** DEĞİŞİKLİK: RAG_LLM_MAX_NEW_TOKENS artırıldı ***\n",
        "    RAG_LLM_MAX_NEW_TOKENS = 65 # Önceki 60 idi.\n",
        "    # *****************************************************\n",
        "\n",
        "    RAG_LLM_TEMPERATURE = 0.01\n",
        "    RAG_LLM_DO_SAMPLE = True\n",
        "    RAG_LOG_RETRIEVED_CHUNKS_CONTENT = False\n",
        "\n",
        "    _log_h12_info(\"Kullanılacak RAG Pipeline Ayarları (Güncellendi - Son Rötuş):\")\n",
        "    if RAG_USE_SEMANTIC_CHUNKER:\n",
        "        _log_h12_info(f\"  Parçalama (Chunking): Anlamsal (Semantic), Threshold Tipi='{RAG_SEMANTIC_THRESHOLD_TYPE}', Miktar='{RAG_SEMANTIC_THRESHOLD_AMOUNT}'\")\n",
        "    else:\n",
        "        _log_h12_info(f\"  Parçalama (Chunking): Yinelemeli (Recursive), Boyut (Size)={RAG_RECURSIVE_CHUNK_SIZE}, Örtüşme (Overlap)={RAG_RECURSIVE_CHUNK_OVERLAP}\")\n",
        "    _log_h12_info(f\"  Getirme (Retrieval): Top K={RAG_RETRIEVAL_TOP_K}\")\n",
        "    _log_h12_info(f\"  LLM Üretim (Generation): Max Yeni Token={RAG_LLM_MAX_NEW_TOKENS} (Değiştirildi), Sıcaklık (Temp)={RAG_LLM_TEMPERATURE if RAG_LLM_DO_SAMPLE else 'N/A'}, Örnekleme (Sample)={RAG_LLM_DO_SAMPLE}\")\n",
        "    _log_h12_info(f\"  Getirilen Chunk İçerikleri Loglansın mı?: {RAG_LOG_RETRIEVED_CHUNKS_CONTENT}\")\n",
        "\n",
        "    def run_rag_pipeline_with_settings(pdf_file_path, user_question, reference_answer_text):\n",
        "        _log_h12_info(f\"RAG Pipeline çalıştırılıyor. Soru (ilk 50): '{str(user_question)[:50]}...'\")\n",
        "        return rag_pipeline(\n",
        "            pdf_path=pdf_file_path,\n",
        "            question=user_question,\n",
        "            reference_answer=reference_answer_text,\n",
        "            llm_model_to_use=llm_model_instance,\n",
        "            llm_tokenizer_to_use=llm_tokenizer_instance,\n",
        "            st_embedding_model_instance=st_embedding_model_instance_main,\n",
        "            lc_embedding_model_for_semantic_instance=globals().get('langchain_embeddings_for_semantic_chunker', None),\n",
        "            use_semantic_chunker=RAG_USE_SEMANTIC_CHUNKER,\n",
        "            recursive_chunk_size=RAG_RECURSIVE_CHUNK_SIZE,\n",
        "            recursive_chunk_overlap=RAG_RECURSIVE_CHUNK_OVERLAP,\n",
        "            semantic_chunker_threshold_type=RAG_SEMANTIC_THRESHOLD_TYPE,\n",
        "            semantic_chunker_threshold_amount=RAG_SEMANTIC_THRESHOLD_AMOUNT,\n",
        "            retrieval_top_k=RAG_RETRIEVAL_TOP_K,\n",
        "            llm_max_new_tokens=RAG_LLM_MAX_NEW_TOKENS, # Güncel değer\n",
        "            llm_temperature=RAG_LLM_TEMPERATURE,\n",
        "            llm_do_sample=RAG_LLM_DO_SAMPLE,\n",
        "            log_retrieved_chunks_content=RAG_LOG_RETRIEVED_CHUNKS_CONTENT\n",
        "        )\n",
        "\n",
        "    default_question = \"Yüksek lisans programlarına öğrenci kabulü ne zaman yapılır?\"\n",
        "    default_reference_answer = \"Yüksek lisans programlarına, güz ve bahar yarıyılları başında öğrenci alınabilir.\"\n",
        "\n",
        "    _log_h12_info(f\"--- Varsayılan Soru ile RAG Pipeline Çalıştırılıyor (Son Rötuş Ayarları) ---\")\n",
        "    _log_h12_info(f\"Varsayılan Soru: {default_question}\")\n",
        "    _log_h12_info(f\"Varsayılan Referans Cevap: {default_reference_answer}\")\n",
        "\n",
        "    final_generated_answer, final_evaluation_metrics = run_rag_pipeline_with_settings(\n",
        "        current_selected_pdf_path,\n",
        "        default_question,\n",
        "        default_reference_answer\n",
        "    )\n",
        "    _log_h12_info(f\"VARSAYILAN SORU İÇİN PIPELINE TAMAMLANDI (Son Rötuş Ayarları). Cevap (ilk 50): '{str(final_generated_answer)[:50]}...', BERTScore_F1: {final_evaluation_metrics.get('BERTScore_F1', 'N/A')}, Token_F1: {final_evaluation_metrics.get('Token_F1', 'N/A')}\")\n",
        "\n",
        "    _log_h12_info(\"Kullanıcıya başka soru sorma döngüsü başlatılıyor.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_response_for_new_question = input(\"\\n🔄 Başka bir soru sormak ister misiniz? (evet/e veya hayır/h): \").strip().lower()\n",
        "            _log_h12_debug(f\"Kullanıcının yeni soru sorma isteği: '{user_response_for_new_question}'\")\n",
        "\n",
        "            if user_response_for_new_question in [\"evet\", \"e\"]:\n",
        "                new_user_question = input(\"\\n❓ Lütfen yeni sorunuzu girin: \").strip()\n",
        "                if not new_user_question:\n",
        "                    print(\"⚠️ Boş soru girdiniz. Lütfen geçerli bir soru sorun.\")\n",
        "                    _log_h12_warning(\"Kullanıcı yeni soru için boş giriş yaptı.\")\n",
        "                    continue\n",
        "\n",
        "                new_reference_answer = input(\"🎯 Bu soru için ideal (referans) cevabı girin (yoksa boş bırakın veya 'yok' yazın): \").strip()\n",
        "                if not new_reference_answer or new_reference_answer.lower() == 'yok':\n",
        "                    new_reference_answer = \"\"\n",
        "                    _log_h12_debug(\"Yeni soru için referans cevap girilmedi.\")\n",
        "                else:\n",
        "                    _log_h12_debug(f\"Yeni soru için referans cevap: '{str(new_reference_answer)[:50]}...'\")\n",
        "\n",
        "                _log_h12_info(f\"--- Kullanıcının Yeni Sorusu ile RAG Pipeline Çalıştırılıyor (Son Rötuş Ayarları) ---\")\n",
        "                final_generated_answer, final_evaluation_metrics = run_rag_pipeline_with_settings(\n",
        "                    current_selected_pdf_path,\n",
        "                    new_user_question,\n",
        "                    new_reference_answer\n",
        "                )\n",
        "                _log_h12_info(f\"YENİ SORU İÇİN PIPELINE TAMAMLANDI (Son Rötuş Ayarları). Cevap (ilk 50): '{str(final_generated_answer)[:50]}...', BERTScore_F1: {final_evaluation_metrics.get('BERTScore_F1', 'N/A')}, Token_F1: {final_evaluation_metrics.get('Token_F1', 'N/A')}\")\n",
        "\n",
        "            elif user_response_for_new_question in [\"hayır\", \"h\"]:\n",
        "                _log_h12_info(\"Kullanıcı işlemi sonlandırdı.\")\n",
        "                print(\"\\n🛑 İşlem sonlandırıldı. İyi günler!\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"⚠️ Geçersiz yanıt. Lütfen 'evet' (veya 'e') ya da 'hayır' (veya 'h') girin.\")\n",
        "                _log_h12_warning(f\"Kullanıcının yeni soru sorma isteği için geçersiz yanıt: '{user_response_for_new_question}'.\")\n",
        "        except EOFError:\n",
        "            _log_h12_info(\"Kullanıcı soru sorma döngüsünü EOF ile sonlandırdı.\")\n",
        "            print(\"\\n🛑 İşlem (EOF ile) sonlandırıldı.\")\n",
        "            break\n",
        "        except Exception as e_loop:\n",
        "            error_msg_loop = f\"Soru sorma döngüsünde beklenmedik bir hata oluştu: {e_loop}\"\n",
        "            _log_h12_error(error_msg_loop, exc_info=True)\n",
        "            print(\"🔁 Döngüde bir sorun oluştu, lütfen tekrar deneyin veya 'hayır' ile çıkın.\")\n",
        "\n",
        "if not ready_to_run_pipeline:\n",
        "    _log_h12_warning(\"Pipeline ön koşulları sağlanamadığı için RAG işlemi çalıştırılmadı.\")\n",
        "\n",
        "if 'log_cell_end' in globals() and 'logger' in globals():\n",
        "    log_cell_end(CELL_NAME_H12)\n",
        "else:\n",
        "    print(f\"Hücre {CELL_NAME_H12} tamamlandı.\")\n",
        "    print(f\"--- {CELL_NAME_H12} TAMAMLANDI (log_cell_end veya logger bulunamadı) ---\")"
      ],
      "metadata": {
        "id": "8x4Rrw0j4tQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}